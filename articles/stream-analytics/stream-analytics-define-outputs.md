---
title: Описание выходных данных из Azure Stream Analytics
description: В этой статье описываются параметры вывода данных для анализа результатов, доступные в Azure Stream Analytics, включая Power BI.
services: stream-analytics
author: jasonwhowell
ms.author: jasonh
manager: kfile
ms.reviewer: jasonh
ms.service: stream-analytics
ms.topic: conceptual
ms.date: 05/14/2018
ms.openlocfilehash: e14c4671669bc00e52c84c821a5229d26b2ba1c1
ms.sourcegitcommit: eb75f177fc59d90b1b667afcfe64ac51936e2638
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 05/16/2018
ms.locfileid: "34211377"
---
# <a name="understand-outputs-from-azure-stream-analytics"></a>Описание выходных данных из Azure Stream Analytics
В этой статье описываются разные типы выходных данных, доступные для задания Azure Stream Analytics. Выходные данные дают возможность сохранить и хранить результаты задания Stream Analytics. Используя выходные данные, можно выполнять расширенную бизнес-аналитику и хранить свои данные. 

При составлении запроса Stream Analytics укажите имя выходных данных с помощью [предложение INTO](https://msdn.microsoft.com/azure/stream-analytics/reference/into-azure-stream-analytics). Можно использовать одни выходные данные на задание или несколько выходных данных для задания потоковой передачи, если требуется, указав в запросе несколько предложений INTO.

Для создания, изменения и проверки выходных данных задания Stream Analytics можно использовать [портал Azure](stream-analytics-quick-create-portal.md#configure-output-to-the-job), [Azure PowerShell](stream-analytics-quick-create-powershell.md#configure-output-to-the-job), [API .Net](https://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.management.streamanalytics.ioutputsoperations?view=azure-dotnet), [REST API](https://docs.microsoft.com/en-us/rest/api/streamanalytics/stream-analytics-output) и [Visual Studio](stream-analytics-tools-for-visual-studio.md).

Некоторые типы выходных данных поддерживают [секционирование](#partitioning), и [размер выходного пакета](#output-batch-size) изменяется для оптимизации пропускной способности.


## <a name="azure-data-lake-store"></a>Хранилище озера данных Azure
Stream Analytics поддерживает [хранилище озера данных Azure](https://azure.microsoft.com/services/data-lake-store/). Хранилище озера данных Azure — это крупномасштабный репозиторий корпоративного уровня для рабочих нагрузок анализа больших данных. Озеро данных Azure позволяет сохранять данные с любым размером, типом и скоростью приема в одном месте для эксплуатационной и исследовательской аналитики. Stream Analytics необходимо разрешение на доступ к Data Lake Store.

### <a name="authorize-an-azure-data-lake-store-account"></a>Авторизация учетной записи Azure Data Lake Store

1. Если для хранения выходных данных на портале Azure выбрано Data Lake Store, вам будет предложено авторизовать подключение к существующей службе Data Lake Store.  

   ![Авторизация хранилища озера данных](./media/stream-analytics-define-outputs/06-stream-analytics-define-outputs.png)  

2. Если у вас уже есть доступ к Data Lake Store, нажмите кнопку **Авторизовать сейчас**, после чего откроется страница с сообщением **Перенаправление для авторизации**. После успешной авторизации откроется страница, на которой можно настроить выходные данные Data Lake Store.

3. После проверки подлинности учетной записи хранилища озера данных можно настроить свойства для выходных данных хранилища озера данных. В таблице ниже приведены имена и описание свойств для настройки выходных данных хранилища озера данных.

   ![Авторизация хранилища озера данных](./media/stream-analytics-define-outputs/07-stream-analytics-define-outputs.png)  

| Имя свойства | ОПИСАНИЕ | 
| --- | --- |
| Псевдоним выходных данных | Понятное имя, которое используется в запросах для направления выходных данных запроса в соответствующее хранилище озера данных. | 
| Имя учетной записи | Имя учетной записи хранения Data Lake, в которую отправляются выходные данные. Появится раскрывающийся список учетных записей Data Lake Store, доступных в вашей подписке. |
| Шаблон префикса пути | Путь к файлу, используемый для записи файлов в указанной учетной записи хранилища озера данных. Вы можете указать один или несколько экземпляров переменных {date} и {time}.</br><ul><li>Пример 1. folder1/logs/{дата}/{время}</li><li>Пример 2. folder1/logs/{дата}</li></ul>Если шаблон пути к файлу не содержит символ "/", то последний шаблон в пути к файлу будет рассматриваться в качестве префикса имени файла. </br></br>Новые файлы создаются в следующих ситуациях:<ul><li>изменения в схеме выходных данных;</li><li>внешний или внутренний перезапуск задания.</li></ul> |
| Формат даты | Необязательный элемент. Если в префиксе пути используется маркер даты, вы можете выбрать формат даты для упорядочивания своих файлов. Пример: ГГГГ/ММ/ДД |
|Формат времени | Необязательный элемент. Если в префиксе пути используется маркер времени, укажите формат времени для упорядочивания своих файлов. В настоящее время поддерживается только один формат — ЧЧ. |
| Формат сериализации событий | Формат сериализации для выходных данных. Поддерживаются форматы JSON, CSV и Avro.| 
| Кодирование | Если используется формат CSV или JSON, необходимо указать формат кодирования. В настоящее время единственным поддерживаемым форматом кодировки является UTF-8.|
| Разделитель | Применяется только для сериализации CSV-файлов. Служба Stream Analytics позволяет использовать ряд распространенных разделителей для сериализации данных в формате CSV. Поддерживаются такие разделители: запятая, точка с запятой, пробел, табуляция и вертикальная черта.|
| Формат | Применяется только для сериализации JSON. Вариант "строки-разделители" предусматривает форматирование выходных данных таким образом, что каждый объект JSON будет отделен новой строкой. Вариант "массив" означает, что выходные данные будут отформатированы как массив объектов JSON. Этот массив будет закрыт только в том случае, если выполнение задания будет остановлено или Stream Analytics перейдет в следующее временное окно. В общем рекомендуется использовать JSON-файл со строками-разделителями, так как для него не требуется никакой специальной обработки. При этом по-прежнему выполняется запись в выходной файл.|

### <a name="renew-data-lake-store-authorization"></a>Обновление авторизации Data Lake Store
Необходимо будет повторно аутентифицировать учетную запись Data Lake Store, если с момента создания задания или последней аутентификации пароль был изменен. Если вы не выполните повторную аутентификацию, ваше задание не будет выводить результаты, а в журналах операций будет зарегистрирована ошибка, указывающая на необходимость повторной авторизации. Сейчас существует ограничение, при котором маркер проверки подлинности необходимо обновлять вручную каждые 90 дней для всех заданий с выходными данными хранилища озера данных. 

Чтобы обновить авторизацию, **остановите** выполнение задания, перейдите к выходным данным Data Lake Store, щелкните ссылку **Обновить авторизацию**, после чего на непродолжительное время откроется страница с сообщением **Перенаправление для авторизации**. Эта страница автоматически закроется, и в случае успешного выполнения появится сообщение **Авторизация успешно обновлена**. В нижней части страницы нажмите кнопку **Сохранить**, а затем перезапустите задание со **времени последней остановки**, чтобы избежать потери данных.

![Авторизация хранилища озера данных](./media/stream-analytics-define-outputs/08-stream-analytics-define-outputs.png)  

## <a name="sql-database"></a>База данных SQL
[База данных SQL Azure](https://azure.microsoft.com/services/sql-database/) может служить местом назначения для выходных реляционных данных, а также для выходных данных приложений, которые зависят от содержимого, размещенного в реляционной базе данных. Задания Stream Analytics будут записывать данные в существующую таблицу в Базе данных SQL Azure.  Схема таблицы должна в точности соответствовать полям и их типам в выходных данных задания. [Хранилище данных SQL Azure](https://azure.microsoft.com/documentation/services/sql-data-warehouse/) также можно задать для выходных данных с помощью параметра вывода базы данных SQL. В таблице ниже приведены имена и описание свойств для создания выходных данных Базы данных SQL.

| Имя свойства | ОПИСАНИЕ |
| --- | --- |
| Псевдоним выходных данных |Понятное имя, которое используется в запросах для направления выходных данных запроса в соответствующую базу данных. |
| База данных | Имя базы данных, в которую отправляются выходные данные. |
| имя сервера; | Имя сервера базы данных SQL. |
| Имя пользователя | Имя пользователя, который имеет право на запись в базу данных. |
| Пароль | Пароль для подключения к базе данных. |
| Таблица | Имя таблицы, в которую записываются выходные данные. В имени таблицы учитывается регистр символов, а схема этой таблицы должна точно соответствовать количеству и типам полей в выходных данных задания. |

> [!NOTE]
> В настоящее время для выходных данных задания в Stream Analytics поддерживается предложение базы данных SQL Azure. Однако менее виртуальная машина Azure с SQL Server, к которой подключена база данных, не поддерживается. Это может быть изменено в будущих выпусках.
> 

## <a name="blob-storage"></a>Хранилище BLOB-объектов
Хранилище BLOB-объектов предоставляет экономичное и масштабируемое решение для хранения в облаке больших объемов неструктурированных данных.  Общие сведения о хранилище BLOB-объектов Azure и его использовании см. в статье [Приступая к работе с хранилищем BLOB-объектов Azure с помощью .NET](../storage/blobs/storage-dotnet-how-to-use-blobs.md).

В таблице ниже приведены имена и описание свойств для создания выходных данных в хранилище BLOB-объектов.

| Имя свойства | ОПИСАНИЕ | 
| --- | --- |
| Псевдоним выходных данных | Понятное имя, которое используется в запросах для направления выходных данных запроса в хранилище BLOB-объектов. |
| Учетная запись хранения | Имя учетной записи хранения, в которую отправляются выходные данные. |
| Ключ учетной записи хранения | Секретный ключ, связанный с учетной записью хранения. |
| Контейнер хранилища | Контейнеры обеспечивают логическую группировку BLOB-объектов, хранящихся в службе BLOB-объектов Microsoft Azure. При передаче BLOB-объекта в службу BLOB-объектов для него необходимо указать контейнер. |
| Шаблон пути | Необязательный элемент. Шаблон пути к файлу, используемый для записи BLOB-объектов в указанном контейнере. </br></br> Чтобы указать периодичность записи больших двоичных объектов, в шаблоне пути можно использовать один или несколько экземпляров переменных даты и времени: </br> {date}, {time} </br> </br>Если вы зарегистрировались на [предварительную версию](https://aka.ms/ASAPreview), можно также задать одно настраиваемое имя поля {field} из данных событий для секционирования больших двоичных объектов. Имя поля может содержать буквы, цифры, пробелы, дефисы и знаки подчеркивания. Существуют следующие ограничения для пользовательских полей. <ul><li>Регистр знаков не учитывается (столбец ID не отличается от столбца id).</li><li>Не допускаются вложенные поля (вместо этого используйте псевдоним в запросе задания, чтобы преобразовать поле в плоскую структуру).</li><li>В имени поля запрещено использовать выражения.</li></ul>Примеры: <ul><li>Пример 1: cluster1/logs/{date}/{time}</li><li>Пример 2: cluster1/logs/{date}</li><li>Пример 3 (предварительная версия): cluster1/{client_id}/{date}/{time}</li><li>Пример 4 (предварительная версия): cluster1/{myField}, где запрос имеет следующий вид: SELECT data.myField AS myField FROM Input;</li></ul><BR> Имя файла подчиняется следующим правилам: </br> {Шаблон префикса пути}/schemaHashcode_Guid_Number.extension </br></br> Выходные файлы примера: </br><ul><li>Myoutput/20170901/00/45434_gguid_1.csv</li><li>Myoutput/20170901/01/45434_gguid_1.csv</li></ul><br/>
| Формат даты | Необязательный элемент. Если в префиксе пути используется маркер даты, вы можете выбрать формат даты для упорядочивания своих файлов. Пример: ГГГГ/ММ/ДД |
| Формат времени | Необязательный элемент. Если в префиксе пути используется маркер времени, укажите формат времени для упорядочивания своих файлов. В настоящее время поддерживается только один формат — ЧЧ. |
| Формат сериализации событий | Формат сериализации для выходных данных.  Поддерживаются форматы JSON, CSV и Avro.
| Кодирование | Если используется формат CSV или JSON, необходимо указать формат кодирования. В настоящее время единственным поддерживаемым форматом кодировки является UTF-8. |
| Разделитель | Применяется только для сериализации CSV-файлов. Служба Stream Analytics позволяет использовать ряд распространенных разделителей для сериализации данных в формате CSV. Поддерживаются такие разделители: запятая, точка с запятой, пробел, табуляция и вертикальная черта. |
| Формат | Применяется только для сериализации JSON. Вариант "строки-разделители" предусматривает форматирование выходных данных таким образом, что каждый объект JSON будет отделен новой строкой. Вариант "массив" означает, что выходные данные будут отформатированы как массив объектов JSON. Этот массив будет закрыт только в том случае, если выполнение задания будет остановлено или Stream Analytics перейдет в следующее временное окно. В общем рекомендуется использовать JSON-файл со строками-разделителями, так как для него не требуется никакой специальной обработки. При этом по-прежнему выполняется запись в выходной файл. |

При использовании хранилища BLOB-объектов для выходных данных в большом двоичном объекте создается файл в следующих случаях:

* Если размер файла превышает максимально допустимое количество блоков (в настоящее время 50 000). Максимальное количество блоков может быть достигнуто без превышения максимально допустимого размера большого двоичного объекта. Например, при высокой скорости вывода данных в блоке будет большее число байтов, значит, и размер файла будет большим. В случае низкой скорости вывода данных в каждом блоке будет меньше данных, а значит, и размер файла будет меньшим.
* Если схема изменена в выходных данных, а для формата выходных данных требуется фиксированная схема (CSV или Avro).  
* При перезапуске задания извне пользователем, который останавливает и затем запускает его, или изнутри для обслуживания системы или восстановления после сбоя.  
* Если запрос полностью секционирован, для каждой секции выходных данных создается файл.  
* Если файл или контейнер учетной записи хранения удален пользователем.  
* Если выходные данные разделены по времени с использованием шаблона префикса пути, новый блок применяется, когда запрос переходит к следующему часу.
* Если выходные данные секционируются по пользовательскому полю, то для каждого ключа секции создается большой двоичный объект, если он не существует.
*   Если выходные данные секционируются по пользовательскому полю и кратность ключа секции превышает 8000, то для каждого ключа секции может быть создан большой двоичный объект.

## <a name="event-hub"></a>Концентратор событий
[Концентраторы событий Azure](https://azure.microsoft.com/services/event-hubs/) — это высокомасштабируемая служба приема данных о событиях публикации и подписки. Она может принимать миллионы событий в секунду. Концентратор событий может использоваться в качестве места назначения выходных данных, например в случае, когда выходные данные задания Stream Analytics становятся входными данными для другого задания потоковой передачи.

Чтобы настроить потоки данных концентраторов событий как выходные данные, требуется ряд параметров.

| Имя свойства | ОПИСАНИЕ |
| --- | --- |
| Псевдоним выходных данных | Понятное имя, которое используется в запросах для направления выходных данных запроса в концентратор событий. |
| пространство имен концентратора событий; |Пространство имен концентратора событий — это контейнер для набора сущностей обмена сообщениями. При создании концентратора событий создается также пространство имен концентратора событий. |
| имя концентратора событий; | Имя выходных данных концентратора событий. |
| Имя политики концентратора событий | Политика общего доступа, которую можно создать на вкладке с настройками концентратора событий. Каждой политике общего доступа присваивается имя, а также для нее задаются разрешения и ключи доступа. |
| Ключ политики концентратора событий | Ключ общего доступа, используемый для аутентификации доступа к пространству имен концентратора событий. |
| Столбец ключа раздела (необязательное свойство) | Этот столбец содержит ключ раздела для выходных данных концентратора событий. |
| Формат сериализации событий | Формат сериализации для выходных данных.  Поддерживаются форматы JSON, CSV и Avro. |
| Кодирование | В настоящее время единственным поддерживаемым форматом кодирования файлов CSV и JSON является UTF-8. |
| Разделитель | Применяется только для сериализации CSV-файлов. Служба Stream Analytics позволяет использовать ряд распространенных разделителей для сериализации данных в формате CSV. Поддерживаются такие разделители: запятая, точка с запятой, пробел, табуляция и вертикальная черта. |
| Формат | Применяется только для сериализации JSON. Вариант "строки-разделители" предусматривает форматирование выходных данных таким образом, что каждый объект JSON будет отделен новой строкой. Вариант "массив" означает, что выходные данные будут отформатированы как массив объектов JSON. Этот массив будет закрыт только в том случае, если выполнение задания будет остановлено или Stream Analytics перейдет в следующее временное окно. В общем рекомендуется использовать JSON-файл со строками-разделителями, так как для него не требуется никакой специальной обработки. При этом по-прежнему выполняется запись в выходной файл. |

## <a name="power-bi"></a>Power BI
[Power BI](https://powerbi.microsoft.com/) . Визуализация позволяет создавать панели мониторинга оперативных данных, формировать отчеты и составлять отчетности на основе метрик.

### <a name="authorize-a-power-bi-account"></a>Авторизация учетной записи Power BI
1. Если служба Power BI выбрана в качестве места назначения для выходных данных на портале Azure, вам будет предложено авторизовать существующего пользователя Power BI или создать новую учетную запись Power BI.  
   
   ![Авторизация пользователя Power BI](./media/stream-analytics-define-outputs/01-stream-analytics-define-outputs.png)  

2. Создайте учетную запись (если она еще не создана), а затем щелкните «Авторизовать сейчас».  Отображается приведенная ниже страница.
   
   ![Power BI в учетной записи Azure](./media/stream-analytics-define-outputs/02-stream-analytics-define-outputs.png)  

3. На этом этапе укажите рабочую или учебную учетную запись для авторизации выходных данных Power BI. Если вы еще не зарегистрировались в Power BI, выберите параметр «Зарегистрироваться сейчас». Рабочая или учебная учетная запись, которая используется для Power BI, может отличаться от учетной записи в подписке Azure, с помощью которой вы вошли в систему.

### <a name="configure-the-power-bi-output-properties"></a>Настройка свойств выходных данных Power BI
После авторизации учетной записи Power BI можно настроить свойства для выходных данных Power BI. В таблице ниже приведены имена и описание свойств для настройки выходных данных в Power BI.

| Имя свойства | description |
| --- | --- |
| Псевдоним выходных данных |Понятное имя, которое используется в запросах для направления выходных данных запроса в соответствующее место назначения в Power BI. |
| Рабочая область группы |Чтобы иметь возможность обмениваться данными с другими пользователями Power BI, вы можете выбрать группы в своей учетной записи Power BI или щелкнуть "Моя рабочая область", если не хотите записывать данные в группу.  Для обновления существующей группы требуется повторно выполнить проверку подлинности в службе Power BI. |
| Имя набора данных |Имя набора данных для использования в выходных данных Power BI. |
| Имя таблицы |Имя таблицы в наборе выходных данных Power BI. В настоящее время для вывода выходных данных из заданий Stream Analytics в Power BI можно использовать только одну таблицу в наборе данных. |

Пошаговые инструкции по настройке выходных данных и панели мониторинга Power BI приведены в статье [Stream Analytics и Power BI. Панель мониторинга для анализа потоковой передачи данных](stream-analytics-power-bi-dashboard.md).

> [!NOTE]
> Не создавайте вручную набор данных и таблицу на панели мониторинга Power BI. Они будут автоматически созданы при запуске задания, когда задание начнет вносить выходные данные в Power BI. Обратите внимание: если запрос задания не создает никаких результатов, набор данных и таблица не создаются. Обратите внимание, что если в Power BI уже есть набор данных и таблица с именем, аналогичным указанному в этом задании Stream Analytics, существующие данные будут перезаписаны.
> 

### <a name="schema-creation"></a>Создание схемы
Azure Stream Analytics создает набор данных и таблицу Power BI от имени пользователя, если он еще не создан. Во всех остальных случаях таблица обновляется с использованием новых значений. В настоящее время существует ограничение: в наборе данных может существовать только одна таблица.

### <a name="data-type-conversion-from-stream-analytics-to-power-bi"></a>Преобразование типов данных из Stream Analytics в Power BI
Azure Stream Analytics обновляет модель данных динамически во время выполнения, если меняется схема вывода. Изменяются имена и типы столбцов, а добавление или удаление столбцов отслеживается.

В этой таблице представлено преобразование [типов данных Stream Analytics](https://msdn.microsoft.com/library/azure/dn835065.aspx) в [типы данных Entity Data Model (EDM)](https://powerbi.microsoft.com/documentation/powerbi-developer-walkthrough-push-data/), используемые в Power BI, если набор данных и таблица POWER BI не созданы.

Из Stream Analytics | В Power BI
-----|-----|------------
bigint | Int64
nvarchar(max) | Строка
Datetime | DateTime
float; | Double
Record array | Тип String, постоянное значение IRecord или IArray

### <a name="schema-update"></a>Обновление схемы
Stream Analytics определяет схему модели данных на основе первого набора событий в выходных данных. Затем (при необходимости) схема модели данных обновляется для размещения входящих событий, которые могут не соответствовать исходной схеме.

Следует избегать запроса `SELECT *`, чтобы не произошло динамическое обновление схемы по строкам. Помимо потенциального влияния на производительность, это также может привести к неопределенности значения времени, затраченного на результаты. Необходимо точно выбрать те поля, которые должны отображаться на панели мониторинга Power BI. Кроме того, значения данных должны соответствовать выбранному типу данных.


Предыдущий или текущий | Int64 | Строка | DateTime | Double
-----------------|-------|--------|----------|-------
Int64 | Int64 | Строка | Строка | Double
Double | Double | Строка | Строка | Double
Строка | Строка | Строка | Строка |  | Строка | 
DateTime | Строка | Строка |  DateTime | Строка


### <a name="renew-power-bi-authorization"></a>Повторная авторизация в Power BI
Если пароль учетной записи Power BI изменился после создания задания Stream Analytics или последней аутентификации, необходимо повторить аутентификацию Stream Analytics. Если в клиенте Azure Active Directory (AAD) настроена Многофакторная идентификация (MFA), вам также потребуется каждые две недели обновлять авторизацию Power BI. Признаком этой проблемы является отсутствие выходных данных задания и наличие записи «Ошибка проверки подлинности пользователя» в журналах операций:

  ![Ошибка маркера обновления Power BI](./media/stream-analytics-define-outputs/03-stream-analytics-define-outputs.png)  

Чтобы устранить эту проблему, остановите выполнение задания и перейдите к выходным данным Power BI.  Щелкните ссылку **Обновить авторизацию** и перезапустите задание **с момента его последней остановки** во избежание потери данных.

  ![Обновление авторизации в Power BI](./media/stream-analytics-define-outputs/04-stream-analytics-define-outputs.png)  

## <a name="table-storage"></a>Хранилище таблиц
[Табличное хранилище Azure](../storage/common/storage-introduction.md) отличается высокой степенью доступности и масштабируемости, позволяя приложению автоматически осуществлять масштабирование в соответствии с нуждами пользователя. Табличное хранилище является хранилищем ключей и атрибутов NoSQL корпорации Майкрософт и позволяет работать со структурированными данными с менее жесткими ограничениями схемы. Хранилище таблиц Azure можно использовать для постоянного хранения данных и эффективного их извлечения.

В таблице ниже приведены имена и описание свойств для создания выходных данных в табличном хранилище.

| Имя свойства | description |
| --- | --- |
| Псевдоним выходных данных |Понятное имя, которое используется в запросах для направления выходных данных запроса в табличное хранилище. |
| Учетная запись хранения |Имя учетной записи хранения, в которую отправляются выходные данные. |
| Ключ учетной записи хранения |Ключ доступа, связанный с учетной записью хранения. |
| Имя таблицы |Это имя таблицы. Создается таблица, если она еще существует. |
| Ключ секции |Имя выходного столбца, содержащего ключ раздела. Ключ раздела — это уникальный идентификатор раздела в пределах конкретной таблицы, являющийся первой частью первичного ключа сущности. Это строковое значение размером до 1 КБ. |
| Ключ строки. |Имя выходного столбца, содержащего ключ строки. Ключ строки — это уникальный идентификатор сущности внутри конкретного раздела. Он является второй частью первичного ключа сущности. Ключ строки — это строковое значение размером до 1 КБ. |
| Размер пакета |Количество записей в пакетной операции. Значения по умолчанию (100) достаточно для большинства заданий. Дополнительные сведения об изменении этого параметра см. в статье о [спецификациях пакетных операций с таблицами](https://msdn.microsoft.com/library/microsoft.windowsazure.storage.table.tablebatchoperation.aspx). |
 
## <a name="service-bus-queues"></a>Очереди служебной шины
[Очереди служебной шины](https://msdn.microsoft.com/library/azure/hh367516.aspx) доставляют сообщения конкурирующим потребителям по типу FIFO (первым пришел, первым вышел). Обычно получатели принимают и обрабатывают сообщения в порядке их добавления в очередь, и каждое сообщение принимается и обрабатывается только одним потребителем сообщений.

В таблице ниже приведены имена и описание свойств для создания выходных данных в очереди.

| Имя свойства | description |
| --- | --- |
| Псевдоним выходных данных |Понятное имя, которое используется в запросах для направления выходных данных запроса в очередь служебной шины. |
| Пространство имен служебной шины |Пространство имен Service Bus — это контейнер для набора сущностей обмена сообщениями. |
| Имя очереди |Имя очереди служебной шины. |
| Имя политики очереди |При создании очереди можно также создать политики общего доступа на вкладке с настройками очереди. Каждой политике общего доступа присваивается имя, а также для нее задаются разрешения и ключи доступа. |
| Ключ политики очереди |Ключ общего доступа, используемый для проверки подлинности при получении доступа к пространству имен служебной шины. |
| Формат сериализации событий |Формат сериализации для выходных данных.  Поддерживаются форматы JSON, CSV и Avro. |
| Кодирование |В настоящее время единственным поддерживаемым форматом кодирования файлов CSV и JSON является UTF-8. |
| Разделитель |Применяется только для сериализации CSV-файлов. Служба Stream Analytics позволяет использовать ряд распространенных разделителей для сериализации данных в формате CSV. Поддерживаются такие разделители: запятая, точка с запятой, пробел, табуляция и вертикальная черта. |
| Формат |Применяется только для типа JSON. Вариант "строки-разделители" предусматривает форматирование выходных данных таким образом, что каждый объект JSON будет отделен новой строкой. Вариант "массив" означает, что выходные данные будут отформатированы как массив объектов JSON. |

Количество разделов [основано на размере и номере SKU служебной шины](../service-bus-messaging/service-bus-partitioning.md). Ключ раздела — это уникальное целое значение для каждого раздела.

## <a name="service-bus-topics"></a>Разделы шины обслуживания
Если очереди служебной шины предоставляют принцип взаимодействия "один к одному" (отправитель с получателем), то [разделы служебной шины](https://msdn.microsoft.com/library/azure/hh367516.aspx) позволяют использовать принцип взаимодействия "один ко многим".

В таблице ниже приведены имена и описание свойств для создания выходных данных в табличном хранилище.

| Имя свойства | ОПИСАНИЕ |
| --- | --- |
| Псевдоним выходных данных |Понятное имя, которое используется в запросах для направления выходных данных запроса в раздел служебной шины. |
| Пространство имен служебной шины |Пространство имен Service Bus — это контейнер для набора сущностей обмена сообщениями. При создании нового концентратора событий создается также пространство имен служебной шины. |
| Имя раздела |Разделы являются сущностями обмена сообщениями, как концентраторы событий и очереди. Они предназначены для сбора потоков событий с нескольких различных устройств и служб. Созданному разделу присваивается определенное имя. Сообщения, отправленные в раздел, будут недоступны, пока не создана подписка. Поэтому убедитесь, что раздел содержит одну или несколько подписок. |
| Имя политики раздела |При создании раздела можно также создать политики общего доступа на вкладке настройки раздела. Каждой политике общего доступа присваивается имя, а также для нее задаются разрешения и ключи доступа. |
| Ключ политики раздела |Ключ общего доступа, используемый для проверки подлинности при получении доступа к пространству имен служебной шины. |
| Формат сериализации событий |Формат сериализации для выходных данных.  Поддерживаются форматы JSON, CSV и Avro. |
| Кодирование |Если используется формат CSV или JSON, необходимо указать формат кодирования. В настоящее время единственным поддерживаемым форматом кодирования является UTF-8. |
| Разделитель |Применяется только для сериализации CSV-файлов. Служба Stream Analytics позволяет использовать ряд распространенных разделителей для сериализации данных в формате CSV. Поддерживаются такие разделители: запятая, точка с запятой, пробел, табуляция и вертикальная черта. |

Количество разделов [основано на размере и номере SKU служебной шины](../service-bus-messaging/service-bus-partitioning.md). Ключ раздела — это уникальное целое значение для каждого раздела.

## <a name="azure-cosmos-db"></a>Azure Cosmos DB
[Azure Cosmos DB](https://azure.microsoft.com/services/documentdb/) — это глобально распределенная многомодельная служба базы данных. Она предоставляет функции неограниченного гибкого масштабирования по всему миру, расширенные возможности выполнения запросов и автоматического индексирования с помощью моделей данных без использования схем, гарантированную низкую задержку, а также ведущие в отрасли полные соглашения об уровне обслуживания. Сведения о параметрах сбора данных Stream Analytics в Cosmos DB см. в статье [Выходные данные Azure Stream Analytics в Azure Cosmos DB](stream-analytics-documentdb-output.md).

> [!Note]
> В настоящее время Azure Stream Analytics поддерживает соединение только с CosmosDB при помощи **API SQL**.
> Другие API Azure Cosmos DB в данный момент не поддерживаются. Если указать модулю Azure Stream Analytics учетные записи Azure Cosmos DB, созданные при помощи других API, это может привести к неправильному сохранению данных. 

В следующей таблице описаны свойства для создания выходных данных Azure Cosmos DB.
| Имя свойства | description |
| --- | --- |
| Псевдоним выходных данных | Псевдоним для ссылки на эти выходные данные в запросе Stream Analytics. |
| Приемник | База данных Cosmos |
| Вариант импорта | Выберите один из вариантов: "Select Cosmos DB from your subscription" (Выбрать Cosmos DB из своей подписки) или "Provide Cosmos DB settings manually" (Указать параметры Cosmos DB вручную).
| Идентификатор учетной записи | Имя или универсальный код ресурса (URI) конечной точки учетной записи Cosmos DB. |
| Ключ учетной записи | Общедоступный ключ доступа к учетной записи Cosmos DB. |
| База данных | Имя базы данных Cosmos DB. |
| Шаблон имен коллекций | Имя или шаблон имени для используемых коллекций. <br/>Формат имени коллекции можно составить с помощью необязательного маркера {partition}, где разделы начинаются с 0. Два примера:  <br/>1. _MyCollection_ — должна существовать одна коллекция с именем MyCollection.  <br/>2. _MyCollection{раздел}_ — основана на столбце секционирования. <br/>В столбце секционирования должны существовать такие коллекции: MyCollection0, MyCollection1, MyCollection2 и т.д. |
| Ключ раздела | Необязательный элемент. Требуется, только если в шаблоне имени коллекции используется маркер {partition}.<br/> Ключ раздела — имя поля в выходных событиях, указывающее ключ для разделения выходных данных между коллекциями.<br/> Для выходных данных одной коллекции можно использовать любой столбец произвольных выходных данных. Например, PartitionId. |
| Идентификатор документа |Необязательный элемент. Имя поля в выходных событиях, используемое для указания первичного ключа, на котором основываются операции вставки или обновления.  

## <a name="azure-functions"></a>Функции Azure
Функции Azure — это независимая от сервера служба вычислений, которая позволяет выполнять код по требованию без необходимости явно подготавливать или администрировать инфраструктуру. Они позволяют реализовать код, который запускается событиями, возникающими в Azure или сторонних службах.  Эта возможность решения "Функции Azure" реагировать на триггеры упрощает вывод данных Azure Stream Analytics. Этот выходной адаптер позволяет пользователям подключать Stream Analytics к Функциям Azure и запускать сценарий или часть кода в ответ на ряд событий.

Azure Stream Analytics вызывает Функции Azure через триггеры HTTP. Новый адаптер выходных данных функции Azure доступен со следующими настраиваемыми свойствами:

| Имя свойства | description |
| --- | --- |
| Приложение-функция |Имя приложения-функции Azure |
| Функция |Имя функции в приложении-функции Azure |
| Ключ |Если нужно использовать службу "Функции Azure" из другой подписки это можно сделать, предоставив ключ для доступа к функции. |
| Максимальный размер пакета |Это свойство может использоваться для задания максимального размера для каждого пакета выходных данных, которые будут отправляться в Функции Azure. По умолчанию это значение равно 256 КБ. |
| Максимальное количество пакетов  |Как видно из названия, это свойство позволяет указать максимальное число событий в каждом пакете, отправляются в службу "Функции Azure". Значение максимального количества пакетов по умолчанию — 100. |

Когда служба Azure Stream Analytics получает исключение 413 (сущность запроса HTTP слишком большая) из службы "Функции Azure", размер пакетов, отправляемых в службу "Функции Azure", уменьшается. В коде функции Azure это исключение позволяет убедится, что Azure Stream Analytics не отправляет пакеты слишком большого размера. Также убедитесь, что максимальное количество пакетов и размеры значений, используемые в функции, соответствуют значениям, введенным на портале Stream Analytics. 

Кроме того, в ситуации, когда во временном окне не происходит целевое событие, никакой выход не генерируется. В результате функция computeResult не вызывается. Такое поведение согласуется со встроенными оконными агрегатными функциями.

## <a name="partitioning"></a>Секционирование

В следующей таблице указаны поддержка секционирования и количество записей выходных данных для каждого типа данных:

| Тип выходных данных | Поддержка секционирования | Ключ секции  | Количество записей выходных данных | 
| --- | --- | --- | --- |
| Хранилище озера данных Azure | Yes | Используйте токены {date} и {time} в шаблоне префикса пути. Выберите формат даты, например ГГГГ-ММ-ДД, ДД-ММ-ГГГГ, ММ-ДД-ГГГГ. Для времени используется формат ЧЧ. | Соответствует секционированию входных данных для [полностью распараллеливаемых запросов](stream-analytics-scale-jobs.md). | 
| Базы данных SQL Azure | Нет  | None | Не применяется | 
| Хранилище больших двоичных объектов Azure | Yes | Используйте в шаблоне пути токены {date} и {time} {fieldname} из полей событий. Выберите формат даты, например ГГГГ-ММ-ДД, ДД-ММ-ГГГГ, ММ-ДД-ГГГГ. Для времени используется формат ЧЧ. В рамках [предварительной версии](https://aka.ms/ASAPreview) выходные данные большого двоичного объекта можно секционировать с помощью одного атрибута настраиваемого события {fieldname}. | Соответствует секционированию входных данных для [полностью распараллеливаемых запросов](stream-analytics-scale-jobs.md). | 
| концентратору событий Azure | Yes | Yes | Изменяется в зависимости от выравнивания секций.</br> Если ключ секции выходных данных концентратора событий согласован с вышестоящим (предыдущим) шагом запроса, то число модулей записи совпадает с количеством секций выходных данных концентратора событий. Каждый модуль записи использует [класс EventHubSender](/dotnet/api/microsoft.servicebus.messaging.eventhubsender?view=azure-dotnet) EventHub для отправки событий в определенную секцию. </br> Если ключ секции выходных данных концентратора событий не согласован с вышестоящим (предыдущим) шагом запроса, то число модулей записи совпадает с количеством секций на этом предыдущем шаге. Каждый модуль записи использует [класс SendBatchAsync](https://docs.microsoft.com/en-us/dotnet/api/microsoft.servicebus.messaging.eventhubclient.sendasync?view=azure-dotnet) EventHubClient для отправки событий во все секции выходных данных. |
| Power BI | Нет  | None | Не применяется | 
| табличное хранилище Azure; | Yes | Любой выходной столбец.  | Соответствует секционированию входных данных для [полностью распараллеливаемых запросов](stream-analytics-scale-jobs.md). | 
| Раздел служебной шины Azure | Yes | Выбран автоматически. Количество разделов [основано на размере и номере SKU служебной шины](../service-bus-messaging/service-bus-partitioning.md). Ключ раздела — это уникальное целое значение для каждого раздела.| Совпадает с количеством секций в разделе выходных данных.  |
| Очередь служебной шины Azure | Yes | Выбран автоматически. Количество разделов [основано на размере и номере SKU служебной шины](../service-bus-messaging/service-bus-partitioning.md). Ключ раздела — это уникальное целое значение для каждого раздела.| Совпадает с количеством секций в очереди выходных данных. |
| Azure Cosmos DB | Yes | Используйте токен {partition} в шаблоне имени коллекции. Значение {partition} основано на предложении PARTITION BY в запросе. | Соответствует секционированию входных данных для [полностью распараллеливаемых запросов](stream-analytics-scale-jobs.md). |
| Функции Azure | Нет  | None | Не применяется | 

## <a name="output-batch-size"></a>Размер выходного пакета
Azure Stream Analytics использует пакеты переменного размера для обработки событий и записи выходных данных. Обычно модуль Stream Analytics не записывает сообщение по одному, и использует пакеты для повышения эффективности. При большой интенсивности передачи входящих и исходящих событий используются пакеты большего размера. При низкой интенсивности исходящего трафика используются пакеты меньшего размера, чтобы обеспечить низкую задержку. 

В следующей таблице описаны некоторые аспекты пакетной обработки выходных данных.

| Тип выходных данных | Максимальный размер сообщения | Оптимизация размера пакета |
| :--- | :--- | :--- | 
| Хранилище озера данных Azure | Ознакомьтесь с [ограничениями хранилища Data Lake](../azure-subscription-service-limits.md#data-lake-store-limits). | До 4 МБ на операцию записи. |
| Базы данных SQL Azure | До 10 000 строк в одной операции пакетной вставки.</br>Не меньше 100 строк в одной операции пакетной вставки. </br>Ознакомьтесь также с [ограничениями среды SQL Azure](../sql-database/sql-database-resource-limits.md). |  Изначально вставляются пакеты максимального размера. Затем пакеты могут разделяться пополам (до достижения минимального размера пакета) при наличии ошибок SQL, допускающих повторение операции. |
| Хранилище больших двоичных объектов Azure | Ознакомьтесь с [ограничениями службы хранилища Azure](../azure-subscription-service-limits.md#storage-limits). | Максимальный размер блока большого двоичного объекта — 4 МБ.</br>Максимальное число блоков большого двоичного объекта — 50 000. |
| концентратору событий Azure   | 256 КБ на сообщение. </br>Ознакомьтесь также с [ограничениями концентраторов событий](../event-hubs/event-hubs-quotas.md). |    Если секционирование входных и выходных данных не согласовано, каждое событие упаковываются отдельно в объект EventData и отправляется в пакете, размер которого равен максимальному размеру сообщения (1 МБ для номера SKU "Премиум"). </br></br>  Если секционирование входных и выходных данных согласовано, то нескольких событий упаковываются в один объект EventData, пока его размер не достигает максимального размера сообщения, после чего он отправляется.    |
| Power BI | Ознакомьтесь с [ограничениями REST API Power BI](https://msdn.microsoft.com/library/dn950053.aspx). |
| табличное хранилище Azure; | Ознакомьтесь с [ограничениями службы хранилища Azure](../azure-subscription-service-limits.md#storage-limits). | По умолчанию разрешено 100 сущностей в одной транзакции. При необходимости можно настроить меньшее значение. |
| Очередь служебной шины Azure   | 256 КБ на сообщение.</br> Ознакомьтесь также с [ограничениями служебной шины](../service-bus-messaging/service-bus-quotas.md). | Одно событие на сообщение. |
| Раздел служебной шины Azure | 256 КБ на сообщение.</br> Ознакомьтесь также с [ограничениями служебной шины](../service-bus-messaging/service-bus-quotas.md). | Одно событие на сообщение. |
| Azure Cosmos DB   | Ознакомьтесь с [ограничениями Azure Cosmos DB](../azure-subscription-service-limits.md#azure-cosmos-db-limits). | Размер пакета и частота записи настраиваются динамически на основе ответов CosmosDB. </br> Предопределенные ограничения Stream Analytics отсутствуют. |
| Функции Azure   | | Размер пакета по умолчанию равен 246 КБ. </br> Число событий в пакете по умолчанию равно 100. </br> Размер пакета можно настроить, его можно увеличить или уменьшить в [параметрах вывода](#azure-functions) Stream Analytics. 

## <a name="next-steps"></a>Дополнительная информация
> [!div class="nextstepaction"]
> [Краткое руководство по созданию задания Stream Analytics с помощью портала Azure](stream-analytics-quick-create-portal.md)

<!--Link references-->
[stream.analytics.developer.guide]: ../stream-analytics-developer-guide.md
[stream.analytics.scale.jobs]: stream-analytics-scale-jobs.md
[stream.analytics.introduction]: stream-analytics-introduction.md
[stream.analytics.get.started]: stream-analytics-real-time-fraud-detection.md
[stream.analytics.query.language.reference]: http://go.microsoft.com/fwlink/?LinkID=513299
[stream.analytics.rest.api.reference]: http://go.microsoft.com/fwlink/?LinkId=517301
