<properties 
	pageTitle="Создание входных данных Stream Analytics | Microsoft Azure" 
	description="Информация о подключении к источникам входных данных, а также их настройке для решений Stream Analytics."
	documentationCenter=""
	services="stream-analytics"
	authors="jeffstokes72" 
	manager="paulettm" 
	editor="cgronlun"/>

<tags 
	ms.service="stream-analytics" 
	ms.devlang="na" 
	ms.topic="article" 
	ms.tgt_pltfrm="na" 
	ms.workload="data-services" 
	ms.date="08/05/2015" 
	ms.author="jeffstok"/>

# Создание входных данных Stream Analytics

## Что такое входные данные Stream Analytics?
---
Под входными данными Stream Analytics понимается подключение к источнику данных. Данные, отправляемые в этот источник данных, принимаются заданием Stream Analytics и обрабатываются в режиме реального времени. Входные данные делятся на две разные группы: входные потоковые данные и входные ссылочные данные. Служба Stream Analytics обеспечивает первоклассную интеграцию с источниками входных данных (концентратором событий и хранилищем больших двоичных объектов) независимо от того, входят ли они в подписку. Поддерживаемые форматы данных: Avro, CSV и JSON.

## Входные потоковые данные
---
На базовом уровне любое определение задания в Stream Analytics должно включать хотя бы один источник входных потоковых данных, потребляемый и изменяемый этим заданием. В качестве источников входных потоковых данных могут выступать хранилище больших двоичных объектов Azure и концентраторы событий Azure. Концентраторы событий Azure как источники входных данных служат для сбора потоков событий с различных устройств и служб. В качестве примеров потоков данных можно привести ленты новостей в социальных сетях, биржевые сводки или данные с датчиков.

Для приема массовых данных можно использовать хранилище больших двоичных объектов Azure. Следует отметить, что в этом случае данные неактивны, так что Stream Analytics интерпретирует все данные в большом двоичном объекте как имеющие метку времени, которая совпадает с отметкой времени создания самого такого объекта. Это происходит в том случае, если записи в большом двоичном объекте не содержат отметки времени и не используется ключевое слово TIMESTAMP BY.

## Входные ссылочные данные
---
Служба Stream Analytics поддерживает также источники входных данных другого типа — так называемые ссылочные данные. Они представляют собой вспомогательные данные, которые обычно используются для проверки взаимосвязи и подстановки. Данные здесь, как правило, не меняются или меняются редко. Хранилище больших двоичных объектов Azure — единственный поддерживаемый источник входных ссылочных данных. Максимальный размер больших двоичных объектов в источнике ссылочных данных — 50 МБ.

Чтобы включить поддержку обновления ссылочных данных, укажите соответствующий шаблон пути во входной конфигурации, используя маркеры {date} и {time}. Задание обеспечит загрузку соответствующего большого двоичного объекта на основе даты и времени, закодированных в именах таких объектов с использованием часового пояса в формате UTC. Эта последовательность больших двоичных объектов ссылочных данных с закодированной датой и временем необходима для достижения согласованных результатов. Например, если при обработке данных возникает задержка и файл большого двоичного объекта приходится перезагружать, необходимо, чтобы этот файл оказался в том же месте и без изменений, иначе выходные данные могут измениться. Единственный поддерживаемый вариант — это добавление новых больших двоичных объектов с закодированными в пути датой и временем, которые относятся к будущему периоду.

Скажем, если задание содержит входные ссылочные данные, настроенные на портале с помощью шаблона пути (например, /sample/{date}/{time}/products.csv с форматом даты "ГГГГ-ММ-ДД" и форматом времени "ЧЧ:мм"), задание выберет объект с именем /sample/2015-04-16/17:30/products.csv в 17:30 16 апреля 2015 г. по часовому поясу в формате UTC.


> [AZURE.NOTE]В настоящее время задания Stream Analytics пытаются обновлять ссылочные данные больших двоичных объектов только в том случае, если время совпадает с временем, закодированным в имени такого большого двоичного объекта; например, объект /sample/2015-04-16/17:30/products.csv задания ищут в промежуток между 17:30 и 17:30:59.999999999 16 апреля 2015 года по часовому поясу в формате UTC. Как только часы бьют 17:31, служба перестает искать объект /sample/2015-04-16/17:30/products.csv и начинает искать /sample/2015-04-16/17:31/products.csv.

Большие двоичные объекты ссылочных данных за прошедшее время принимаются во внимание только при запуске задания. В этот момент задание ищет большой двоичный объект, в имени которого указаны последние дата и время до начала задания (самый новый большой двоичный объект ссылочных данных до момента начала задания). Это позволяет обеспечить наличие непустого набора ссылочных данных на момент начала задания. Если такой набор данных не найден, задание завершается ошибкой, а на экране отображается следующее диагностическое сообщение:

    “Initializing input without a valid reference data blob for UTC time <job start time>.”

 
## Создание потока входных данных
---
Чтобы создать поток входных данных, откройте в задании Stream Analytics вкладку **Входные данные** и нажмите кнопку **Добавить входные данные** в нижней части страницы.

![рисунок 1](./media/stream-analytics-connect-data-event-inputs/01-stream-analytics-create-inputs.png)

 При создании потока входных данных пользователь может выбрать либо [**концентратор событий**](Creating-an-Event-hub-input-data-stream), либо [**хранилище больших двоичных объектов**](Creating-a Blob-storage-input-data-stream). Выберите вариант, соответствующий типу потока данных.

![рисунок 2](./media/stream-analytics-connect-data-event-inputs/02-stream-analytics-create-inputs.png)

## Создание входного потока данных с использованием концентратора событий
---
### Общие сведения о концентраторах событий
Концентраторы событий — это высокомасштабируемые активные приемники событий, которые, как правило, используются для получения входящих событий Stream Analytics. Они предназначены для сбора потоков событий с нескольких различных устройств и служб. Концентраторы событий в сочетании со службой Stream Analytics образуют комплексное решение для аналитики в режиме реального времени. Концентраторы событий позволяют клиентам отправлять события в Azure в реальном времени, а задания Stream Analytics могут обрабатывать их в режиме реального времени. Например, клиенты могут отправлять в концентраторы событий данные о щелчках на веб-страницах, показания датчиков и сетевые события в журнале, а также создавать задания Stream Analytics, используя концентраторы событий в качестве потоков входных данных для фильтрации в режиме реального времени, выполнения статистических вычислений и присоединения. Концентраторы событий можно также использовать для вывода данных. Концентраторы событий могут использоваться в качестве источников выходных данных, если выходные данные задания Stream Analytics становятся входными данными для другого задания потоковой передачи. Дополнительные сведения о концентраторах событий см. в документации о [концентраторах событий](https://azure.microsoft.com/services/event-hubs/ "Концентраторы событий").

### Группы получателей
Каждый источник входных данных для задания Stream Analytics нужно настроить таким образом, чтобы у него была собственная группа получателей концентратора событий. Если задание включает самосоединение или несколько источников входных данных, некоторые входные данные могут последовательно считываться несколькими модулями чтения, так что общее число модулей чтения в одной группе получателей превышает предельное значение для концентратора событий (пять модулей чтения на группу получателей). В этом случае запрос нужно будет разбить на несколько запросов, а промежуточные результаты направлять через дополнительные концентраторы событий. Обратите внимание, что у каждого концентратора событий должно быть не более 20 групп получателей. Дополнительные сведения см. в [руководстве по программированию концентраторов событий](https://msdn.microsoft.com/library/azure/dn789972.aspx "Руководство по программированию концентраторов событий").

## Пример создания источника входных данных с использованием концентратора событий на портале Azure
---
Ниже приводится пошаговая инструкция по настройке источника входных данных с использованием концентратора событий. Чтобы приступить к работе с входными данными концентратора событий, пользователю необходимо получить следующие данные о концентраторе событий:

1. Псевдоним входных данных: понятное название входных данных, которое будет использоваться в запросе задания.
2. Имя пространства имен Service Bus. 
3. Имя концентратора событий.
3. Имя политики концентратора событий.
4. (Необязательно) имя группы получателей концентратора событий.
	- Группа получателей принимает данные из концентратора событий. Если она не задана, задания Stream Analytics будут использовать для приема данных из концентратора событий группу получателей по умолчанию. Для каждого задания Stream Analytics рекомендуется использовать отдельную группу получателей.
5. Для потока данных (Avro, CSV, JSON) используется формат сериализации.

Сначала выберите параметр **ДОБАВИТЬ ВХОДНЫЕ ДАННЫЕ** на странице входных данных задания Stream Analytics.

![рисунок 1](./media/stream-analytics-connect-data-event-inputs/01-stream-analytics-create-inputs.png)

Выберите в качестве источника входных данных концентратор событий.

![рисунок 6](./media/stream-analytics-connect-data-event-inputs/06-stream-analytics-create-inputs.png)

Заполните параметры концентратора событий, как указано ниже:

![рисунок 7](./media/stream-analytics-connect-data-event-inputs/07-stream-analytics-create-inputs.png)

Проверьте правильность формата сериализации для потока данных.

![рисунок 8](./media/stream-analytics-connect-data-event-inputs/08-stream-analytics-create-inputs.png)

Нажмите **Готово**. После этого поток входных данных концентратора событий будет создан.

## Создание потока входных данных с использованием хранилища больших двоичных объектов
---
Хранилище больших двоичных объектов служит экономичным и масштабируемым решением в случаях, связанных с хранением больших объемов неструктурированных данных в облаке. Эти данные обычно называются неактивными. Данный вариант пригодится, например, в случае анализа журналов, когда журналы уже получены из систем и теперь их нужно проанализировать и извлечь значимые данные. Другой пример применения — анализ неактивных данных складского учета. Дополнительные сведения о хранилище больших двоичных объектов см. в документации по [хранилищу больших двоичных объектов](http://azure.microsoft.com/services/storage/blobs/).

Ниже приводится пошаговая инструкция по настройке хранилища больших двоичных объектов в качестве источника входных данных. Чтобы начать использовать хранилище больших двоичных объектов как источник входных данных, необходимо получить следующие данные:

1. Псевдоним входных данных: понятное название входных данных, которое будет использоваться в запросе задания.
2. Если учетная запись хранения относится не к той подписке, в которую входит задание потоковой передачи, потребуются имя и ключ учетной записи хранения.
3. Имя контейнера.
4. Префикс имени файла.
5. Формат сериализации, используемый для потока данных (Avro, CSV, JSON).

На вкладке входных данных задания Stream Analytics нажмите **ДОБАВИТЬ ВХОДНЫЕ ДАННЫЕ** и выберите параметр по умолчанию **Поток данных**.![рисунок 1](./media/stream-analytics-connect-data-event-inputs/01-stream-analytics-create-inputs.png)

Затем выберите **хранилище больших двоичных объектов**.

![рисунок 2](./media/stream-analytics-connect-data-event-inputs/02-stream-analytics-create-inputs.png)

Заполните параметры учетной записи хранения, как указано ниже:

![image3](./media/stream-analytics-connect-data-event-inputs/03-stream-analytics-create-inputs.png)

> [AZURE.NOTE]Щелкнув поле "Настроить дополнительные параметры", вы сможете указать шаблон префикса пути для чтения больших двоичных объектов в настраиваемом пути. Если это поле не заполнено, Stream Analytics будет считывать все большие двоичные объекты в контейнере.

![image4](./media/stream-analytics-connect-data-event-inputs/04-stream-analytics-create-inputs.png)

Выберите соответствующий параметр сериализации данных. Возможные варианты: JSON, CSV и Avro.

![рисунок 5](./media/stream-analytics-connect-data-event-inputs/05-stream-analytics-create-inputs.png)

Нажмите **Готово**. После этого поток данных хранилища больших двоичных объектов будет создан.

## Создание потока входных ссылочных данных с использованием хранилища больших двоичных объектов
---
Для реализации функциональных возможностей ссылочных данных можно использовать хранилище больших двоичных объектов.

Ниже приводится пошаговая инструкция по настройке хранилища больших двоичных объектов в качестве источника входных ссылочных данных. Для начала вам потребуются следующие данные:

1. Псевдоним входных данных: понятное название входных данных, которое будет использоваться в запросе задания.
2. Если учетная запись хранения относится не к той подписке, в которую входит задание потоковой передачи, потребуются имя и ключ учетной записи хранения.
3. Имя контейнера.
4. Префикс имени файла.
5. Формат сериализации, используемый для потока данных (CSV, JSON).
6. Шаблон пути. Путь к файлу, используемый для поиска больших двоичных объектов в указанном контейнере. В пути можно указать один или несколько экземпляров следующих двух переменных: {date} и {time}.


На вкладке входных данных задания Stream Analytics нажмите **ДОБАВИТЬ ВХОДНЫЕ ДАННЫЕ** и выберите параметр по умолчанию **Ссылочные данные**.

![image9](./media/stream-analytics-connect-data-event-inputs/09-stream-analytics-create-inputs.png)

Заполните параметры хранилища больших двоичных объектов и учетной записи хранения, как указано ниже:

![image10](./media/stream-analytics-connect-data-event-inputs/10-stream-analytics-create-inputs.png)

Прокрутите экран вниз и укажите шаблона префикса для иерархии пути, который указывает на большой двоичный объект, а также формат полей даты и времени.

![image12](./media/stream-analytics-connect-data-event-inputs/12-stream-analytics-create-inputs.png)

Выберите соответствующий параметр сериализации данных. Возможные варианты: JSON, CSV и Avro.

![image11](./media/stream-analytics-connect-data-event-inputs/11-stream-analytics-create-inputs.png)

Нажмите **Готово**. После этого поток входных ссылочных данных хранилища больших двоичных объектов будет создан.


## Получение справки
Дополнительную помощь и поддержку вы можете получить на нашем [форуме Azure Stream Analytics](https://social.msdn.microsoft.com/Forums/ru-ru/home?forum=AzureStreamAnalytics).

## Дальнейшие действия

- [Введение в Azure Stream Analytics](stream-analytics-introduction.md)
- [Приступая к работе с Azure Stream Analytics](stream-analytics-get-started.md)
- [Масштабирование заданий в службе Azure Stream Analytics](stream-analytics-scale-jobs.md)
- [Справочник по языку запросов Azure Stream Analytics](https://msdn.microsoft.com/library/azure/dn834998.aspx)
- [Справочник по API-интерфейсу REST управления Stream Analytics](https://msdn.microsoft.com/library/azure/dn835031.aspx)

<!---HONumber=August15_HO6-->