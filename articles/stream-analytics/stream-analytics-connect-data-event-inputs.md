<properties 
	pageTitle="Подключение входных данных Stream Analytics | Microsoft Azure" 
	description="Информация о подключении к источникам входных данных, а также их настройке для решений Stream Analytics."
	documentationCenter=""
	services="stream-analytics"
	authors="jeffstokes72" 
	manager="paulettm" 
	editor="cgronlun"/>

<tags 
	ms.service="stream-analytics" 
	ms.devlang="na" 
	ms.topic="article" 
	ms.tgt_pltfrm="na" 
	ms.workload="data-services" 
	ms.date="11/23/2015" 
	ms.author="jeffstok"/>

# Подключение источника входных данных Stream Analytics

## Основные сведения о входных данных Stream Analytics
---
Под входными данными Stream Analytics понимается подключение к источнику данных. Служба Stream Analytics обеспечивает первоклассную интеграцию с источниками данных Azure (концентратором событий и хранилищем BLOB-объектов) независимо от того, входят ли они в подписку. Данные, отправляемые в этот источник данных, принимаются заданием Stream Analytics и обрабатываются в режиме реального времени. Входные данные делятся на два типа: входные потоковые данные и входные справочные данные.

## Входные потоковые данные
---
Задания Stream Analytics должны включать в себя как минимум один поток входных данных, потребляемый и изменяемый заданием. В качестве источников входных потоковых данных могут выступать хранилище больших двоичных объектов Azure и концентраторы событий Azure. Концентраторы событий Azure используются для сбора потоков событий из нескольких устройств и служб, таких как ленты новостей социальных сетей, сведения о торговле акциями или данные датчиков. Для приема массовых данных можно использовать хранилище больших двоичных объектов Azure.

## Входные ссылочные данные
---
Stream Analytics также поддерживает второй тип входных данных — справочные данные. Они представляют собой вспомогательные данные, которые обычно используются для проверки взаимосвязи и подстановки. Данные здесь, как правило, не меняются или меняются редко. Хранилище больших двоичных объектов Azure — единственный поддерживаемый источник входных ссылочных данных. Максимальный размер больших двоичных объектов в источнике ссылочных данных — 50 МБ.

## Создание входного потока данных с использованием концентратора событий
---
### Общие сведения о концентраторах событий
Концентраторы событий — это высокомасштабируемые активные приемники событий, наиболее часто используемые для приема данных в заданиях Stream Analytics. Концентраторы событий в сочетании со службой Stream Analytics образуют комплексное решение для аналитики в режиме реального времени. Концентраторы событий позволяют клиентам отправлять события в Azure в реальном времени, а задания Stream Analytics могут обрабатывать их в режиме реального времени. Например, клиенты могут отправлять в концентраторы событий данные о щелчках на веб-страницах, показания датчиков и сетевые события в журнале, а также создавать задания Stream Analytics, используя концентраторы событий в качестве потоков входных данных для фильтрации в режиме реального времени, выполнения статистических вычислений и присоединения. Концентраторы событий можно также использовать для вывода данных. Дополнительные сведения о концентраторах событий см. в документации о [концентраторах событий](https://azure.microsoft.com/services/event-hubs/ "Концентраторы событий").

### Группы получателей
Каждый концентратор событий Stream Analytics нужно настроить таким образом, чтобы у него была собственная группа получателей. Если задание включает самосоединение или несколько источников входных данных, некоторые входные данные могут последовательно считываться несколькими модулями чтения, что влияет на количество модулей чтения в группе получателей. Чтобы не превысить лимит на количество модулей чтения для концентратора событий (5 на каждую группу получателей в разделе), рекомендуется назначить группу получателей для каждого задания Stream Analytics. Обратите внимание, что у каждого концентратора событий должно быть не более 20 групп получателей. Дополнительные сведения см. в [руководстве по программированию концентраторов событий](https://msdn.microsoft.com/library/azure/dn789972.aspx "Руководство по программированию концентраторов событий").

## Создание потока входных данных с помощью концентратора событий
---
### Добавление концентратора событий в качестве потока входных данных  ###

1. На вкладке входных данных задания Stream Analytics щелкните **ДОБАВИТЬ ВХОДНЫЕ ДАННЫЕ**, выберите параметр по умолчанию **Поток данных** и нажмите кнопку справа.

    ![рисунок 1](./media/stream-analytics-connect-data-event-inputs/01-stream-analytics-create-inputs.png)

2. Затем выберите **Концентратор событий**.

    ![рисунок 6](./media/stream-analytics-connect-data-event-inputs/06-stream-analytics-create-inputs.png)

3. Введите или выберите данные в указанных ниже полях, после чего нажмите кнопку справа.

    - **Входной псевдоним**: понятное имя, которое будет использоваться в запросе задания для ссылки на эти входные данные.  
    - **Пространство имен Service Bus**: контейнер для набора сущностей обмена сообщениями. При создании нового концентратора событий создается также пространство имен Service Bus.  
    - **Концентратор событий**: имя вашего концентратора событий.  
    - **Имя политики концентратора событий**: политика общего доступа, которую можно создать на вкладке "Настройка концентратора событий". Каждой политике общего доступа присваивается имя, разрешения и ключи доступа.  
    - **Группа пользователей концентратора событий** (необязательно): принимает данные из концентратора событий. Если она не задана, задания Stream Analytics будут использовать для приема данных из концентратора событий группу получателей по умолчанию. Для каждого задания Stream Analytics рекомендуется использовать отдельную группу получателей.  

    ![рисунок 7](./media/stream-analytics-connect-data-event-inputs/07-stream-analytics-create-inputs.png)

4. Укажите следующие параметры:

    - **Формат сериализации событий**: чтобы запросы работали как следует, в задании Stream Analytics нужно указать, какой формат сериализации (JSON, CSV или Avro) используется для потоков входящих данных.  
    - **Кодировка**: в настоящее время единственным поддерживаемым форматом кодировки является UTF-8.  

    ![рисунок 8](./media/stream-analytics-connect-data-event-inputs/08-stream-analytics-create-inputs.png)

5. Нажмите кнопку с галочкой, чтобы завершить работу мастера и убедится, что Stream Analytics может успешно подключаться к концентратору событий.

## Создание потока входных данных с помощью хранилища BLOB-объектов
---
Хранилище больших двоичных объектов служит экономичным и масштабируемым решением в случаях, связанных с хранением больших объемов неструктурированных данных в облаке. Данные в хранилище BLOB-объектов обычно считаются неактивными, но служба Stream Analytics может обрабатывать их как поток данных. Один из типичных сценариев обработки входных данных хранилища BLOB-объектов с помощью Stream Analytics — это обработка журнала, при которой данные телеметрии собираются из системы, а затем анализируются и обрабатываются для получения значимых данных. Важно отметить, что по умолчанию отметка времени событий хранилища BLOB-объектов в Stream Analytics — это отметка времени создания BLOB-объекта. Для обработки данных как потока с помощью отметки времени в полезной нагрузке события необходимо использовать ключевое слово [TIMESTAMP BY](https://msdn.microsoft.com/library/azure/dn834998.aspx). Дополнительные сведения о хранилище BLOB-объектов см. в документации по [хранилищу BLOB-объектов](http://azure.microsoft.com/services/storage/blobs/).

### Добавление хранилища BLOB-объектов в качестве потока входных данных  ###

1. На вкладке входных данных задания Stream Analytics щелкните **ДОБАВИТЬ ВХОДНЫЕ ДАННЫЕ**, выберите параметр по умолчанию **Поток данных** и нажмите кнопку справа.

    ![рисунок 1](./media/stream-analytics-connect-data-event-inputs/01-stream-analytics-create-inputs.png)

2. Выберите пункт **Хранилище BLOB-объектов** и нажмите кнопку справа.

    ![рисунок 2](./media/stream-analytics-connect-data-event-inputs/02-stream-analytics-create-inputs.png)

3. Введите или выберите данные в следующих полях:

    - **Входной псевдоним**: понятное имя, которое будет использоваться в запросе задания для ссылки на эти входные данные.  
    - **Учетная запись хранения**: если учетная запись хранения относится не к той подписке, в которую входит задание потоковой передачи, потребуются имя и ключ учетной записи хранения.  
    - **Контейнер хранилища**: контейнеры обеспечивают логическую группировку BLOB-объектов, хранящихся в службе BLOB-объектов Microsoft Azure. При передаче BLOB-объекта в службу BLOB-объектов для него необходимо указать контейнер.  

    ![image3](./media/stream-analytics-connect-data-event-inputs/03-stream-analytics-create-inputs.png)

4. Щелкните поле **Настроить дополнительные параметры**, чтобы указать шаблон префикса пути для чтения BLOB-объектов в настраиваемом пути. Если это поле не заполнено, Stream Analytics будет считывать все большие двоичные объекты в контейнере.

    ![image4](./media/stream-analytics-connect-data-event-inputs/04-stream-analytics-create-inputs.png)

5. Выберите следующие параметры:

    - **Формат сериализации событий**: чтобы запросы работали как следует, в задании Stream Analytics нужно указать, какой формат сериализации (JSON, CSV или Avro) используется для потоков входящих данных.  
    - **Кодировка**: в настоящее время единственным поддерживаемым форматом кодировки является UTF-8.  


    ![рисунок 5](./media/stream-analytics-connect-data-event-inputs/05-stream-analytics-create-inputs.png)

6. Нажмите кнопку с галочкой, чтобы завершить работу мастера и убедится, что Stream Analytics может успешно подключаться к учетной записи хранилища BLOB-объектов.

## Создание справочных данных с помощью хранилища BLOB-объектов
---
Хранилище BLOB-объектов можно использовать как источник справочных данных для задания Stream Analytics. Это статические или медленно меняющиеся данные, используемые для выполнения поиска или сопоставления данных. Чтобы включить поддержку обновления справочных данных, укажите соответствующий шаблон пути во входной конфигурации, используя маркеры {date} и {time}. На основе этого шаблона пути служба Stream Analytics будет обновлять определения справочных данных. Например, шаблон `"/sample/{date}/{time}/products.csv"` с форматом даты "ГГГГ-ММ-ДД" и форматом времени "ЧЧ:ММ" указывает службе Stream Analytics выбрать обновленный BLOB-объект `"/sample/2015-04-16/17:30/products.csv"` 16 апреля 2015 г. в 17:30 в часовом поясе UTC.

> [AZURE.NOTE]В настоящее время задания Stream Analytics пытаются обновлять ссылочные данные больших двоичных объектов только в том случае, если время совпадает с временем, закодированным в имени такого большого двоичного объекта; например, объект /sample/2015-04-16/17:30/products.csv задания ищут в промежуток между 17:30 и 17:30:59.999999999 16 апреля 2015 года по часовому поясу в формате UTC. Как только часы бьют 17:31, служба перестает искать объект /sample/2015-04-16/17:30/products.csv и начинает искать /sample/2015-04-16/17:31/products.csv.

Большие двоичные объекты ссылочных данных за прошедшее время принимаются во внимание только при запуске задания. В этот момент задание ищет большой двоичный объект, в имени которого указаны последние дата и время до начала задания (самый новый большой двоичный объект ссылочных данных до момента начала задания). Это позволяет обеспечить наличие непустого набора ссылочных данных на момент начала задания. Если такой набор данных не найден, задание завершается ошибкой, а на экране отображается следующее диагностическое сообщение:

### Добавление хранилища BLOB-объектов в качестве источника справочных данных  ###

1. На вкладке входных данных задания Stream Analytics нажмите **ДОБАВИТЬ ВХОДНЫЕ ДАННЫЕ**, выберите параметр по умолчанию **Справочные данные** и нажмите кнопку справа.

    ![image9](./media/stream-analytics-connect-data-event-inputs/09-stream-analytics-create-inputs.png)

2.	Введите или выберите данные в следующих полях:

    - **Входной псевдоним**: понятное имя, которое будет использоваться в запросе задания для ссылки на эти входные данные.  
    - **Учетная запись хранения**: если учетная запись хранения относится не к той подписке, в которую входит задание потоковой передачи, потребуются имя и ключ учетной записи хранения.  
    - **Контейнер хранилища**: контейнеры обеспечивают логическую группировку BLOB-объектов, хранящихся в службе BLOB-объектов Microsoft Azure. При передаче BLOB-объекта в службу BLOB-объектов для него необходимо указать контейнер.  
    - **Путь к файлу**: путь к файлу, используемый для поиска BLOB-объектов в указанном контейнере. В пути можно указать один или несколько экземпляров следующих двух переменных: {date} и {time}.  

    ![image10](./media/stream-analytics-connect-data-event-inputs/10-stream-analytics-create-inputs.png)

3. Выберите следующие параметры:

    - **Формат сериализации событий**: чтобы запросы работали как следует, в задании Stream Analytics нужно указать, какой формат сериализации (JSON, CSV или Avro) используется для потоков входящих данных.  
    - **Кодировка**: в настоящее время единственным поддерживаемым форматом кодировки является UTF-8.  

    ![image12](./media/stream-analytics-connect-data-event-inputs/12-stream-analytics-create-inputs.png)

4.	Нажмите кнопку с галочкой, чтобы завершить работу мастера и убедится, что Stream Analytics может успешно подключаться к учетной записи хранилища BLOB-объектов.

    ![image11](./media/stream-analytics-connect-data-event-inputs/11-stream-analytics-create-inputs.png)


## Получение справки
Дополнительную помощь и поддержку вы можете получить на нашем [форуме Azure Stream Analytics](https://social.msdn.microsoft.com/Forums/ru-RU/home?forum=AzureStreamAnalytics).

## Дальнейшие действия

- [Введение в Azure Stream Analytics](stream-analytics-introduction.md)
- [Приступая к работе с Azure Stream Analytics](stream-analytics-get-started.md)
- [Масштабирование заданий в службе Azure Stream Analytics](stream-analytics-scale-jobs.md)
- [Справочник по языку запросов Azure Stream Analytics](https://msdn.microsoft.com/library/azure/dn834998.aspx)
- [Справочник по API-интерфейсу REST управления Stream Analytics](https://msdn.microsoft.com/library/azure/dn835031.aspx)

<!---HONumber=AcomDC_1125_2015-->