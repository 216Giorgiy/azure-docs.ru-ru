<properties
	pageTitle="Подключение данных: потоки входных данных из потока событий | Microsoft Azure"
	description="Узнайте больше о настройке подключения данных к Stream Analytics, которые называются входными. К входным данным относятся поток данных из событий, а также справочные данные."
	keywords="поток данных, подключение данных, поток событий"
	services="stream-analytics"
	documentationCenter=""
	authors="jeffstokes72"
	manager="jhubbard"
	editor="cgronlun"/>

<tags
	ms.service="stream-analytics"
	ms.devlang="na"
	ms.topic="article"
	ms.tgt_pltfrm="na"
	ms.workload="data-services"
	ms.date="09/26/2016"
	ms.author="jeffstok"/>

# Подключение данных: узнайте о потоках входных данных из событий в Stream Analytics

Подключения данных в Stream Analytics — это поток данных событий из источника данных. Он называется входным. Stream Analytics полностью интегрируется с источниками потоков данных Azure — концентратором событий, центром IoT и хранилищем BLOB-объектов — расположенными в той же подписке Azure, что и задание аналитики, или в другой.

## Типы входных данных: поток данных и справочные данные
Данные, отправляемые в источник данных, принимаются заданием Stream Analytics и обрабатываются в режиме реального времени. Входные данные делятся на два типа: входные потоковые данные и входные справочные данные.

### Входные потоковые данные
Поток данных — это несвязанная последовательность событий, поступающих с течением времени. Задания Stream Analytics должны включать в себя как минимум один поток входных данных, потребляемый и изменяемый заданием. В качестве источников входных потоковых данных могут выступать хранилище больших двоичных объектов, концентраторы событий и центры IoT. Концентраторы событий используются для сбора потоков событий из нескольких устройств и служб, таких как ленты новостей социальных сетей, сведения о торговле акциями или данные датчиков. Центры IoT оптимизированы для сбора данных с подключенных устройств в сценариях «Интернет вещей» (IoT). Для приема массовых данных в качестве потока можно использовать хранилище больших двоичных объектов.

### Ссылочные данные
Stream Analytics также поддерживает второй тип входных данных — справочные данные. Они представляют собой вспомогательные данные, которые не меняются вообще или меняются редко. Обычно эти данные используются для определения взаимосвязей и подстановки. Хранилище больших двоичных объектов Azure — единственный поддерживаемый источник входных эталонных данных. Максимальный размер больших двоичных объектов в источнике данных ссылок — 100 МБ. Сведения о том, как создать ссылку на входные данные, см. в статье [Использование ссылочных данных](stream-analytics-use-reference-data.md).

## Создание потока входных данных с помощью концентратора событий

[Концентраторы событий Azure](https://azure.microsoft.com/services/event-hubs/) — это высокомасштабируемая служба приема данных о событиях публикации и подписки. Она может обрабатывать миллионы событий в секунду, позволяя вам обрабатывать и анализировать огромное количество данных, создаваемых подключенными устройствами и приложениями. Она является одним из наиболее часто используемых источников входных данных для службы Stream Analytics. Благодаря концентраторам событий и службе Stream Analytics клиенты получают комплексное решение для анализа данных в режиме реального времени. Концентраторы событий позволяют клиентам отправлять события в Azure в реальном времени, где задания Stream Analytics обрабатывают их тоже в реальном времени. Например, клиенты могут отправлять в концентраторы событий сведения о щелчках на веб-страницах, показания датчиков и данные журнала сетевых событий, предварительно создав задания Stream Analytics, которые используют концентраторы событий в качестве потоков входных данных для фильтрации в режиме реального времени, выполнения статистических вычислений и определения взаимосвязей.

Обратите внимание, что по умолчанию меткой времени при поступлении событий из концентраторов в Stream Analytics является метка времени поступления события в концентратор, то есть EventEnqueuedUtcTime. Для обработки данных как потока с помощью метки времени в полезных данных события необходимо использовать ключевое слово [TIMESTAMP BY](https://msdn.microsoft.com/library/azure/dn834998.aspx).

### Группы получателей

Каждый концентратор событий Stream Analytics нужно настроить таким образом, чтобы у него была собственная группа получателей. Если задание включает самосоединение или несколько источников входных данных, некоторые входные данные могут последовательно считываться несколькими модулями чтения, что влияет на количество модулей чтения в группе получателей. Чтобы не превысить лимит на количество модулей чтения для концентратора событий (5 на каждую группу получателей в разделе), рекомендуется назначить группу получателей для каждого задания Stream Analytics. Обратите внимание, что у каждого концентратора событий должно быть не более 20 групп получателей. Дополнительные сведения см. в [руководстве по программированию концентраторов событий](../event-hubs/event-hubs-programming-guide.md).

### Настройка концентратора событий в качестве потока входных данных

В таблице ниже приводится описание каждого свойства, доступного на вкладке входных данных концентратора событий.

| ИМЯ СВОЙСТВА | ОПИСАНИЕ |
|------|------|
| Псевдоним входных данных | Понятное имя, с помощью которого запрос задания будет ссылаться на эти входные данные. |
| Пространство имен служебной шины | Пространство имен Service Bus — это контейнер для набора сущностей обмена сообщениями. При создании нового концентратора событий создается также пространство имен Service Bus. |
| Концентратор событий | Имя входных данных концентратора событий. |
| Имя политики концентратора событий | Политика общего доступа, которую можно создать на вкладке с настройками концентратора событий. Каждой политике общего доступа присваивается имя, разрешения и ключи доступа. |
| Ключ политики концентратора событий | Ключ общего доступа, используемый для проверки подлинности при получении доступа к пространству имен служебной шины. |
| Группа пользователей концентратора событий (необязательное свойство) | Группа получателей, принимающих данные из концентратора событий. Если она не задана, задания Stream Analytics будут использовать для приема данных из концентратора событий группу получателей по умолчанию. Для каждого задания Stream Analytics рекомендуется использовать отдельную группу получателей. |
| Формат сериализации событий | Чтобы запросы работали как следует, в задании Stream Analytics нужно указать, какой формат сериализации (JSON, CSV или Avro) используется для потоков входящих данных. |
| Кодирование | В настоящее время единственным поддерживаемым форматом кодирования является UTF-8. |

Если данные поступают из источника, которым выступают концентраторы событий, запрос Stream Analytics может получить доступ к нескольким полям метаданных. В таблице ниже перечислены такие поля и их описание.

| СВОЙСТВО | ОПИСАНИЕ |
|------|------|
| EventProcessedUtcTime | Дата и время обработки события службой Stream Analytics. |
| EventEnqueuedUtcTime | Дата и время получения события концентраторами событий. |
| PartitionId | Идентификатор секции для входного адаптера (нумерация идет от нуля). |

Вот пример запроса:

````
SELECT
	EventProcessedUtcTime,
	EventEnqueuedUtcTime,
	PartitionId
FROM Input
````

## Создание потока входных данных центра IoT

Центр Azure IoT — это высокомасштабируемая служба приема данных о событиях публикации и подписки, оптимизированная под сценарии "Интернет вещей". Обратите внимание, что по умолчанию меткой времени при поступлении событий из центров IoT в Stream Analytics является метка времени поступления события в центр IoT, то есть EventEnqueuedUtcTime. Для обработки данных как потока с помощью метки времени в полезных данных события необходимо использовать ключевое слово [TIMESTAMP BY](https://msdn.microsoft.com/library/azure/dn834998.aspx).

> [AZURE.NOTE] Обработать можно только отправленные сообщения, имеющие свойство DeviceClient.

### Группы получателей

Каждый центр IoT Stream Analytics нужно настроить таким образом, чтобы у него была собственная группа получателей. Если задание включает самосоединение или несколько источников входных данных, некоторые входные данные могут последовательно считываться несколькими модулями чтения, что влияет на количество модулей чтения в группе получателей. Чтобы не превысить лимит на количество модулей чтения для центра IoT (5 на каждую группу получателей в разделе), рекомендуется назначить группу получателей для каждого задания Stream Analytics.

### Настройка центра IoT как потока входных данных

В таблице ниже приводится описание каждого свойства, доступного на вкладке входных данных центра IoT.

| ИМЯ СВОЙСТВА | ОПИСАНИЕ |
|------|------|
| Псевдоним входных данных | Понятное имя, с помощью которого запрос задания будет ссылаться на эти входные данные. |
| Центр IoT | Центр IoT — это контейнер для набора сущностей обмена сообщениями. |
| Конечная точка | Имя конечной точки центра IoT. |
| Имя политики общего доступа | Политика общего доступа для предоставления доступа к центру IoT. Каждой политике общего доступа присваивается имя, разрешения и ключи доступа. |
| Ключ политики общего доступа | Ключ общего доступа, используемый для проверки подлинности при получении доступа к центру IoT. |
| Группа получателей (необязательное свойство) | Группа получателей, принимающих данные из центра IoT. Если она не задана, задания Stream Analytics будут использовать для приема данных из центра IoT группу получателей по умолчанию. Для каждого задания Stream Analytics рекомендуется использовать отдельную группу получателей. |
| Формат сериализации событий | Чтобы запросы работали как следует, в задании Stream Analytics нужно указать, какой формат сериализации (JSON, CSV или Avro) используется для потоков входящих данных. |
| Кодирование | В настоящее время единственным поддерживаемым форматом кодирования является UTF-8. |

Если данные поступают из источника, которым выступают центры IoT, запрос Stream Analytics может получить доступ к нескольким полям метаданных. В таблице ниже перечислены такие поля и их описание.

| СВОЙСТВО | ОПИСАНИЕ |
|------|------|
| EventProcessedUtcTime | Дата и время обработки события. |
| EventEnqueuedUtcTime | Дата и время получения события центром IoT. |
| PartitionId | Идентификатор секции для входного адаптера (нумерация идет от нуля). |
| IoTHub.MessageId | Используется для корреляции двустороннего обмена данными в центре IoT. |
| IoTHub.CorrelationId | Используется в ответах на сообщения и отзывах в центре IoT. |
| IoTHub.ConnectionDeviceId | Идентификатор, прошедший проверку подлинности, который используется для отправки этого сообщения, помещается в сообщения, связанные со службой, центром IoT. |
| IoTHub.ConnectionDeviceGenerationId | Идентификатор создания устройства, прошедшего проверку подлинности, используемый для отправки этого сообщения, помещается в сообщения, связанные со службой, центром IoT. |
| IoTHub.EnqueuedTime | Время, когда центр IoT получил сообщение. |
| IoTHub.StreamId | Настраиваемое свойство события, добавленное устройством отправителя. |

## Создание потока входных данных с помощью хранилища BLOB-объектов

Хранилище больших двоичных объектов служит экономичным и масштабируемым решением в случаях, связанных с хранением больших объемов неструктурированных данных в облаке. Данные в [хранилище больших двоичных объектов](https://azure.microsoft.com/services/storage/blobs/) обычно считаются неактивными, но служба Stream Analytics может обрабатывать их как поток данных. Один из типичных сценариев обработки входных данных хранилища BLOB-объектов с помощью Stream Analytics — это обработка журнала, при которой данные телеметрии собираются из системы, а затем анализируются и обрабатываются для получения значимых данных.

Важно отметить, что по умолчанию временем событий хранилища больших двоичных объектов в службе Stream Analytics считается время последнего изменения большого двоичного объекта (*BlobLastModifiedUtcTime*). Для обработки данных как потока с помощью метки времени в полезных данных события необходимо использовать ключевое слово [TIMESTAMP BY](https://msdn.microsoft.com/library/azure/dn834998.aspx).

Также учтите, что для входных данных в формате CSV **необходимо**, чтобы строка заголовка определяла поля для набора данных. Дополнительные поля заголовка строки должны быть **уникальными**.

> [AZURE.NOTE] Stream Analytics не поддерживает добавление содержимого в существующий большой двоичный объект. Stream Analytics только один раз просмотрит большой двоичный объект, и любые изменения, внесенные после этого, обрабатываться не будут. Рекомендуется передать все данные сразу и не добавлять какие-либо дополнительные события в хранилище BLOB-объектов.

В таблице ниже приводится описание каждого свойства, доступного на вкладке входных данных хранилища BLOB-объектов.

<table>
<tbody>
<tr>
<td>ИМЯ СВОЙСТВА</td>
<td>ОПИСАНИЕ</td>
</tr>
<tr>
<td>Псевдоним входных данных</td>
<td>Понятное имя, с помощью которого запрос задания будет ссылаться на эти входные данные.</td>
</tr>
<tr>
<td>Учетная запись хранения</td>
<td>Имя учетной записи хранения, в которой находятся файлы больших двоичных объектов.</td>
</tr>
<tr>
<td>Ключ учетной записи хранения</td>
<td>Секретный ключ, связанный с учетной записью хранения.</td>
</tr>
<tr>
<td>Контейнер хранилища
</td>
<td>Контейнеры обеспечивают логическую группировку BLOB-объектов, хранящихся в службе BLOB-объектов Microsoft Azure. При передаче BLOB-объекта в службу BLOB-объектов для него необходимо указать контейнер.</td>
</tr>
<tr>
<td>Шаблон префикса пути (необязательное свойство)</td>
<td>Путь к файлу, используемый для поиска больших двоичных объектов в указанном контейнере. В пути можно указать один или несколько экземпляров следующих трех переменных:<BR>{date}, {time},<BR>{partition}<BR>Пример&#160;1: cluster1/logs/{date}/{time}/{partition}<BR>Пример&#160;2: cluster1/logs/{date}<P>Обратите внимание, что символ * не разрешен в префиксе пути. Допустимыми являются только <a HREF="https://msdn.microsoft.com/library/azure/dd135715.aspx">символы больших двоичных объектов Azure</a>.</td>
</tr>
<tr>
<td>Формат даты (необязательное свойство)</td>
<td>Если в префиксе пути используется маркер даты, вы можете выбрать формат даты для упорядочивания своих файлов. Пример: ГГГГ/ММ/ДД</td>
</tr>
<tr>
<td>Формат времени (необязательное свойство)</td>
<td>Если в префиксе пути используется маркер времени, укажите формат времени для упорядочивания своих файлов. В настоящее время поддерживается только один формат&#160;— ЧЧ.</td>
</tr>
<tr>
<td>Формат сериализации событий</td>
<td>Чтобы запросы работали как следует, в задании Stream Analytics нужно указать, какой формат сериализации (JSON, CSV или Avro) используется для потоков входящих данных.</td>
</tr>
<tr>
<td>Кодирование</td>
<td>В настоящее время единственным поддерживаемым форматом кодирования файлов CSV и JSON является UTF-8.</td>
</tr>
<tr>
<td>Разделитель</td>
<td>Служба Stream Analytics позволяет использовать ряд распространенных разделителей для сериализации данных в формате CSV. Поддерживаются такие разделители: запятая, точка с запятой, пробел, табуляция и вертикальная черта.</td>
</tr>
</tbody>
</table>

Если данные поступают из хранилища больших двоичных объектов, запрос Stream Analytics может получить доступ к нескольким полям метаданных. В таблице ниже перечислены такие поля и их описание.

| СВОЙСТВО | ОПИСАНИЕ |
|------|------|
| BlobName | Имя входного большого двоичного объекта, от которого поступило событие. |
| EventProcessedUtcTime | Дата и время обработки события службой Stream Analytics. |
| BlobLastModifiedUtcTime | Дата и время последнего изменения большого двоичного объекта. |
| PartitionId | Идентификатор секции для входного адаптера (нумерация идет от нуля). |

Вот пример запроса:

````
SELECT
	BlobName,
	EventProcessedUtcTime,
	BlobLastModifiedUtcTime
FROM Input
````


## Получение справки
Дополнительную помощь и поддержку вы можете получить на нашем [форуме Azure Stream Analytics](https://social.msdn.microsoft.com/Forums/ru-RU/home?forum=AzureStreamAnalytics).

## Дальнейшие действия
Вы ознакомились с параметрами подключения данных для заданий Stream Analytics в Azure. Дополнительные сведения о службе Stream Analytics см. в следующих статьях:

- [Приступая к работе с Azure Stream Analytics](stream-analytics-get-started.md)
- [Масштабирование заданий в службе Azure Stream Analytics](stream-analytics-scale-jobs.md)
- [Справочник по языку запросов Azure Stream Analytics](https://msdn.microsoft.com/library/azure/dn834998.aspx)
- [Справочник по API-интерфейсу REST управления Stream Analytics](https://msdn.microsoft.com/library/azure/dn835031.aspx)

<!--Link references-->
[stream.analytics.developer.guide]: ../stream-analytics-developer-guide.md
[stream.analytics.scale.jobs]: stream-analytics-scale-jobs.md
[stream.analytics.introduction]: stream-analytics-introduction.md
[stream.analytics.get.started]: stream-analytics-get-started.md
[stream.analytics.query.language.reference]: http://go.microsoft.com/fwlink/?LinkID=513299
[stream.analytics.rest.api.reference]: http://go.microsoft.com/fwlink/?LinkId=517301

<!---HONumber=AcomDC_0928_2016-->