<properties 
	pageTitle="Определение входных данных | Microsoft Azure" 
	description="Основные сведения о входных данных Stream Analytics" 
	keywords="big data analytics,cloud service,internet of things,managed service,stream processing,streaming analytics,streaming data"
	services="stream-analytics" 
	documentationCenter="" 
	authors="jeffstokes72" 
	manager="paulettm" 
	editor="cgronlun"/>

<tags 
	ms.service="stream-analytics" 
	ms.devlang="na" 
	ms.topic="article" 
	ms.tgt_pltfrm="na" 
	ms.workload="data-services" 
	ms.date="09/09/2015" 
	ms.author="jeffstok"/>

# Основные сведения о входных данных Stream Analytics

Под входными данными Azure Stream Analytics понимается подключение к источнику данных. Служба Stream Analytics имеет первоклассную интеграцию с источниками данных Azure (концентратором событий и хранилищем BLOB-объектов) независимо от того, входят ли они в подписку Azure, в которой выполняется ваше задание. Данные, отправляемые в этот источник данных, принимаются заданием Stream Analytics и обрабатываются в режиме реального времени. Входные данные делятся на два типа: входные потоковые данные и входные справочные данные.

## Поток входных данных

Поток данных — это несвязанная последовательность событий, поступающих с течением времени. Задания Stream Analytics должны включать в себя как минимум один поток входных данных, потребляемый и изменяемый заданием. В качестве источников входных потоковых данных могут выступать хранилище больших двоичных объектов Azure и концентраторы событий Azure. Концентраторы событий Azure используются для сбора потоков событий из нескольких устройств и служб, таких как ленты новостей социальных сетей, сведения о торговле акциями или данные датчиков. Хранилище больших двоичных объектов Azure также можно использовать как источник входных данных для приема больших массивов данных в виде потока.

## Входные ссылочные данные

Stream Analytics также поддерживает второй тип входных данных — ссылочные данные. Они представляют собой вспомогательные данные, которые не меняются вообще или меняются редко. Обычно эти данные используются для определения взаимосвязей и подстановки. Хранилище больших двоичных объектов Azure — единственный поддерживаемый источник входных ссылочных данных. Максимальный размер больших двоичных объектов в источнике ссылочных данных — 50 МБ.

Сведения о том, как создать ссылку на входные данные, см. в статье [Использование ссылочных данных](./articles/stream-analytics-use-reference-data.md).

## Создание входного потока данных с использованием концентратора событий

[Концентраторы событий](https://azure.microsoft.com/services/event-hubs/) — это высокомасштабируемая служба приема данных о событиях публикации и подписки. Она может обрабатывать миллионы событий в секунду, позволяя вам обрабатывать и анализировать огромное количество данных, создаваемых подключенными устройствами и приложениями. Она является одним из наиболее часто используемых источников входных данных для службы Stream Analytics. Благодаря концентраторам событий и службе Stream Analytics клиенты получают комплексное решение для анализа данных в режиме реального времени. Концентраторы событий позволяют клиентам отправлять события в Azure в реальном времени, где задания Stream Analytics обрабатывают их тоже в реальном времени. Например, клиенты могут отправлять в концентраторы событий сведения о щелчках на веб-страницах, показания датчиков и данные журнала сетевых событий, предварительно создав задания Stream Analytics, которые используют концентраторы событий в качестве потоков входных данных для фильтрации в режиме реального времени, выполнения статистических вычислений и определения взаимосвязей.

Обратите внимание, что по умолчанию отметкой времени при поступлении событий из концентратора в Stream Analytics является отметка времени поступления события в концентратор, то есть *EventEnqueuedUtcTime*. Для обработки данных как потока с помощью отметки времени в полезных данных события необходимо использовать ключевое слово [TIMESTAMP BY](https://msdn.microsoft.com/library/azure/dn834998.aspx).

## Группы получателей

Каждый концентратор событий Stream Analytics нужно настроить таким образом, чтобы у него была собственная группа получателей. Если задание включает самосоединение или несколько источников входных данных, некоторые входные данные могут последовательно считываться несколькими модулями чтения, что влияет на количество модулей чтения в группе получателей. Чтобы не превысить лимит на количество модулей чтения для концентратора событий (5 на каждую группу получателей в разделе), рекомендуется назначить группу получателей для каждого задания Stream Analytics. Обратите внимание, что у каждого концентратора событий должно быть не более 20 групп получателей. Дополнительные сведения см. в [руководстве по программированию концентраторов событий](./articles/event-hubs-programming-guide.md).

## Настройка концентратора событий в качестве потока входных данных

В таблице ниже приводится описание каждого свойства, доступного на вкладке входных данных концентратора событий.

| Имя свойства | Описание |
|-------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Псевдоним входных данных | Понятное имя, с помощью которого запрос задания будет ссылаться на эти входные данные. |
| Пространство имен служебной шины | Пространство имен Service Bus — это контейнер для набора сущностей обмена сообщениями. При создании нового концентратора событий создается также пространство имен служебной шины. |
| Концентратор событий | Имя входных данных концентратора событий. |
| Имя политики концентратора событий | Политика общего доступа, которую можно создать на вкладке с настройками концентратора событий. Каждой политике общего доступа присваивается имя, а также задаются разрешения и ключи доступа. |
| Ключ политики концентратора событий | Ключ общего доступа, используемый для проверки подлинности при получении доступа к пространству имен служебной шины. |
| Группа пользователей концентратора событий (необязательное свойство) | Группа получателей, принимающих данные из концентратора событий. Если она не задана, задания Stream Analytics будут использовать для приема данных из концентратора событий группу получателей по умолчанию. Для каждого задания службы Stream Analytics рекомендуется использовать отдельную группу пользователей. |
| Формат сериализации событий | Чтобы запросы работали как следует, в задании Stream Analytics нужно указать, какой формат сериализации (JSON, CSV или Avro) используется для потоков входящих данных. |
| Кодирование | В настоящее время единственным поддерживаемым форматом кодирования является UTF-8. |

Если данные поступают из источника, которым выступают концентраторы событий, запрос Stream Analytics может получить доступ к нескольким полям метаданных. В таблице ниже перечислены такие поля и их описание.

| Свойство | Описание |
|------------------------------|--------------------------------------------------------------------|
| EventProcessedUtcTime | Дата и время обработки события службой Stream Analytics. |
| EventEnqueuedUtcTime | Дата и время получения события концентраторами событий. |
| PartitionId | Идентификатор раздела для входного адаптера (нумерация идет от нуля) |

Вот пример запроса:


    SELECT
    	EventProcessedUtcTime,
    	EventEnqueuedUtcTime,
    	PartitionId
    FROM Input

## Создание потока входных данных с помощью хранилища BLOB-объектов

Хранилище больших двоичных объектов служит экономичным и масштабируемым решением в случаях, связанных с хранением больших объемов неструктурированных данных в облаке. Данные в [хранилище BLOB-объектов](http://azure.microsoft.com/services/storage/blobs/) обычно считаются неактивными, но служба Stream Analytics может обрабатывать их как поток данных. Один из типичных сценариев обработки входных данных хранилища BLOB-объектов с помощью Stream Analytics — это обработка журнала, при которой данные телеметрии собираются из системы, а затем анализируются и обрабатываются для получения значимых данных.

Важно отметить, что по умолчанию временем событий хранилища BLOB-объектов в службе Stream Analytics считается время последнего изменения BLOB-объекта (*BlobLastModifiedUtcTime*). Для обработки данных как потока с помощью отметки времени в полезных данных события необходимо использовать ключевое слово [TIMESTAMP BY](https://msdn.microsoft.com/library/azure/dn834998.aspx).

## Настройка хранилища BLOB-объектов в качестве потока входных данных

В таблице ниже приводится описание каждого свойства, доступного на вкладке входных данных хранилища BLOB-объектов.

<table>
<tbody>
<tr>
<td>Имя свойства</td>
<td>Описание</td>
</tr>
<tr>
<td>Псевдоним входных данных</td>
<td>Понятное имя, с помощью которого запрос задания будет ссылаться на эти входные данные.</td>
</tr>
<tr>
<td>Учетная запись хранения</td>
<td>Имя учетной записи хранения, в которой находятся файлы больших двоичных объектов.</td>
</tr>
<tr>
<td>Ключ учетной записи хранения</td>
<td>Секретный ключ, связанный с учетной записью хранения.</td>
</tr>
<tr>
<td>Контейнер хранилища</td>
<td>Контейнеры обеспечивают логическую группировку BLOB-объектов, хранящихся в службе BLOB-объектов Microsoft Azure. При передаче BLOB-объекта в службу BLOB-объектов для него необходимо указать контейнер.</td>
</tr>
<tr>
<td>Шаблон префикса пути (необязательное свойство)</td>
<td>Путь к файлу, используемый для поиска больших двоичных объектов в указанном контейнере.<BR>В пути можно указать один или несколько экземпляров следующих трех переменных:<BR>{date}, {time} и {partition}<BR>Пример&#160;1: cluster1/logs/{date}/{time}/{partition}<BR>Пример&#160;2: cluster1/logs/{date}</td>
</tr>
<tr>
<td>Формат даты (необязательное свойство)</td>
<td>Если в префиксе пути используется маркер даты, вы можете выбрать формат даты для упорядочивания своих файлов. Пример: ГГГГ/ММ/ДД</td>
</tr>
<tr>
<td>Формат времени (необязательное свойство)</td>
<td>Если в префиксе пути используется маркер времени, укажите формат времени для упорядочивания своих файлов. В настоящее время поддерживается только один формат&#160;— ЧЧ.</td>
</tr>
<tr>
<td>Формат сериализации событий</td>
<td>Чтобы запросы работали как следует, в задании Stream Analytics нужно указать, какой формат сериализации (JSON, CSV или Avro) используется для потоков входящих данных.</td>
</tr>
<tr>
<td>Кодирование</td>
<td>В настоящее время единственным поддерживаемым форматом кодирования файлов CSV и JSON является UTF-8.</td>
</tr>
<tr>
<td>Разделитель</td>
<td>Служба Stream Analytics позволяет использовать ряд распространенных разделителей для сериализации данных в формате CSV. Поддерживаются такие разделители: запятая, точка с запятой, пробел, табуляция и вертикальная черта.</td>
</tr>
</tbody>
</table>

Если данные поступают из источника, которым выступает хранилище BLOB-объектов, запрос Stream Analytics может получить доступ к нескольким полям метаданных. В таблице ниже перечислены такие поля и их описание.

| Свойство | Описание |
|--------------------------------|--------------------------------------------------------------------|
| BlobName | Имя входного большого двоичного объекта, от которого поступило событие. |
| EventProcessedUtcTime | Дата и время обработки события службой Stream Analytics. |
| BlobLastModifiedUtcTime | Дата и время последнего изменения большого двоичного объекта |
| PartitionId | Идентификатор раздела для входного адаптера (нумерация идет от нуля) |

Вот пример запроса:


    SELECT
    	BlobName,
    	EventProcessedUtcTime,
    	BlobLastModifiedUtcTime
    FROM Input


## Получение справки
Дополнительную помощь и поддержку вы можете получить на нашем [форуме Azure Stream Analytics](https://social.msdn.microsoft.com/Forums/RU-RU/home?forum=AzureStreamAnalytics).

## Дальнейшие действия
Вы получили основные сведения о Stream Analytics, управляемой службе аналитики потоковой передачи данных из Интернета вещей. Дополнительные сведения об этой службе см. на следующих ресурсах:

- [Приступая к работе с Azure Stream Analytics](stream-analytics-get-started.md)
- [Масштабирование заданий в службе Azure Stream Analytics](stream-analytics-scale-jobs.md)
- [Справочник по языку запросов Azure Stream Analytics](https://msdn.microsoft.com/library/azure/dn834998.aspx)
- [Справочник по API-интерфейсу REST управления Stream Analytics](https://msdn.microsoft.com/library/azure/dn835031.aspx)

<!--Link references-->
[stream.analytics.developer.guide]: ../stream-analytics-developer-guide.md
[stream.analytics.scale.jobs]: stream-analytics-scale-jobs.md
[stream.analytics.introduction]: stream-analytics-introduction.md
[stream.analytics.get.started]: stream-analytics-get-started.md
[stream.analytics.query.language.reference]: http://go.microsoft.com/fwlink/?LinkID=513299
[stream.analytics.rest.api.reference]: http://go.microsoft.com/fwlink/?LinkId=517301

<!---HONumber=Sept15_HO4-->