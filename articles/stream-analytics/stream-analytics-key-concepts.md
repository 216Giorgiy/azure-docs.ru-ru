<properties 
	pageTitle="Знакомство с основными понятиями Stream Analytics | Microsoft Azure" 
	description="Ознакомьтесь с основными понятиями Azure Stream Analytics — компонентами задания Stream Analytics, в том числе со сведениями о поддерживаемых входных и выходных данных, конфигурации задания и метриках." 
	keywords="event processing,data stream,key concepts,serialization"	
	services="stream-analytics" 
	documentationCenter="" 
	authors="jeffstokes72" 
	manager="paulettm" 
	editor="cgronlun" />

<tags 
	ms.service="stream-analytics" 
	ms.devlang="na" 
	ms.topic="article" 
	ms.tgt_pltfrm="na" 
	ms.workload="data-services" 
	ms.date="09/09/2015" 
	ms.author="jeffstok" />


# Основные понятия Stream Analytics: руководство по основам работы с заданиями Stream Analytics 

Azure Stream Analytics — это полностью управляемая служба, обеспечивающая малые задержки, высокий уровень доступности и масштабируемости, а также возможности сложной обработки событий для потока данных в облаке. Stream Analytics дает клиентам возможность настраивать задания потоковой передачи данных для анализа потоков данных и выполнять его в режиме, близком к режиму реального времени. В этой статье описываются основные понятия, связанные с заданиями Stream Analytics.

## Возможности Stream Analytics
В Stream Analytics поддерживаются перечисленные ниже функциональные возможности.

- Сложная обработка событий в потоках больших объемов данных, передающихся с большой скоростью.   
- Сбор данных о событиях с глобально распределенных ресурсов или оборудования, например с подключенных к сети автомобилей или из сетей коммунальных служб. 
- Обработка данных телеметрии для мониторинга и диагностики в режиме, близком к режиму реального времени. 
- Захват и архивирование событий в реальном времени для последующей обработки

Дополнительные сведения см. в статье [Введение в Azure Stream Analytics](stream-analytics-introduction.md).

Задание Stream Analytics включает следующие компоненты: - один или несколько источников входных данных; - запрос относительно потока входных данных; - целевые выходные данные.


## Входные данные

### Поток данных

В каждом определении задания Stream Analytics должен содержаться как минимум один входной источник потока данных, потребляемый и изменяемый заданием. [Хранилище больших двоичных объектов Azure](http://azure.microsoft.com/documentation/services/storage/) и [концентраторы событий Azure](http://azure.microsoft.com/services/event-hubs/) поддерживаются как источники входных потоковых данных. Входные источники концентраторов событий используются для сбора потоков событий из различных устройств и служб, в то время как хранилище больших двоичных объектов может быть входным источником для приема данных большого объема. Так как большие двоичные объекты не выполняют потоковую передачу данных, задания Stream Analytics, работающие с такими объектами, не будут по своей природе временными, если только в большом двоичном объекте нет меток времени.

### Ссылочные данные
Служба Stream Analytics также поддерживает входные источники второго типа: ссылочные данные. Это вспомогательные данные, используемые для проверки взаимосвязи и подстановки. Данные здесь, как правило, не меняются или меняются редко. [Хранилище больших двоичных объектов](http://azure.microsoft.com/documentation/services/storage/) — единственный поддерживаемый входной источник для ссылочных данных. Максимальный размер больших двоичных объектов в источнике ссылочных данных — 50 МБ.

Чтобы включить поддержку обновления ссылочных данных, необходимо указать список больших двоичных объектов во входной конфигурации, вставив маркеры {date} и {time} в шаблон пути. Задание обеспечит загрузку соответствующего большого двоичного объекта на основе даты и времени, закодированных в именах таких объектов с использованием часового пояса в формате UTC.

Например, если задание включает ссылочные входные данные, настроенные на портале с помощью шаблона пути (например, /sample/{date}/{time}/products.csv с форматом даты "ГГГГ-ММ-ДД" и форматом времени "ЧЧ: мм"), задание выберет файл /sample/2015-04-16/17:30/products.csv в 17:30, 16 апреля 2015 г. по часовому поясу в формате UTC (что эквивалентно 10:30, 16 апреля 2015 г. по часовому поясу в формате PST).


### Сериализация
Чтобы обеспечить правильное поведение запросов, Stream Analytics должен учитывать формат сериализации, применяемый к входящим потокам данных. В настоящее время поддерживаются следующие форматы сериализации: JSON, CSV и Avro для потоков данных и CSV или JSON для ссылочных данных.

### Создаваемые свойства
В зависимости от используемого в задании входного типа будет создано несколько дополнительных полей с метаданными событий. Эти поля можно использовать в запросах так же, как и другие входные столбцы. Если в существующем событии есть поле с таким же именем, как одно из указанных ниже свойств, оно будет перезаписано входными метаданными.

<table border="1">
	<tr>
		<th></th>
		<th>Свойство</th>
		<th>Описание</th>
	</tr>
	<tr>
		<td rowspan="4" valign="top"><strong>Большой двоичный объект</strong></td>
		<td>BlobName</td>
		<td>Имя входного большого двоичного объекта, от которого поступило событие.</td>
	</tr>
	<tr>
		<td>EventProcessedUtcTime</td>
		<td>Дата и время обработки записи большого двоичного объекта.</td>
	</tr>
	<tr>
		<td>BlobLastModifiedUtcTime</td>
		<td>Дата и время последнего изменения большого двоичного объекта.</td>
	</tr>
	<tr>
		<td>PartitionId</td>
		<td>Идентификатор секции для входного адаптера (нумерация идет от нуля).</td>
	</tr>
	<tr>
		<td rowspan="3" valign="top"><b>Концентратор событий</b></td>
		<td>EventProcessedUtcTime</td>
		<td>Дата и время обработки события.</td>
	</tr>
	<tr>
		<td>EventEnqueuedUtcTime</td>
		<td>Дата и время получения события концентраторами событий.</td>
	</tr>
	<tr>
		<td>PartitionId</td>
		<td>Идентификатор секции для входного адаптера (нумерация идет от нуля).</td>
	</tr>
</table>

### Секции с медленно записываемыми входными данными или без входных данных.
Если данные считываются из входных источников, включающих несколько секций, и при этом в одной или нескольких секциях наблюдается задержка или нет данных, заданию потоковой передачи необходимо решить, как устранить эту проблему для дальнейшей поддержки потока событий в системе. Входной параметр "Максимально допустимая задержка прибытия" управляет этими действиями и по умолчанию ожидает поступления данных в течение неопределенного времени. Это значит, что метки времени событий не будут изменены, однако при этом события будут проходить с учетом скорости потока в самой медленной входной секции, останавливаясь в случае отсутствия данных в одной или нескольких входных секциях. Это полезно, если данные равномерно распределяются между входными секциями и крайне важно обеспечить согласованность событий по времени.

Кроме того, вы можете выбрать ожидание данных только в течение ограниченного периода. В этом случае параметр "Максимально допустимая задержка прибытия" определяет задержку, после которой задание будет продолжено. При этом запаздывающие входные секции пропускаются, а задание реагирует на события в соответствии со значением параметра "Действие с поздними событиями", удаляя события или изменяя их метки времени, если данные поступают позже. Это полезно, если задержка имеет первостепенное значение и при этом допускается смещение меток времени, однако входные данные могут распределяться неравномерно.

### Секции с неупорядоченными событиями
Если в запросе задания потоковой передачи используется ключевое слово TIMESTAMP BY, невозможно гарантировать порядок поступления событий во входной источник. Некоторые события в одной входной секции могут отставать. Параметр "Максимально допустимое нарушение порядка во входном источнике" определяет действия задания потоковой передачи в отношении событий, которые нарушают установленный порядок, в соответствии со значением параметра "Действие с поздними событиями", удаляя события или изменяя их метки времени.

### Дополнительные ресурсы
Дополнительные сведения о создании входных источников см. в [руководстве по программированию концентраторов событий](http://msdn.microsoft.com/library/azure/dn789972.aspx) и статье [Использование хранилища BLOB-объектов из .NET](../storage/storage-dotnet-how-to-use-blobs.md).



## Запрос
Логические операции фильтрации и обработки входных данных задаются в запросах заданий Stream Analytics. Запросы создаются на языке запросов Stream Analytics, SQL-подобном языке, во многом представляющем собой подмножество стандартного синтаксиса T-SQL с некоторыми специфическими расширениями для временных запросов.

### Оконное расширение
Оконные расширения позволяют выполнять агрегирование и вычисление над подмножествами событий, попадающими в определенный период времени. Функции оконного расширения вызываются с помощью инструкции **GROUP BY**. Например, следующий запрос подсчитывает количество событий, полученное в секунду:

	SELECT Count(*) 
	FROM Input1 
	GROUP BY TumblingWindow(second, 1) 

### Шаги выполнения
При составлении более сложных запросов можно использовать стандартное SQL-предложение **WITH**, чтобы определить временное именованный результирующий набор. Например, предложение **WITH** используется в этом запросе, чтобы выполнить преобразование в два этапа:
 
	WITH step1 AS ( 
		SELECT Avg(Reading) as avr 
		FROM temperatureInput1 
		GROUP BY Building, TumblingWindow(hour, 1) 
	) 

	SELECT Avg(avr) AS campus_Avg 
	FROM step1 
	GROUP BY TumblingWindow (day, 1) 

Дополнительные сведения о языке запросов см. в [справочнике по языку запросов Azure Stream Analytics](http://go.microsoft.com/fwlink/?LinkID=513299).

## Выходные данные
В назначение выходных данных записываются результаты выполнения задания Stream Analytics. Результаты постоянно записываются в назначение выходных данных по мере обработки заданием входных событий. Поддерживаются следующие назначения выходных данных.

- Концентраторы событий Azure. Если нужно объединить несколько потоковых конвейеров (например, для ответной выдачи команд устройствам), выберите концентратор событий в качестве назначения выходных данных.
- Хранилище больших двоичных объектов Azure. Используйте хранилище больших двоичных объектов для долгосрочного архивирования выходных данных или хранения данных для последующей обработки.
- Хранилище таблиц Azure. Оно представляет собой хранилище структурированных данных с меньшим числом ограничений для схемы. В одной таблице Azure можно хранить сущности разных типов, использующие различные схемы. Хранилище таблиц Azure можно использовать для постоянного хранения данных и эффективного их извлечения. Дополнительные сведения см. в статьях [Введение в хранилище Azure](../storage/storage-introduction.md) и [Проектирование стратегии масштабируемого секционирования для хранилища таблиц Azure](https://msdn.microsoft.com/library/azure/hh508997.aspx).
- База данных SQL Azure. Это назначение выходных данных подходит для реляционных данных или приложений, которые зависят от содержимого, размещенного в базе данных.

## Единицы потоковой передачи
Для обеспечения более прогнозируемой производительности в Azure Stream Analytics используются единицы потоковой передачи, которые представляют необходимые для выполнения задания ресурсы и требуемую мощность. Единицы потоковой передачи предоставляют способ описания относительной мощности обработки события, основываясь на измерении загрузки ЦП, памяти и скорости чтения и записи. Каждая единица потоковой передачи соответствует пропускной способности около 1 МБ/с. Каждое задание Azure Stream Analytics требует не менее одной единицы потоковой передачи; одна единица – это используемое по умолчанию для всех заданий значение. Дополнительные сведения о выборе корректного числа единиц потоковой передачи для задания см. в разделе [Задания по масштабированию в Azure Stream Analytics](stream-analytics-scale-jobs.md)

## Задания масштабирования

Метрика процентного использования единиц потоковой передачи данных, приведенная ниже, является индикатором необходимости масштабирования задания Azure Stream Analytics. Высокий процент использования единиц потоковой передачи данных может быть результатом использования длительного периода запросов, событий большого размера во входных данных и неупорядоченного диапазона отклонений – или всего вышеперечисленного вместе. Во избежание этого возможно предпринять следующее: секционировать запрос или разбить его на несколько этапов, а также добавить дополнительные единицы потоковой передачи данных на вкладке «Масштаб»

Базовое использование ресурсов может наблюдаться даже без входных событий, поскольку система потребляет определенное количество ресурсов. Объем ресурсов, потребляемых системой, может со временем меняться.

Дополнительные сведения см. в статье [Масштабирование заданий в службе Azure Stream Analytics](stream-analytics-scale-jobs.md).


## Мониторинг заданий и устранение неполадок в них

### Учетная запись хранения регионального мониторинга

Чтобы отслеживать задания в службе Stream Analytics, вам необходимо назначить учетную запись хранения Azure для мониторинга данных в каждом регионе, содержащем задания Stream Analytics. Этот процесс производится при создании задания.

### Метрики
Для мониторинга использования и производительности заданий Stream Analytics доступны следующие метрики.

- Процент использования единиц потоковой передачи является индикатором относительной мощности обработки событий для одного или нескольких этапов запроса. Как только этот индикатор достигнет 80 % и более существует высокая вероятность приостановки или отсрочки обработки события.
- Ошибки. Количество сообщений об ошибках, вызванных заданием Stream Analytics.
- Входные события. Объем данных, полученных заданием Stream Analytics, измеренный счетчиком событий.
- Выходные события. Объем данных, отправляемых заданием Stream Analytics в выходной источник, измеренный счетчиком событий.
- Неупорядоченные события. Количество событий, полученных в неактуальное время, которые были удалены или получили откорректированную метку времени в соответствии с политикой в отношении неупорядоченных событий.
- Ошибки преобразования данных. Количество ошибок преобразования данных, вызванных заданием Stream Analytics.

### Журналы операций
Отладку и диагностику заданий Stream Analytics лучше всего осуществлять с помощью журналов операций Azure. Журналы операций можно открыть в разделе **Службы управления** портала. Чтобы просмотреть журналы своего задания, задайте для параметра **Тип службы** значение **Stream Analytics**, а для параметра **Имя службы** — имя задания.


## Управление заданиями 

### Запуск и остановка заданий
При запуске задания вам будет предложено указать значение параметра **Начало вывода**, который определяет, когда это задание начнет выдавать результирующие выходные данные. Если связанный запрос включает в себя окно, задание начнет получать входные данные из входных источников ввода в начале требуемого периода окна, чтобы подготовить первое выходное событие к указанному времени. Доступны три параметра: **Время запуска задания**, **Пользовательский** и **Время последней остановки**. По умолчанию задается параметр **Время запуска задания**. Если задание временно остановлено, для параметра **Время последней остановки** рекомендуется установить значение **Начало вывода**, чтобы возобновить выполнение задания с момента последнего вывода и избежать потери данных. При выборе параметра **Пользовательский** вам необходимо указать дату и время. Этот параметр полезен для указания объема используемых исторических данных из входных источников и для приема данных в конкретное время.

### Настройка заданий
Вы сможете настроить следующие параметры верхнего уровня для задания Stream Analytics.

- **Начало вывода**. Используйте этот параметр, чтобы указать, когда это задание начнет выдавать результирующие выходные данные. Если связанный запрос включает в себя окно, задание начнет получать входные данные из входных источников в начале требуемого периода, чтобы подготовить первое выходное событие к указанному времени. Доступны два параметра: **Время запуска задания** и **Пользовательский**. По умолчанию задается параметр **Время запуска задания**. При выборе параметра **Пользовательский** вам необходимо указать дату и время. Этот параметр полезен для указания объема используемых данных из входных источников и для приема данных из конкретного времени: например, с момента последней остановки задания. 
- **Неупорядоченная политика**: параметры для обработки событий, которые не поступают в задание Stream Analytics последовательно. Вы можете указать временной порог, в пределах которого события будут переупорядочены, задав диапазон отклонений, а также указать действие, применяемое к событиям за пределами этого диапазона: **Удалить** или **Изменить**. Если выбрано действие **Удалить**, будут удалены все события, полученные в неверном порядке. Если выбрано действие **Изменить**, будут внесены изменения в систему. Отношение метки времени неупорядоченных событий к метке времени последнего полученного упорядоченного события. 
- **Политика в отношении позднего прибытия**. Если данные считываются из входных источников, включающих несколько секций, и при этом в одной или нескольких секциях наблюдается задержка или нет данных, заданию потоковой передачи необходимо решить, как устранить эту проблему для дальнейшей поддержки потока событий в системе. Входной параметр "Максимально допустимая задержка прибытия" управляет этими действиями и по умолчанию ожидает поступления данных в течение неопределенного времени. Это значит, что метки времени событий не будут изменены, однако при этом события будут проходить с учетом скорости потока в самой медленной входной секции, останавливаясь в случае отсутствия данных в одной или нескольких входных секциях. Это полезно, если данные равномерно распределяются между входными секциями и крайне важно обеспечить согласованность событий по времени. Пользователь также может решить ожидать данные только в течение ограниченного времени. В этом случае параметр "Максимально допустимая задержка прибытия" определяет задержку, после которого задание будет продолжено. При этом запаздывающие входные секции пропускаются, а задание реагирует на события в соответствии со значением параметра "Действие с поздними событиями", удаляя события или изменяя их метки времени, если данные поступают позже. Это полезно, если задержка имеет первостепенное значение и при этом допускается смещение меток времени, однако входные данные могут распределяться неравномерно.
- **Локаль**. Этот параметр используется для указания приоритета интернационализации для задания Stream Analytics. Хотя метки времени данных не зависят от локали, данные параметры влияют на анализ, сравнение и сортировку данных заданием. В предварительной версии поддерживается только локаль **ru-RU**.

### Состояние

Состояние заданий Stream Analytics можно проверить на портале Azure. Исполняемые задания могут находиться в одном из двух состояний: **Выполняется** или **Деградация**. Ниже приведены определения всех этих состояний.

- **Выполняется**. Задание выделено, обрабатываются входные данные, или входные данные ожидают обработки. Если задание задерживается в состоянии выполнения и не выдает выходные данные, то, скорее всего, это обусловлено большим временным окном, выделенным на обработку данных, или сложностью логических операций в запросе. Другой причиной может быть то, что в настоящий момент задание не получает данных.
- **Деградация**. Это состояние указывает, что при выполнении задания Stream Analytics возникла одна из следующих ошибок: ошибка связи с входными или выходными данными, ошибка запроса или ошибка во время выполнения повторной попытки. Чтобы определить тип ошибок, которые возникли при выполнении задания, просмотрите журналы операций.


## Получение поддержки
За дополнительной помощью обращайтесь на наш [форум Azure Stream Analytics](https://social.msdn.microsoft.com/Forums/ru-RU/home?forum=AzureStreamAnalytics).


## Дальнейшие действия

Теперь, когда вы познакомились с основными понятиями Stream Analytics, прочитайте указанные ниже статьи:

- [Введение в Azure Stream Analytics](stream-analytics-introduction.md)
- [Приступая к работе с Azure Stream Analytics](stream-analytics-get-started.md)
- [Масштабирование заданий в службе Azure Stream Analytics](stream-analytics-scale-jobs.md)
- [Справочник по языку запросов Azure Stream Analytics](https://msdn.microsoft.com/library/azure/dn834998.aspx)
- [Справочник по API-интерфейсу REST управления Stream Analytics](https://msdn.microsoft.com/library/azure/dn835031.aspx)
 

<!---HONumber=Sept15_HO2-->