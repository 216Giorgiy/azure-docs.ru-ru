---
title: "Прогнозирование рабочей нагрузки сервера в Azure на основе терабайтов данных | Документация Майкрософт"
description: "Вы узнаете, как обучить модель машинного обучения на основе больших данных в Azure ML Workbench."
services: machine-learning
documentationcenter: 
author: daden
manager: mithal
editor: daden
ms.assetid: 
ms.service: machine-learning
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 09/15/2017
ms.author: daden
ms.translationtype: HT
ms.sourcegitcommit: c3a2462b4ce4e1410a670624bcbcec26fd51b811
ms.openlocfilehash: b76253fad43be231591023c4d4466bf6e3f329a0
ms.contentlocale: ru-ru
ms.lasthandoff: 09/25/2017

---

# <a name="server-workload-forecasting-on-terabytes-data"></a>Прогнозирование рабочей нагрузки сервера на основе терабайтов данных

В этом примере показано, как специалисты по обработке и анализу данных могут применять Azure ML Workbench для разработки решений, которые требуют использования больших данных. Мы покажем простую и удобную процедуру использования Azure ML Workbench — начиная с выборки из большого набора данных, подготовки данных, проектирования компонентов, машинного обучения и завершая полной обработкой большого набора данных. 

Попутно мы рассмотрим следующие основные возможности Azure ML Workbench:
* Простое переключение между целевыми объектами вычисления. Мы покажем, как настроить различные целевые объекты вычисления и использовать их при экспериментировании. В этом примере как целевые объекты вычисления мы используем виртуальную машину для обработки и анализа данных Ubuntu и кластер HDInsight. Кроме того, вы узнаете, как настраивать целевые объекты вычисления в зависимости от доступности ресурсов. В частности, как использовать ресурсы с помощью Azure ML Workbench для ускорения экспериментов после масштабирования кластера Spark (включая дополнительные рабочие узлы в кластере Spark).
* Отслеживание журнала выполнения. Мы покажем, как использовать Azure ML Workbench для отслеживания производительности моделей ML и других необходимых метрик.
* Ввод модели машинного обучения в эксплуатацию. Вы узнаете, как использовать встроенные средства Azure ML Workbench, чтобы развернуть обученную модель машинного ML как веб-службу в Службе контейнеров Azure (ACS). Также мы покажем, как использовать веб-службу, чтобы получать мини-пакеты прогнозов при помощи вызовов REST API. 
* Поддержка терабайтов данных.



## <a name="link-to-the-gallery-github-repository"></a>Ссылка на репозиторий коллекции на GitHub

Общедоступный репозиторий GitHub содержит все материалы для этого сценария, в том числе примеры кода: 
 
[https://github.com/Azure/MachineLearningSamples-BigData](https://github.com/Azure/MachineLearningSamples-BigData)



## <a name="use-case-overview"></a>Обзор вариантов использования


Прогнозирование рабочей нагрузки серверов — это обычное бизнес-требование для технологических компаний, которые управляют своей собственной инфраструктурой. Чтобы сократить затраты на инфраструктуру, службы, которые выполняются на мало использующихся серверах, нужно сгруппировать. Это позволит запустить их на меньшем число компьютеров. А службам, которые выполняются на серверах с большой нагрузкой, необходимо предоставить больше компьютеров. В этом сценарии мы рассмотрим прогноз рабочей нагрузки для каждого компьютера (или сервера). В частности, мы используем данные сеансов для каждого сервера, чтобы спрогнозировать класс его рабочей нагрузки в будущем. Мы присвоим класс (низкий, средний или высокий) нагрузке каждого сервера с помощью классификатора случайного леса в [Apache Spark ML](https://spark.apache.org/docs/2.1.1/ml-guide.html). Методы и рабочий процесс машинного обучения в этом примере можно легко применять для решения других подобных проблем. 


## <a name="prerequisites"></a>Предварительные требования

Предварительные требования для выполнения этого сценария:

* [Учетная запись Azure](https://azure.microsoft.com/free/) (доступны бесплатные пробные версии).
* Установленная копия [Azure Machine Learning Workbench](./overview-what-is-azure-ml.md). Чтобы установить эту программу и создать рабочую область, выполните инструкции из [краткого руководства по установке](./quickstart-installation.md).
* В этом сценарии предполагается, что Machine Learning (ML) Workbench выполняется в Windows 10. Если вы используете macOS, большинство инструкций будут аналогичными.
* Виртуальная машина для обработки и анализа данных для Linux (Ubuntu). Вы можете подготовить виртуальную машину для обработки и анализа данных Ubuntu, выполнив эти [инструкции](https://docs.microsoft.com/azure/machine-learning/machine-learning-data-science-provision-vm). Нажмите [здесь](https://ms.portal.azure.com/#create/microsoft-ads.linux-data-science-vm-ubuntulinuxdsvmubuntu) для быстрого запуска. Рекомендуем использовать виртуальную машину с как минимум 8 ядрами и 32 ГБ памяти.  Чтобы воспользоваться этим примером, необходимо указать IP-адрес виртуальной машины для обработки и анализа данных, имя пользователя и пароль. Сохраните таблицу ниже с данными виртуальной машины для обработки и анализа данных для выполнения дальнейших действий:

 Имя поля| Значение |  
 |------------|------|
IP-адрес виртуальной машины для обработки и анализа данных | xxx|
 Имя пользователя  | xxx|
 Пароль   | xxx|

 Вы можете использовать любую виртуальную машину (VM) с установленной [подсистемой Docker](https://docs.docker.com/engine/).

* Кластер HDInsight Spark с HDP 3.6 и Spark 2.1.x. Дополнительные сведения по созданию кластеров HDInsight см. в разделе [Создание кластера Apache Spark в Azure HDInsight] (https://docs.microsoft.com/azure/hdinsight/hdinsight-apache-spark-jupyter-spark-sql). Рекомендуем использовать кластер с тремя рабочими узлами (по 16 ядер и 112 ГБ памяти в каждом). Или можно выбрать тип VM `D12 V2` для головного узла и `D14 V2` для рабочего узла. Развертывание кластера занимает около 20 минут. Чтобы воспользоваться этим примером, необходимо указать имя кластера, имя пользователя SSH и пароль. Сохраните таблицу ниже с данными кластера HDInsight для выполнения дальнейших действий:

 Имя поля| Значение |  
 |------------|------|
 Имя кластера| xxx|
 Имя пользователя  | xxx (по умолчанию — sshuser)|
 Пароль   | xxx|


* Учетная запись хранения Azure. Вы можете создать учетную запись хранения Azure, выполнив эти [инструкции](https://docs.microsoft.com/azure/storage/common/storage-create-storage-account). Кроме того, создайте в этой учетной записи хранения два частных контейнера больших двоичных объектов с именами `fullmodel` и `onemonthmodel`. Учетная запись хранения используется для сохранения промежуточных результатов вычислений и моделей машинного обучения. Чтобы воспользоваться этим примером, необходимо указать имя учетной записи хранения и ключ доступа. Сохраните таблицу ниже с данными учетной записи хранения Azure для выполнения дальнейших действий:

 Имя поля| Значение |  
 |------------|------|
 Имя учетной записи хранения| xxx|
 Ключ доступа  | xxx|


Целевыми объектами вычисления являются созданные в рамках предварительных требований виртуальная машина для обработки и анализа данных Ubuntu и кластер Azure HDInsigh. Целевые объекты вычисления — это вычислительные ресурсы, применяемые с Azure ML Workbench, которые могут быть отличными от компьютера, где выполняется Azure ML Workbench.   

## <a name="create-a-new-workbench-project"></a>Создание проекта в Workbench

Создайте проект, используя в качестве шаблона следующий пример:
1.  Откройте Azure Machine Learning Workbench.
2.  На странице **Projects** (Проекты) щелкните знак **+** и выберите **New Project** (Создать проект).
3.  В области **Create New Project** (Создание проекта) введите информацию о новом проекте.
4.  В поле поиска **Search Project Templates** (Поиск шаблонов проектов) введите Workload Forecasting on Terabytes Data (Прогнозирование рабочей нагрузки на основе терабайтов данных) и выберите шаблон.
5.  Нажмите кнопку **Создать**

Можно создать проект Azure ML Workbench с предварительно созданным репозиторием Git, выполнив эти [инструкции](./tutorial-classifying-iris-part-1.md).  
Запустите проверку состояния Git, чтобы просмотреть данные состояния файлов для отслеживания версий.

## <a name="data-description"></a>Описание данных

Данные, которые используются в этом сценарии, — это синтезированные данные рабочей нагрузки сервера, размещенные в общедоступной учетной записи хранилища BLOB-объектов Azure. Сведения об определенной учетной записи хранения можно найти в поле `dataFile` файла [`Config/storageconfig.json`](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Config/fulldata_storageconfig.json). Можно использовать данные непосредственно из хранилища BLOB-объектов Azure. Если с хранилищем одновременно работают несколько пользователей, вы можете использовать [azcopy](https://docs.microsoft.com/azure/storage/common/storage-use-azcopy-linux), чтобы загрузить данные в собственное хранилище. 

Общий объем данных — около 1 ТБ. Каждый файл занимает около 1–3 ГБ и представлен в формате CSV без заголовка. Каждая строка данных представляет нагрузку для транзакции на конкретном сервере.  Подробные сведения о схеме данных выглядят следующим образом:

Номер столбца | Имя поля| Тип | Описание |  
|------------|------|-------------|---------------|
1  | `SessionStart` | Datetime |    Время начала сеанса
2  |`SessionEnd`    | Datetime | Время окончания сеанса
3 |`ConcurrentConnectionCounts` | Целое число  | Количество одновременных подключений
4. | `MbytesTransferred` | Double | Нормализованные переданные данные в мегабайтах
5 | `ServiceGrade` | Целое число  |  Уровень службы для сеанса
6 | `HTTP1` | Целое число |  Используемый протокол для сеанса: HTTP1 или HTTP2
7 |`ServerType` | Целое число    |Тип сервера
8 |`SubService_1_Load` | Double |   Нагрузка подслужбы 1
9 | `SubService_1_Load` | Double |  Нагрузка подслужбы 2
10 | `SubService_1_Load` | Double |     Нагрузка подслужбы 3
11 |`SubService_1_Load` | Double |  Нагрузка подслужбы 4
12 | `SubService_1_Load`| Double |      Нагрузка подслужбы 5
13. |`SecureBytes_Load`  | Double | Безопасная нагрузка в байтах
14 |`TotalLoad` | Double | Общая нагрузка на сервер
15 |`ClientIP` | Строка|    IP-адрес клиента
16 |`ServerIP` | Строка|    IP-адрес сервера



В таблице выше перечислены ожидаемые типы данных. Обратите внимание, что из-за отсутствующих или некорректных значений типы данных могут отличаться от ожидаемых. Это следует учесть при обработке данных. 


## <a name="scenario-structure"></a>Структура сценария

Файлы в этом примере упорядочены следующим образом:

| Имя файла | Тип | Описание |
|-----------|------|-------------|
| `Code` | Папка | Папка содержит весь код в примере |
| `Config` | Папка | Папка содержит файлы конфигурации |
| `Image` | Папка | Папка используется для сохранения изображений для файла README |
| `Model` | Папка | Папка используется для сохранения файлов модели, которые загружены из хранилища BLOB-объектов Azure |
| `Code/etl.py` | Файл Python | Файл используется для подготовки данных и проектирования компонентов |
| `Code/train.py` | Файл Python | Файл используется для обучения модели трех классов с множественной классификацией  |
| `Code/webservice.py` | Файл Python | Файл используется для ввода в эксплуатацию  |
| `Code/scoring_webservice.py` | Файл Python |  Файл используется для преобразования данных и вызова веб-службы |
| `Code/O16Npreprocessing.py` | Файл Python | Файл используется для предварительной обработки данных для scoring_webservice.py  |
| `Code/util.py` | Файл Python | Файл содержит код для чтения и записи больших двоичных объектов Azure  
| `Config/storageconfig.json` | Файл JSON | Файл конфигурации для контейнера больших двоичных объектов Azure. В нем хранятся промежуточные результаты и модель для обработки и обучения на основе данных за один месяц. |
| `Config/fulldata_storageconfig.json` | Файл JSON |  Файл конфигурации для контейнера больших двоичных объектов Azure. В нем хранятся промежуточные результаты и модель для обработки и обучения на основе полного набора данных.|
| `Config/webservice.json` | Файл JSON | Файл конфигурации для scoring_webservice.py|
| `Config/conda_dependencies.yml` | Файл YAML | Файл зависимостей Conda |
| `Config/conda_dependencies_webservice.yml` | Файл YAML | Файл зависимостей Conda для веб-службы|
| `Config/dsvm_spark_dependencies.yml` | Файл YAML | Файл зависимостей Spark для виртуальной машины для обработки и анализа данных Ubuntu |
| `Config/hdi_spark_dependencies.yml` | Файл YAML | Файл зависимостей Spark для кластера HDInsight Spark |
| `README.md` | Файл Markdown | Файл сведений в формате Markdown |
| `Code/download_model.py` | Файл Python | Файл используется для загрузки файлов модели из хранилища BLOB-объектов Azure на локальный диск |
| `Docs/DownloadModelsFromBlob.md` | Файл Markdown | Файл содержит инструкции по запуску `Code/download_model.py` |



### <a name="data-flow"></a>Поток данных

Код в [`Code/etl.py`](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Code/etl.py) предназначен для загрузки данных из общедоступного контейнера (поле `dataFile` файла [`Config/storageconfig.json`](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Config/fulldata_storageconfig.json)). Он охватывает подготовку данных и проектирование компонентов. Промежуточные результаты вычислений и модели сохраняются в частном контейнере. Код в [`Code/train.py`](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Code/train.py) предназначен для загрузки промежуточных результатов вычислений из частного контейнера, обучения модели с многоклассовой классификацией и записи обученной модели в частный контейнер. Рекомендуем использовать отдельные контейнеры для экспериментов с набором данных за месяц и экспериментов с полным набором данных. Данные и модели сохраняются в файле Parquet. Поэтому каждый файл фактически является папкой в контейнере с несколькими большими двоичными объектами. Полученный контейнер выглядит следующим образом:

| Имя префикса большого двоичного объекта | Тип | Описание |
|-----------|------|-------------|
| featureScaleModel | Parquet | Стандартная модель масштабирования для числовых функций |
| stringIndexModel | Parquet | Модель индексатора строк для нечисловых функций|
| oneHotEncoderModel|Parquet | Модель прямого кодирования для категориальных признаков |
| mlModel | Parquet | Обученная модель машинного обучения |
| info| Сериализованный файл Python | Сведения о преобразованных данных. Включают время начала и окончания обучения, длительность, метку времени для разделения тестового обучения и столбцы для индексирования и прямого кодирования.

Все файлы и большие двоичные объекты в предыдущей таблице используются для ввода в эксплуатацию.


### <a name="model-development"></a>Разработка модели

#### <a name="architecture-diagram"></a>Схема архитектуры


На схеме ниже представлен полный рабочий процесс использования Azure ML Workbench для разработки модели: ![архитектура](media/scenario-big-data/architecture.PNG)



Далее мы рассмотрим разработку модели с использованием функции для удаленных целевых объектов вычисления в Azure ML Workbench. Сначала мы загрузим небольшой пример данных и выполним скрипт [`Code/etl.py`](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Code/etl.py) на виртуальной машине для обработки и анализа данных Ubuntu для ускорения итерации. Мы можем еще больше сократить число операций в [`Code/etl.py`](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Code/etl.py), передав дополнительный аргумент для ускорения итерации. В завершение мы используем кластер HDInsight для обучения на основе полного набора данных.     

Файл [`Code/etl.py`](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Code/etl.py) предназначен для загрузки и подготовки данных, а также для проектирования компонентов. Он принимает два аргумента: (1) файл конфигурации для контейнера в хранилище BLOB-объектов Azure (в нем хранятся промежуточные результатов вычислений и модели); (2) конфигурация отладки для ускорения итерации.

Первый аргумент, `configFilename`, —это локальный файл конфигурации, где хранятся данные хранилища BLOB-объектов Azure и указывается расположение для загрузки данных. По умолчанию это файл [`Config/storageconfig.json`](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Config/storageconfig.json), который будет использоваться при обработке данных за один месяц. Кроме того, мы включили [`Config/fulldata_storageconfig.json`](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Config/fulldatastorageconfig.json), который потребуется при обработке полного набора данных. Содержимое в конфигурации выглядит следующим образом: 

| Поле | Тип | Описание |
|-----------|------|-------------|
| storageAccount | Строка | Имя учетной записи хранения Azure |
| storageContainer | Строка | Контейнер в учетной записи хранения Azure для хранения промежуточных результатов |
| storageKey | Строка |Ключ доступа к учетной записи хранения Azure |
| dataFile|Строка | Файлы источника данных  |
| длительность| Строка | Период, который охватывают данные в файлах источника данных|

Измените `Config/storageconfig.json` и `Config/fulldata_storageconfig.json`, чтобы настроить учетную запись хранения, ключ к хранилищу данных и контейнер больших двоичных объектов для хранения промежуточных результатов. По умолчанию контейнер больших двоичных объектов для обработки данных за месяц — `onemonthmodel`, а контейнер больших двоичных объектов для полного набора данных — `fullmodel`. Убедитесь, что эти два контейнера созданы в учетной записи хранения. В поле `"dataFile"` файла [`Config/fulldata_storageconfig.json`](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Config/fulldatastorageconfig.json) задается тип данных, которые загружаются в [`Code/etl.py`](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Code/etl.py), а в поле `"duration"` задается интервал времени, который охватывают данные. Если установлен период ONE_MONTH, из семи файлов с данными загружаться должен только один CSV-файл за июнь 2016 г. Если для периода задано значение FULL, загружается полный набор данных объемом 1 ТБ. Не нужно изменять `"dataFile"` и `"duration"` в двух этих файлах конфигурации.

Второй аргумент — DEBUG. Задав для него значение FILTER_IP, можно ускорить итерацию. Этот параметр полезен при отладке скрипта.

> [!NOTE]
> Замените любую переменную аргумента во всех командах ниже фактическим значением.
> 


#### <a name="model-development-on-the-docker-of-ubuntu-dsvm"></a>Разработка модели при помощи Docker на виртуальной машине для обработки и анализа данных Ubuntu

#####  <a name="1-setting-up-the-compute-target-for-docker-on-ubuntu-dsvm"></a>1. Настройка целевого объекта вычисления для Docker на виртуальной машине для обработки и анализа данных Ubuntu

Запустите командную строку из Azure ML Workbench. Для этого щелкните меню File (Файл) в левом верхнем углу окна Azure ML Workbench и выберите Open Command Prompt (Открыть командную строку). 

```az ml computetarget attach --name dockerdsvm --address $DSVMIPaddress  --username $user --password $password --type remotedocker```

После успешного выполнения команд в командной строке в папке проекта aml_config появятся два следующих файла:

    dockerdsvm.compute: contains the connection and configuration information for a remote execution target
    dockerdsvm.runconfig: set of run options used when executing within the Azure ML Workbench application

Перейдите к dockerdsvm.runconfig и измените конфигурацию следующих полей, как показано ниже:

    PrepareEnvironment: true 
    CondaDependenciesFile: Config/conda_dependencies.yml 
    SparkDependenciesFile: Config/dsvm_spark_dependencies.yml

Подготовьте среду проекта с помощью команды ниже.

```az ml experiment prepare -c dockerdsvm```


Если вы установить для PrepareEnvironment значение true, Azure ML Workbench будет создавать среду выполнения при каждой отправке задания. `Config/conda_dependencies.yml` и `Config/dsvm_spark_dependencies.yml` отвечают за настройку среды выполнения. Вы можете в любой момент изменить зависимости Conda, конфигурацию Spark и зависимости Spark, изменив эти два файла YAML. В этом примере мы добавили `azure-storage` и `azure-ml-api-sdk` как дополнительные пакеты Python в `Config/conda_dependencies.yml`. Кроме того, мы добавили `spark.default.parallelism`, `spark.executor.instances`, `spark.executor.cores` и т. д. в `Config/dsvm_spark_dependencies.yml`. 

#####  <a name="2-data-preparation-and-feature-engineering-on-dsvm-docker"></a>2) Подготовка данных и проектирование компонентов на виртуальной машине для обработки и анализа данных Docker.

Запустите сценарий `etl.py` на виртуальной машине для обработки и анализа данных Docker с параметром отладки, который фильтрует загруженные данные по определенным IP-адресам серверов:

```az ml experiment submit -t dockerdsvm -c dockerdsvm ./Code/etl.py ./Config/storageconfig.json FILTER_IP```

Перейдите к боковой панели и щелкните Run (Выполнение), чтобы просмотреть журнал выполнения `etl.py`. Обратите внимание, что выполнение занимает около двух минут. Если вы планируете изменить код, включив новые функции, можно указать FILTER_IP как второй аргумент, чтобы ускорить итерацию. При возникновении проблем с собственным машинным обучением вам может потребоваться выполнить этот шаг несколько раз, чтобы просмотреть набор данных или создать компоненты. Настраиваемые ограничения данных для загрузки и последующая фильтрация данных для обработки ускоряют итерацию при разработке модели. Проводя эксперименты, периодически сохраняйте изменения в коде в репозиторий Git.  Обратите внимание, что мы использовали в `etl.py` код ниже для обеспечения доступа к частному контейнеру:

```python
def attach_storage_container(spark, account, key):
    config = spark._sc._jsc.hadoopConfiguration()
    setting = "fs.azure.account.key." + account + ".blob.core.windows.net"
    if not config.get(setting):
        config.set(setting, key)

# attach the blob storage to the spark cluster or VM so that the storage can be accessed by the cluster or VM        
attach_storage_container(spark, storageAccount, storageKey)
```


Затем выполните скрипт `etl.py` на виртуальной машине для обработки и анализа данных Docker без параметра отладки FILTER_IP.

```az ml experiment submit -t dockerdsvm -c dockerdsvm ./Code/etl.py ./Config/storageconfig.json FALSE```

Перейдите к боковой панели и щелкните Run (Выполнение), чтобы просмотреть журнал выполнения `etl.py`. Обратите внимание, что выполнение занимает около четырех минут. Обработанный результат этого шага сохраняется в контейнере и загружается для обучения в train.py. Кроме того, индексаторы строк, конвейеры кодировщиков и стандартные средства маршрутизации сохраняются в частном контейнере и используются при вводе в эксплуатацию (O16N). 


##### <a name="3-model-training-on-dsvm-docker"></a>3. Обучение модели на виртуальной машине для обработки и анализа данных Docker

Запустите скрипт `train.py` на виртуальной машине для обработки и анализа данных Docker:

```az ml experiment submit -t dockerdsvm -c dockerdsvm ./Code/train.py ./Config/storageconfig.json```

При этом загружаются промежуточные результаты вычислений на основе выполнения `etl.py` и обучается модель машинного обучения. Этот этап занимает около двух минут.

После успешного выполнения экспериментов с небольшим объемом данных можно приступать к экспериментам с полным набором данных. Начать можно с того же кода, а затем поэкспериментировать, изменяя аргументы и целевые объекты вычисления.  

####  <a name="model-development-on-the-hdinsight-cluster"></a>Разработка модели в кластере HDInsight

##### <a name="1-create-compute-target-in-azure-ml-workbench-for-the-hdinsight-cluster"></a>1. Создание целевого объекта вычисления в Azure ML Workbench для кластера HDInsight.

```az ml computetarget attach --name myhdi --address $clustername-ssh.azurehdinsight.net --username $username --password $password --type cluster```

После успешного выполнения команд в командной строке в папке aml_config появятся два следующих файла:
    
    myhdo.compute: contains connection and configuration information for a remote execution target
    myhdi.runconfig: set of run options used when executing within the Azure ML Workbench application


Перейдите к myhdi.runconfig и измените конфигурацию из следующих полей, как показано ниже:

    PrepareEnvironment: true 
    CondaDependenciesFile: Config/conda_dependencies.yml 
    SparkDependenciesFile: Config/hdi_spark_dependencies.yml

Подготовьте среду проекта с помощью команды ниже.

```az ml experiment prepare -c myhdi```

Этот этап может занять до семи минут.

##### <a name="2-data-preparation-and-feature-engineering-on-hdinsight-cluster"></a>2) Подготовка данных и проектирование компонентов в кластере HDInsight.

Запустите скрипт `etl.py` с полным набором данных в кластере HDInsight.

```az ml experiment submit -a -t myhdi -c myhdi ./Code/etl.py Config/fulldata_storageconfig.json FALSE```

Так как это задание выполняется достаточно долго (около двух часов), мы используем "-a", чтобы отключить поток вывода. После выполнения задания в разделе Run History (Журнал выполнения) можно просмотреть журналы драйверов и контроллеров. Для большого кластера в любой момент можно перенастроить конфигурации в `Config/hdi_spark_dependencies.yml` для использования нескольких экземпляров или большего числа ядер. Например, если в кластере четыре рабочих узла, можно увеличить значение `spark.executor.instances` с 5 до 7. Выходные данные этого действия можно просмотреть в контейнере fullmodel в вашей учетной записи хранения. 


##### <a name="3-model-training-on-hdinsight-cluster"></a>3. Обучение модели в кластере HDInsight

Запустите скрипт `train.py` в кластере HDInsight:

```az ml experiment submit -a -t myhdi -c myhdi ./Code/train.py Config/fulldata_storageconfig.json```

Так как это задание выполняется достаточно долго (около получаса), мы используем "-a", чтобы отключить поток вывода.

#### <a name="run-history-exploration"></a>Просмотр журнала выполнения

Журнал выполнения — это компонент, который позволяет отслеживать ваши эксперименты в Azure ML Workbench. По умолчанию отслеживается длительность экспериментов. В нашем конкретном примере, когда мы переходим к экспериментам с полным набором данных для `Code/etl.py`, длительность значительно увеличивается. Также можно вносить в журнал определенные метрики для отслеживания. Чтобы включить отслеживание метрики, добавьте в начало файла Python следующие строки кода:
```python
# import logger
from azureml.logging import get_azureml_logger

# initialize logger
run_logger = get_azureml_logger()
```
Ниже приведен пример для отслеживания конкретной метрики:

```python
run_logger.log("Test Accuracy", testAccuracy)
```

Перейдите к элементу Runs (Запуски) на боковой панели Azure ML Workbench справа, чтобы просмотреть журнал выполнения для каждого файла Python. Кроме того, в репозитории GitHub создается ветвь с именем, которое начинается с AMLHistory. Она используется для отслеживания изменений, вносимых в скрипт при каждом запуске. 


### <a name="operationalization"></a>Ввод в эксплуатацию

В этом разделе мы введем модель, которую создали в предыдущих шагах, в эксплуатацию в качестве веб-службы. Также вы узнаете, как использовать веб-службу для прогнозирования рабочей нагрузки. Мы используем интерфейсы командной строки (CLI) для ввода в эксплуатацию Azure ML, чтобы упаковать код и зависимости как образы Docker и опубликовать эту модель в качестве контейнерной веб-службы. Дополнительные сведения см. в обзорной статье о [вводе в эксплуатацию](https://github.com/Azure/Machine-Learning-Operationalization/blob/master/documentation/operationalization-overview.md). Чтобы запустить интерфейсы CLI для ввода Azure ML в эксплуатацию, вы можете воспользоваться командной строкой.  Кроме того, можно запустить интерфейсы CLI для ввода Azure ML в эксплуатацию в Ubuntu Linux, следуя инструкциям в [руководстве по установке](https://github.com/Azure/Machine-Learning-Operationalization/blob/master/documentation/install-on-ubuntu-linux.md). 

> [!NOTE]
> Замените любую переменную аргумента во всех командах ниже фактическим значением. Выполнение задач в этом разделе займет около 40 минут.
> 


Выберите уникальную строку в качестве среды для ввода в эксплуатацию. Для ее представления мы используем строку [unique].

1. Создайте среду для ввода в эксплуатацию и группу ресурсов.

        az ml env setup -c -n [unique] --location eastus2 --cluster -z 5 --yes

   Обратите внимание, что мы используем в качестве среды Службу контейнеров Azure при помощи `--cluster` в команде `az ml env setup`. Мы решили ввести модель машинного обучения в эксплуатацию в [Службе контейнеров Azure](https://docs.microsoft.com/azure/container-service/kubernetes/container-service-intro-kubernetes), так как в ней используется [Kubernetes](https://kubernetes.io/). Это позволяет автоматизировать развертывание и масштабирование контейнерных приложений, а также управление ими. Выполнение этой команды занимает около 20 минут. Использование 

        az ml env show -g [unique]rg -n [unique]

   чтобы проверить, успешно ли завершено развертывание.

   Установите среду развертывания так же, как и недавно созданную, выполнив следующую команду:

        az ml env set -g [unique]rg -n [unique]

2. Создайте учетную запись управления моделями и воспользуйтесь ею.

   Создайте учетную запись управления моделями, выполнив следующую команду:

    az ml account modelmanagement create --location  eastus2 -n [unique]acc -g [unique]rg --sku-instances 4 --sku-name S3. 

   Используйте учетную запись управления моделями для ввода в эксплуатацию, выполнив следующую команду:

        az ml account modelmanagement set  -n [unique]acc -g [unique]rg  

   Учетная запись управления моделями используется для управления моделями и веб-службами. На портале Azure появится новая учетная запись управления моделями. Ее можно использовать для просмотра моделей, манифестов, образов Docker и служб, которые создаются с ее помощью.

3. Загрузите и зарегистрируйте модели.

   Загрузите модели в контейнере fullmodel на локальный компьютер в каталог кода. Не загружайте файл данных Parquet с именем vmlSource.parquet, так как он не является файлом модели, а содержит промежуточные результаты вычисления. Также можно повторно использовать файлы модели, которые мы поместили в репозиторий Git. Дополнительные сведения о загрузке файлов Parquet см. в [DownloadModelsFromBlob.md](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Docs/DownloadModelsFromBlob.md). 

   Перейдите в CLI в папку `Model` и зарегистрируйте модели, как показано ниже:

        az ml model register -m  mlModel -n vmlModel -t fullmodel
        az ml model register -m  featureScaleModel -n featureScaleModel -t fullmodel
        az ml model register -m  oneHotEncoderModel -n  oneHotEncoderModel -t fullmodel
        az ml model register -m  stringIndexModel -n stringIndexModel -t fullmodel
        az ml model register -m  info -n info -t fullmodel

   Выходные данные каждой команды содержат идентификатор модели, который используется в следующем шаге. Сохраните их в текстовый файл для последующего использования.

4. Создайте манифест для веб-службы.

   Манифест представляет собой инструкции, которые используются для создания образа Docker для контейнеров веб-служб. Он содержит код для веб-службы, всех моделей машинного обучения и зависимостей среды выполнения.  В CLI перейдите к папке `Code` и запустите командную строку:

        az ml manifest create -n $webserviceName -f webservice.py -r spark-py -c ../Config/conda_dependencies_webservice.yml -i $modelID1 -i $modelID2 -i $modelID3 -i $modelID4 -i $modelID5

   Выходные данные содержат идентификатор манифеста для следующего шага. 

   В каталоге `Code` проверьте файл webservice.py, выполнив следующую команду: 

        az ml experiment submit -t dockerdsvm -c dockerdsvm webservice.py

5. Создайте образ Docker. 

        az ml image create -n [unique]image --manifest-id $manifestID

   Выходные данные содержат идентификатор образа для следующего шага, так как этот образ Docker используется в ACS. 

6. Разверните веб-службу в кластере ACS.

        az ml service create realtime -n [unique] --image-id $imageID --cpu 0.5 --memory 2G

   Выходные данные содержат идентификатор службы. Он потребуется, чтобы получить ключ авторизации и URL-адрес службы.

7. Вызовите веб-службу в коде Python, чтобы выполнить оценку мини-пакетов.

   Чтобы получить ключ авторизации, используйте следующую команду:

         az ml service keys realtime -i $ServiceID 

   и выполните команду ниже, чтобы получить URL-адрес оценки службы.

        az ml service usage realtime -i $ServiceID

   Замените содержимое `./Config/webservice.json` правильным URL-адресом оценки службы и ключом авторизации (оставьте в исходном файле маркер носителя и замените часть xxx). 
   
   Перейдите в корневую папку проекта и протестируйте веб-службу, выполнив оценку мини-пакетов при помощи следующей команды:

        az ml experiment submit -t dockerdsvm -c dockerdsvm ./Code/scoring_webservice.py ./Config/webservice.json

8. Масштабируйте веб-службу. 

   Чтобы масштабировать веб-службу, прочтите статью о [масштабировании при вводе в эксплуатацию в кластере ACS](https://github.com/Azure/Machine-Learning-Operationalization/blob/master/documentation/how-to-scale.md).
 

## <a name="conclusion"></a>Заключение

На этом примере мы показали, как использовать Azure ML Workbench, чтобы обучить модель машинного обучения на основе больших данных и ввести ее в эксплуатацию. В частности, вы узнали, как:

* настраивать и использовать различные целевые объекты вычисления;

* запускать журнал отслеживаемых метрик и различных запусков;

* вводить модели в эксплуатацию.

Пользователи могут расширить код и изучить настройки перекрестной проверки и гиперпараметров. Дополнительные сведения о настройке перекрестной проверки и гиперпараметров доступны по адресу https://github.com/Azure/MachineLearningSamples-DistributedHyperParameterTuning.  
Дополнительные сведения о прогнозировании временных рядов см. по адресу https://github.com/Azure/MachineLearningSamples-EnergyDemandTimeSeriesForecasting.

