<properties 
	pageTitle="Выбор алгоритмов машинного обучения | Microsoft Azure" 
	description="Как выбрать алгоритмы машинного обучения Azure для контролируемого и неконтролируемого обучения в экспериментах кластеризации, классификации или регрессии." 
	services="machine-learning"
	documentationCenter="" 
	authors="garyericson" 
	manager="paulettm" 
	editor="cgronlun"/>

<tags 
	ms.service="machine-learning" 
	ms.workload="data-services" 
	ms.tgt_pltfrm="na" 
	ms.devlang="na" 
	ms.topic="article" 
	ms.date="06/01/2015" 
	ms.author="bradsev;garye" />


# Выбор алгоритмов машинного обучения Azure для кластеризации, классификации или регрессии

В этом разделе объясняется некоторые основные аспекты метода машинного обучения. В частности вы узнаете, как выбрать соответствующие алгоритмы обучения для анализа данных типов данных, отвечать на поставленные вопросы, выполнять конкретные задачи или предоставить критерии для принятия решений.

> [AZURE.TIP][Памятка алгоритма машинного обучения Microsoft Azure](machine-learning-algorithm-cheat-sheet.md) — это удобное пособие, которое дополняет эту статью.

При использовании машинного обучения для выполнения анализа нам, как правило, требуется ответить на два вопроса:

* Какой вид анализа необходим для достижения поставленных целей при использовании доступных данных? 
* Какой наиболее подходящий алгоритм или какую модель следует использовать для этого анализа?

Далее рассматриваются три типа анализа машинного обучения и сравниваются сценарии их использования:

* **Кластеризация**
* **Классификация** 
* **Регрессия** 

[AZURE.INCLUDE [machine-learning-free-trial](../../includes/machine-learning-free-trial.md)]


<a name="anchor-1"></a>
## Алгоритмы машинного обучения по данным

Машинное обучение — это дисциплина, изучающая класс алгоритмов, в том смысле, что они обучаются по данным, а не используют конкретную, предварительно определенную модель для их проверки. Идея заключается в том, чтобы получать знания более индуктивно, изучая закономерности в наборе данных, а не с помощью *гипотетически-дедуктивного* метода, при применении которого сначала предпринимается попытка угадать подходящую модель для всего набора данных, а затем проверить ее эмпирически.

Существует два типа машинного обучения или обучения по данным: *контролируемое* и *неконтролируемое*.

<a name="anchor-2"></a>
## Контролируемое обучение  

Для контролируемого обучения требуется, чтобы целевая переменная была определена должным образом и чтобы было предоставлено достаточное количество ее значений.

Контролируемое обучение представляет собой такой тип машинного обучения, которое выполняется, когда известны правильные выходные результаты (или целевые переменные) для вводимых обучающих экземпляров. Алгоритм машинного обучения обучается для того, чтобы найти модель (то есть, правило или функцию), которая сопоставляет входные данные с известными выходными значениями. Этот механизм работает, как диспетчер, который может сообщить агенту алгоритма о том, правильно ли он сопоставляет входные данные с выходными. После завершения процесса обучения работоспособную модель можно применить к новым входным данным для прогнозирования ожидаемого результата, в котором, в отличие от набора данных для обучения, целевое значение не известно заранее.

Тип модели зависит от характера целевой переменной.

![Схема контролируемого обучения: модель из помеченных данных и ее использование для прогнозирования результатов на основании новых данных.](./media/machine-learning-algorithm-choice/supervised-learning-using-known-data-to-model-solution.png)

Существуют две общие категории анализа с использованием контролируемого обучения: *классификация* и *регрессия*. Контролируемое обучение довольно часто используется для решения проблем классификации, так как цель зачастую состоит в том, чтобы обучить компьютер созданной системе классификации. Ответы, как правило, представляют собой несколько известных значений (меток), таких как true, false, high, medium или low. Алгоритмы классификации применяются к номинальным значениям, а не порядковым значениям ответа. Типичный пример обучения классификации — распознавание цифр. В более широком понимании обучение классификации подходит для любой проблемы, для решения которой будет полезно и легко определить классификацию.

При контролируемом обучении исследуемые переменные можно разделить на две группы: независимые переменные (также называемые предикторами) и зависимые переменные (также называемые переменными ответа). Цель анализа — определить связь между пояснительными и зависимыми переменными, как это сделано в *регрессионном анализе*. Значения зависимых переменных должны быть известны для достаточно большой части набора данных. В регрессии ответы или выходная переменная принимают непрерывные значения, такие как количество километров, которые конкретный автомобиль может проехать на одном литре топлива, возраст человека и т. д.

Контролируемое обучение также является наиболее распространенным способом обучения нейронных сетей и деревьев принятия решений.

> Оба этих метода сильно зависят от информации, предоставляемой предварительно определенными классификациями. В случае нейронных сетей классификация используется для определения ошибки сети и последующей настройки сети, которая сведет ошибку к минимуму. А в деревьях решений классификации используются для определения атрибутов, которые обеспечивают большую часть информации, которая может использоваться для решения головоломки классификации. Оба этих примера работают при наличии некоторого «контроля» в виде предопределенной классификации.

>  Распознавание речи с помощью скрытых моделей Маркова и Байесовских сетей также зависит от некоторых элементов контроля. Чтобы настроить параметры, как обычно, необходимо свести к минимуму ошибки в заданных входных данных. [[Машинное обучение, часть II: контролируемое и неконтролируемое обучение](http://www.aihorizon.com/essays/generalai/supervised_unsupervised_machine_learning.htm), [период AI](http://www.aihorizon.com/)]


<a name="anchor-3"></a>
##Неконтролируемое обучение

В машинном обучении задача неконтролируемого обучения заключается в определении шаблонов или скрытой структуры в неразмеченных данных. Модели не предоставляются «правильные результаты» для набора данных, по которому будет проводится обучение. Поскольку неразмеченные примеры предоставляются ученик, отсутствует обратная связь (нет ни ошибки, ни вознаграждения) для оценки возможного решения. Цель заключается в том, чтобы обучить компьютер делать что-то без явных указаний о том, как справиться с задачей. При неконтролируемом обучении все переменные обрабатываются одинаково независимо от типа (независимые или зависимые переменные). Но достигнуты еще не все главные цели, например сокращение данных, или более конкретные цели, такие как поиск кластеров.

В Машинном обучении Azure можно выполнять как неконтролируемое, так и контролируемое обучение при **классификации**, **кластеризации** и **регрессии**.

   ![screenshot\_of\_experiment](./media/machine-learning-algorithm-choice/help2.png)

<a name="anchor-4"></a>
##Кластеризация
Кластеризация — пример неконтролируемого обучения. В этом типе обучения цель заключается в том, чтобы определить сходства в данных для обучения и разделить набор данных на подмножества, разграниченные по этим сходным чертам. Расчет на то, что наиболее значимые кластеры, обнаруженные в ходе этих управляемых данными процедур, согласуются с интуитивной классификацией, зачастую вполне оправдан, хотя и не всегда.

Несмотря на то, что алгоритм кластеризации не назначает этим кластерам соответствующие имена, он может создавать их, а затем использовать для прогнозирования сходных черт, которые предположительно должны быть в новых примерах, классифицируя их как принадлежащие к наиболее подходящему кластеру. Это управляемый данными подход, который хорошо работает при наличии достаточного количества данных. Например, алгоритмы фильтрации информации из социальных сетей, примером которых могут служить алгоритмы, используемые компанией Amazon.com для рекомендации книг, предполагают поиск схожих групп людей и назначение новых пользователей в эти группы для предоставления рекомендаций.

Алгоритм кластеризации, доступный в Машинном обучении Azure, — это [кластеризация методом К-средних][k-means-clustering].

![Эксперимент кластеризации методом K-средних: снимок экрана](./media/machine-learning-algorithm-choice/k-means-clustering-algorithm-menu.png)

Метод К-средних — один из простейших алгоритмов неконтролируемого обучения, который помогает решить известные проблемы кластеризации. Метод K-средних предполагает кластеризацию данных в попытке разделить примеры на N групп с равной дисперсией и свести к минимуму критерий, известный как инерция или сумма квадратов в пределах кластера. Для этого алгоритма необходимо указывать количество кластеров. Метод K-средних хорошо обрабатывает большое количество примеров и применялся в различных сферах.

Алгоритм [Кластеризация методом K-средних][k-means-clustering] возвращает необученную модель кластеризации методом К-средних, которую можно передать в модуль [Обучение модели кластеризации][train-clustering-model] для обучения.

![screenshot\_of\_experiment](./media/machine-learning-algorithm-choice/k4.png)

На этом рисунке показано, что при использовании кластеризации методом К-средних необходимо настроить определенные параметры. Метод К-средних находит указанное число кластеров для набора D-мерных точек данных. Начиная с *начального набора центроидов К-средних*, метод использует алгоритм Ллойда для итеративного уточнения расположения центроидов. Выполнение алгоритма прекращается, когда стабилизируются центроиды или когда выполняется *конкретное количество итераций*. Модуль инициализирует массив «K на D» с использованием последних центроидов, определяющих K кластеров, которые были обнаружены в N точках данных. Алгоритм также использует вектор длиной N, при этом каждая точка данных назначена одному из K кластеров. Если требуется найти определенное число кластеров (К), модуль назначает первые К точек данных К кластерам по порядку.


Этот модуль также принимает или создает начальные точки для определения исходной конфигурации кластера. *Метрика* определяет метод, используемый для измерения расстояния между точкой данных и центроидом. Каждая точка данных назначается кластеру, центроид которого расположен ближе всего к этой точке данных. По умолчанию в методе используется *эвклидова метрика*. В качестве альтернативы для метода также можно использовать *метрику косинусов* Обратите внимание, что метод К-средних может найти только локально оптимальную конфигурацию кластера для набора данных. При наличии другой исходной конфигурации метод может найти другую, возможно, улучшенную конфигурацию.

<a name="anchor-5"></a>
##Классификация 
При анализе классификации примеры разделяются на классы и используется обученный набор предварительно помеченных данных. Этот метод используется, чтобы спрогнозировать принадлежность экземпляров данных к определенным группам. В Машинном обучении Azure доступны следующие алгоритмы классификации.

![screenshot\_of\_experiment](./media/machine-learning-algorithm-choice/help3.png)

*Двухклассовые алгоритмы* используются для двоичных переменных ответа («да» или «нет», «0» или «1», True или False и т. д.), а *Многоклассовые алгоритмы* — для любой номинальной переменной ответа, позволяющей классифицировать экземпляры по более чем двум классам.

### Рекомендации по выбору алгоритма классификации
Такой длинный список алгоритмов вызывает ряд вопросов:

* Как узнать, какой из этих классификаторов наиболее подходящий для конкретного набора данных? 
* Бывают ли случаи, когда один классификатор является наиболее логичным вариантом? 
* Почему следует выбрать именно его?

Один из рекомендуемых подходов заключается в том, чтобы протестировать несколько разных классификаторов, а также разных наборов параметров в каждом из алгоритмов, а затем выбрать лучшие из них с помощью перекрестной проверки.

> [AZURE.TIP] [Azure Machine Learning Studio](https://studio.azureml.net/) позволяет применить несколько алгоритмов параллельно к одним и тем же данным и сравнить результаты. Ниже приведен пример из [коллекции машинного обучения Azure](http://gallery.azureml.net/): [Сравнение многоклассовых классификаторов: распознавание букв](http://gallery.azureml.net/Details/a635502fc98b402a890efe21cec65b92).

Ниже приведены некоторые общие рекомендации, с которых можно начать выбор алгоритма. Учтите следующие факторы при выборе алгоритма для использования: [эта схема предложена в статье [Выбор классификатора машинного обучения](http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/), Эдвин Чен (Edwin Chen)]

**Каков размер данных для обучения?** Если обучающий набор небольшого размера и вам необходимо обучить контролируемый классификатор, согласно теории машинного обучения следует выбрать классификаторы с высоким значением смещения или низкой дисперсией, такие как классификаторы упрощенного алгоритма Байеса. Эти классификаторы имеют преимущество перед классификаторами с низким значением смещения или высокой дисперсией, такими как классификаторы по методу k-ближайших соседних результатов, так как для последнего типа классификаторов характерна склонность к лжевзаимосвязям. Но классификаторы с низким значением смещения или высокой дисперсией лучше справятся с задачей, если у вас больший обучающий набор данных, так как они имеют меньше асимптотических ошибок. В этих случаях мощности классификатора с высоким значением смещения не хватит для предоставления правильной модели. Теоретические и эмпирические результаты указывают на то, что в такой ситуации прекрасно подходит упрощенный алгоритм Байеса. Однако лучше иметь более качественные данные и функции, чем более эффективный алгоритм. Это даст вам больше преимуществ. Кроме того, во время классификации больших наборов данных используемый алгоритм не очень сильно влияет на производительность. Поэтому выбирать алгоритм, учитывая такие факторы, как его масштабируемость, скорость и простота использования.

**Каким образом лучше всего выполнять обучение: поэтапно или в пакетном режиме?** Если вам необходимо использовать большой объем данных или часто обновлять данные, возможно, лучше использовать байесовские алгоритмы, которые очень эффективно выполняют обновление. Как нейронные сети, так и SVM требуют использования пакетного режима при работе с данными для обучения.

**Какие данные вы используете: исключительно категориальные, исключительно числовые или их сочетание?** Байесовский классификатор показывает самые лучшие результаты при работе с категориальными и биномиальными данными. Деревья принятия решений не могут прогнозировать числовые значения.

**Существует ли необходимость в понимании того, как работает классификатор, для вас или вашей аудитории?** Байесовские алгоритмы или деревья принятия решений очень легко объяснить. Гораздо сложнее увидеть или объяснить, как нейронные сети и SVM классифицируют данные.

**Насколько быстро вам необходимо создавать классификацию?** Сложные деревья принятия решений могут быть медленными. Методы SVM выполняют классификацию быстрее, им необходимо только определить, на какой стороне «линии» находятся ваши данные.

**Какие сложности подразумевает проблема?** Нейронные сети и SVM могут обрабатывать сложную нелинейную классификацию.

### Преимущества и недостатки алгоритмов классификации
Для каждого из этих алгоритмов классификации характерны некоторые преимущества и недостатки.

<a name="anchor-5a"></a> **Преимущества и недостатки логистической регрессии:** «анализ логистической регрессии основывается на вычислении вероятности результата в виде соотношения вероятности получения результатов, разделенной на вероятность их отсутствия». [[Логистическая регрессия и линейный дискриминантный анализ в оценке факторов, связанных с заболеваемостью астмой среди детей от 10 до 12-лет: расхождения и подобия двух статистических методов](http://www.hindawi.com/journals/ijpedi/2009/952042/), Джордж Антоногеоргос (George Antonogeorgos) и другие (Международный журнал по вопросам педиатрии, 2009 г.), ИД статьи 952042]
 
Логистическая модель —это параметрическая модель, и поэтому ее преимуществом является то, что она предоставляет информацию о влиянии каждой предикторной переменной на переменную ответа.


Благодаря наличию естественных вероятностных интерпретаций (в отличие от деревьев принятия решений или SVM) можно с легкостью обновлять модель для использования с новыми данными. Такую модель можно упорядочивать множеством способов. Кроме того, с ней не придется уделять много внимания корреляции признаков, как при использовании наивного байесовского классификатора. Логистическую регрессию удобно использовать если:

* вам нужна вероятностная платформа для классификации пороговых значений;
* вам нужно быстро включать дополнительные обучающие данные.  

Логистическая регрессия классифицирует данные высокой размерности лучше, чем дерево принятия решений. Пусть, например, нужно классифицировать текст более чем 100 тысяч документов, в которых содержатся 500 тысяч разных слов (признаков). В этом случае более эффективным является такое простое правило, как обучение линейной гиперплоскости, так как в деревьях принятия решений слишком много степеней свободы и для них характерно образование лжевзаимосвязей. Вы можете выбрать признаки, чтобы использовать их в дереве принятия решений для классификации текстовых данных, но в случае выбора слишком сокращенного подмножества признаков будет утеряно много ценных данных. При обучении модели используются с высокоразмерными данными, что способствует росту ошибок, связанных с дисперсией. В этом случае лучше использовать простые модели с высоким уровнем защиты.

Недостаток логистической регрессии заключается в нестабильной работе, которая проявляется, когда одним предиктором можно практически полностью объяснить переменную ответа, потому что коэффициент этой переменной резко увеличится.

<a name="anchor-5b"></a> **Преимущества и недостатки дерева принятия решений:** [деревья решений](http://research.microsoft.com/pubs/155552/decisionForests_MSR_TR_2011_114.pdf) легко интерпретировать и объяснять.

> Они непараметричны и с легкостью обрабатывают взаимодействия признаков, поэтому не нужно беспокоиться о выбросах или о линейной разделимости данных (например, деревья принятия решений легко справляются со случаями, когда на нижнем пределе некоего признака x находится класс A, в середине диапазона — класс B, а на верхнем пределе — также класс A). Один из недостатков заключается в том, что такие деревья принятия решений не поддерживают обучение по сети, поэтому при поступлении новых примеров дерево придется перестраивать. Другим недостатком является возникновение лжевзаимосвязей, но его легко исправить с помощью таких ансамблевых методов, как метод случайного леса (или увеличивающихся деревьев). Плюс, случайные леса справляются с большинством проблем в классификации (на мой взгляд, в этом они сильно опережают SVM). Они отличаются быстродействием, их можно масштабировать и не беспокоиться о настройке множества параметров, как в случае с SVM. [[Выбор классификатора машинного обучения](http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/), Эдвин Чен (Edwin Chen)]

Деревья принятия решений создадут результат в виде правил и метрик, таких как *Поддержка*, *Достоверность* и *Подъем*.


<a name="anchor-5c"></a> **Преимущества и недостатки методов опорных векторов:** Методы опорных векторов (SVM) эффективно работают в пространствах высокой размерности. Этот метод эффективен даже в тех случаях, когда количество измерений больше, чем количество примеров. Однако, если количество признаков значительно больше, чем количество примеров, применение этого метода, вероятно, будет не столь эффективным. Данный метод также эффективно использует память за счет применения подмножества точек обучения для функции принятия решений (известной как опорные векторы). Это очень универсальный метод: для функции принятия решения можно указывать различные функции ядра, общую и настраиваемую. Функция ядра используется для преобразования примеров для обучения низкой размерности в примеры для обучения более высокой размерности, что удобно для решения проблем линейной разделимости.

Однако SVM не предоставляет непосредственные оценки вероятности. Они вычисляются с помощью затратной пятикратной перекрестной проверки.

>[Существуют] теоретические гарантии высокой точности по поводу переобучения. При наличии соответствующего ядра они могут работать, даже если данные нельзя линейно разделить в пространстве базовой функции. [Машины опорных векторов] особенно широко применяются для решения проблем классификации текста, в которых высокоразмерные пробелы являются нормой. [[Выбор классификатора машинного обучения](http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/), Чен Эдвин]

В отличие от метода с использованием лесов, метод SVM был создан в качестве двухклассового классификатора. Тем не менее, недавно он был адаптирован для работы с несколькими классами. Мы можем использовать метод обучения по схеме «один по всем», чтобы создать многоклассовый классификатор, который может быть менее оптимальным. Так как SVM может обрабатывать только двухклассовые выходные данные (такие как категориальная выходная переменная с многообразием 2) с N классами, этот метод обучает N экземпляров SVM (SVM 1 изучает «Output==1» по «Output != 1», SVM 2 изучает «Output==2» по «Output != 2»,..., SVM N изучает «Output==N» по «Output != N»). Затем, чтобы спрогнозировать выходные данные для новых входных данных, создаются прогнозы для каждой модели SVM и определяется прогноз, которой наиболее приближен к области положительных результатов. [[Метод опорных векторов](http://www.astro.caltech.edu/~george/aybi199/AMooreTutorials/svm.ppt), Эндрю В. Мур (Andrew W. Moore) (Университет Карнеги — Меллон, 2001 г.)]

<a name="anchor-5d"></a> **Преимущества и недостатки упрощенного алгоритма Байеса:** [Классификаторы упрощенного алгоритма Байеса](http://www.aaai.org/Papers/FLAIRS/2004/Flairs04-097.pdf) часто применяются для решения проблем классификации. Они предполагают, что признаки независимы. Именно поэтому этот алгоритм называется «упрощенным».

> Если предположение условной независимости этого алгоритма подтверждается, классификатор упрощенного алгоритма Байеса сойдется быстрее, чем избирательные модели, такие как логистическая регрессия, поэтому вам нужно меньше обучающих данных. Даже если предположение этого алгоритма не подтверждается, классификатор упрощенного алгоритма Байеса все равно зачастую дает эффективные результаты. Его основной недостаток заключается в том, что он не может изучить взаимодействия признаков (например, его нельзя обучить тому, что хотя человеку нравятся фильмы с Брэдом Питтом и Toмом Крузом, он не выносит фильмы, в которых играют оба эти актера). [[Выбор классификатора машинного обучения](http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/), Эдвин Чен (Edwin Chen)]


<a name="anchor-5e"></a> **Один по всем** — это стратегия, которая позволяет устранить одну серьезную проблему, связанную с многоклассовой классификацией, сведя ее к набору нескольких задач двоичной классификации. Эта стратегия предусматривает обучение одного классификатора каждого класса, при котором примеры одного класса рассматриваются как положительные, а примеры всех других классов — как отрицательные. Для этой стратегии требуются базовые классификаторы, которые будут производить оценку вероятности для решения, а не только метку класса. Дискретные метки классов могут привести к неоднозначности, когда для одного примера прогнозируются несколько классов [[Многоклассовая классификация](http://en.wikipedia.org/wiki/Multiclass_classification) (Wikipedia 2006)].


<a name="anchor-6"></a>
##Регрессия
 
При регрессионном анализе новые значения прогнозируются на основе последнего вывода. Новые значения зависимой переменной вычисляются на основе значения одного или нескольких измеренных атрибутов. Ниже приведены различные алгоритмы регрессии, доступные в Машинном обучении Azure:

![screenshot\_of\_experiment](./media/machine-learning-algorithm-choice/help4.png)

В зависимости от сценария использования и имеющихся данных одному алгоритму отдается предпочтение над другим. Далее приведено описание некоторых алгоритмов регрессии и основных сценариев их использования.

<a name="anchor-6b"></a> **[Байесовская линейная регрессия][bayesian-linear-regression]** — это подход к линейной регрессии, который предусматривает выполнение статистического анализа в контексте байесовского вывода. Явные результаты доступны для апостериорных распределений вероятности параметров модели при гауссовском распределении ошибок модели, если можно предположить определенную форму предыдущего распределения.[[Линейная регрессия](http://en.wikipedia.org/wiki/Bayesian_linear_regression) ([Wikipedia](http://en.wikipedia.org))]

<a name="anchor-6f"></a> **[Регрессия увеличивающегося дерева принятия решений][boosted-decision-tree-regression]** Регрессия увеличивающегося дерева принятия решений позволяет вычислить связь между предиктором и переменными ответа. Структура дерева регрессии аналогична структуре дерева классификации. Конечные узлы являются спрогнозированными значениями функции (модели). Спрогнозированные значения ограничены значениями на конечных узлах. Ниже приведены некоторые преимущества использования деревьев принятия решений:

* правила принятия решений легко интерпретировать; 
* они непараметрические, поэтому можно легко использовать диапазон уровней числовых или категориальных данных и не нужно использовать унимодальные данные для обучения;
* они надежные с точки зрения выбросов в данных для обучения; 
* классификация быстро выполняется после разработки правил. 

Однако есть некоторые недостатки использования дерева принятия решений:

* при применении к полному набору данных они обычно переобучают обучающие данные, давая неточные результаты;
* невозможно предсказать переменную ответа в обучающих данных за пределами минимального и максимального значений.


<a name="anchor-6g"></a> **[Регрессия леса принятия решений][decision-forest-regression]** Леса принятия решений можно использовать для классификации (категориальные переменные) и регрессии (непрерывные переменные). Леса регрессии можно использовать для нелинейной регрессии зависимых переменных, если заданы независимые входные данные, при этом и входные и выходные данные могут быть многомерными. Леса регрессии используются не так часто, как аналогичные методы классификации. Основное отличие заключается в том, что метка выходных данных лесов принятия решений, которая будет связана с входными данными, а значит и обучающие метки, должны быть непрерывными. Следовательно, целевую функцию необходимо адаптировать соответствующим образом. Преимущества лесов регрессии во многом схожи с преимуществами лесов классификации, например эффективность и гибкость.

<a name="anchor-6a"></a> **[Линейная регрессия][linear-regression]** Линейная регрессия широко используется для моделирования связи между скалярной зависимой переменной Y и одной или несколькими независимыми переменными, обозначаемыми X. Как правило, ее используют для прогнозирования и сокращения объема. Линейную регрессию можно использовать, чтобы установить соответствие модели прогнозирования наблюдаемому набору данных значений Y и X. Этот тип регрессии предполагает, что базовая структура Y — это линейная комбинация переменных X. Если дополнительное значение X предоставляется без сопутствующего значения y, для прогнозирования этого значения Y можно использовать соответствующую модель линейной регрессии. Чтобы установить соответствие модели линейной регрессии, обычно используется подход наименьших квадратов, но для этого существуют и другие способы.[[Линейная регрессия](http://en.wikipedia.org/wiki/Bayesian_linear_regression) ([Wikipedia](http://en.wikipedia.org))]

<a name="anchor-6c"></a> **[Регрессия нейросетей][neural-network-regression]** Нейронные сети — удобный статистический инструмент для непараметрической регрессии. Непараметрическая регрессия устраняет проблему соответствия модели к переменной Y в наборе возможных пояснительных переменных X1; : : : ;Xp, и где связь между X и Y может быть сложнее, чем простая линейная связь. [[Платформа для непараметрической регрессии с использованием нейронных сетей](http://ftp.isds.duke.edu/WorkingPapers/00-32.pdf), Герберт К. Х. Ли (Herbert K. H. Lee) (ISDS, Университет Дьюка)]

<a name="anchor-6d"></a> **[Порядковая регрессия][ordinal-regression]** Порядковая регрессия — это тип регрессионного анализа, используемый для моделирования и прогнозирования порядковой зависимой переменной. Для порядковых зависимых переменных можно ранжировать значения, однако фактическое расстояние между категориями неизвестно. Важна только относительная упорядоченность разных значений. Так как метки или целевые значения имеют естественный порядок или ранжирование, в качестве порядковой цели можно использовать любой числовой столбец. Для ранжирования используется естественный порядок чисел. Например, болезни оцениваются по шкале от наименее серьезной до самой серьезной. Участники опроса выбирают ответы от «полностью согласен» до «категорически не согласен». Студенты оцениваются по шкале от 1 до 5. По сути, порядковая регрессия — является расширением логистической регрессии и основывается на модели *пропорциональной вероятности*.


<a name="anchor-6e"></a> **[Регрессия Пуассона][poisson-regression]** Регрессия Пуассона часто используется для моделирования счетных данных. Этот метод предполагает, что переменная ответа распределяется по модели Пуассона. Данные, распределенные по модели Пуассона, по своей сути являются целыми значениями (дискретными и положительными), что логично для счетных данных. В случае регрессии Пуассона на основе заданного набора данных для обучения предпринимается попытка найти оптимальные значения путем максимального увеличения логарифмического правдоподобия параметров при заданных входных данных. Правдоподобие параметров — это вероятность того, что данные для обучения были отобраны из распределения с использованием этих параметров. Например, регрессию Пуассона удобно использовать в следующих случаях:

* моделирование числа случаев простуды при авиаперелетах; 
* оценка количества звонков, связанных с мероприятием или рекламной акцией; 
* создание таблиц вероятностей.

## Ссылки

Полный список всех алгоритмов машинного обучения, доступных в Студии машинного обучения, см. в разделе [Инициализация модели](https://msdn.microsoft.com/library/azure/0c67013c-bfbc-428b-87f3-f552d8dd41f6/) в [справке по алгоритмам и модулям Студии машинного обучения](https://msdn.microsoft.com/library/azure/dn905974.aspx).

Дополнительные сведения обо всех типах алгоритмов машинного обучения можно найти в следующих документах, многие из которых были использованы при создании этой статьи.

* [Choosing a Machine Learning Classifier](http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/), Edwin Chen.

* [Decision Forests for Classification, Regression, Density Estimation, Manifold Learning and Semi-Supervised Learning](http://research.microsoft.com/pubs/155552/decisionForests_MSR_TR_2011_114.pdf), A. Criminisi1, J. Shotton2 and E. Konukoglu (Microsoft Research, 2011) technical report TR-2011-114.

* [A Framework for Nonparametric Regression Using Neural Networks](http://ftp.isds.duke.edu/WorkingPapers/00-32.pdf), Herbert K. H. Lee (ISDS, Duke University).

* Handbook of Computational Statistics: Concepts and Methods edited by James E. Gentle, Wolfgang Karl Härdle, Yuichi Mori (Springer-Verlag Berlin Heidelberg New York, 2004).

* [Logistic Regression and Linear Discriminant Analyses in Evaluating Factors Associated with Asthma Prevalence among 10- to 12-Years-Old Children: Divergence and Similarity of the Two Statistical Methods](http://www.hindawi.com/journals/ijpedi/2009/952042/), George Antonogeorgos, Demosthenes B. Panagiotakos, Kostas N. Priftis, and Anastasia Tzonou (International Journal of Pediatrics, 2009) Article ID 952042.

* [The Optimality of Naive Bayes](http://www.aaai.org/Papers/FLAIRS/2004/Flairs04-097.pdf) (University of New Brunswick 2004) Harry Zhang.

* [Support Vector Machines](http://www.astro.caltech.edu/~george/aybi199/AMooreTutorials/svm.ppt), Andrew W. Moore (Carnegie Mellon University 2001).

* [Machine Learning, Part II: Supervised and Unsupervised Learning](http://www.aihorizon.com/essays/generalai/supervised_unsupervised_machine_learning.htm), [AI Horizon](http://www.aihorizon.com/).

* [What are the advantages of logistic regression over decision trees?](http://www.quora.com/What-are-the-advantages-of-logistic-regression-over-decision-trees) ([Quora](http://www.quora.com/)).

* [Какова разница между контролируемым и неконтролируемым обучением?](http://stackoverflow.com/questions/1832076/what-is-the-difference-between-supervised-learning-and-unsupervised-learning) ([Stackoverflow](http://stackoverflow.com/)).

* [Когда следует использовать классификатор машинного обучения?](http://stackoverflow.com/questions/2595176/when-to-choose-which-machine-learning-classifier) ([Stackoverflow](http://stackoverflow.com/)).

* [Wikipedia](http://en.wikipedia.org):
	* [Bayesian linear regression](http://en.wikipedia.org/wiki/Bayesian_linear_regression)
	* [Linear regression](http://en.wikipedia.org/wiki/Linear_regression)
	* [Multiclass classification](http://en.wikipedia.org/wiki/Multiclass_classification)
	* [Unsupervised learning](http://en.wikipedia.org/wiki/Unsupervised_learning)

Также:

* [Microsoft Azure Machine Learning Algorithm Cheat Sheet](machine-learning-algorithm-cheat-sheet.md) (Microsoft).

* [Выбор правильного оценщика](http://scikit-learn.org/stable/tutorial/machine_learning_map/) ([scikit-learn](http://scikit-learn.org/stable/index.html)).


<!-- Module References -->
[k-means-clustering]: https://msdn.microsoft.com/library/azure/5049a09b-bd90-4c4e-9b46-7c87e3a36810/
[train-clustering-model]: https://msdn.microsoft.com/library/azure/bb43c744-f7fa-41d0-ae67-74ae75da3ffd/
[bayesian-linear-regression]: https://msdn.microsoft.com/library/azure/ee12de50-2b34-4145-aec0-23e0485da308/
[boosted-decision-tree-regression]: https://msdn.microsoft.com/library/azure/0207d252-6c41-4c77-84c3-73bdf1ac5960/
[decision-forest-regression]: https://msdn.microsoft.com/library/azure/562988b2-e740-4e3a-8131-358391bad755/
[linear-regression]: https://msdn.microsoft.com/library/azure/31960a6f-789b-4cf7-88d6-2e1152c0bd1a/
[neural-network-regression]: https://msdn.microsoft.com/library/azure/d7ee222c-669f-4200-a576-a761a9c1a928/
[ordinal-regression]: https://msdn.microsoft.com/library/azure/ffb557f8-dc7f-44bd-8fd0-b25666dd23f1/
[poisson-regression]: https://msdn.microsoft.com/library/azure/80e21b9d-3827-40d8-b733-b53148becbc2/

 

<!---HONumber=August15_HO6-->