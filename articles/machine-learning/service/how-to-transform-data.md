---
title: Преобразование данных с помощью пакета SDK Python для подготовки данных машинного обучения Azure
description: Дополнительные сведения о преобразовании и очистке данных с помощью пакета SDK для подготовки данных машинного обучения Azure. Чтобы добавлять столбцы, отфильтровывать нежелательные строки или столбцы и ввести отсутствующие значения, используйте методы преобразования.
services: machine-learning
ms.service: machine-learning
ms.component: core
ms.topic: conceptual
ms.author: cforbe
author: cforbe
manager: cgronlun
ms.reviewer: jmartens
ms.date: 09/24/2018
ms.openlocfilehash: 06e7d227511a9b651a905df3172f59a191acce01
ms.sourcegitcommit: 9e179a577533ab3b2c0c7a4899ae13a7a0d5252b
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 10/23/2018
ms.locfileid: "49945677"
---
# <a name="transform-data-with-the-azure-machine-learning-data-prep-sdk"></a>Преобразование данных с помощью пакета SDK для подготовки данных машинного обучения Azure

[Пакет SDK для подготовки данных машинного обучения Azure](https://docs.microsoft.com/python/api/overview/azure/dataprep?view=azure-dataprep-py) предлагает другие методы преобразование для очистки данных. Эти методы упрощают добавление столбцов, фильтрование нежелательных строк или столбцов и ввод отсутствующих значений.

В настоящее время существуют методы для выполнения следующих задач.
- [Добавление столбца с помощью выражения](#column)
- [Добавление отсутствующих значений](#impute-missing-values)
- [Получение столбца по образцу](#derive-column-by-example)
- [Фильтрация](#filtering)
- [Пользовательские преобразования Python](#custom-python-transforms)

<a name=column>
## <a name="add-column-using-an-expression"></a>Добавление столбца с помощью выражения

Пакет SDK для подготовки данных машинного обучения Azure включает выражения `substring`, которые можно использовать для вычисления значения из существующих столбцов и потом указать это значение в новом столбце. В этом примере мы загрузим данные и попытаемся добавить столбцы к входным данным.

```
import azureml.dataprep as dprep

# loading data
dataflow = dprep.read_csv(path=r'data\crime0-10.csv')
dataflow.head(3)
```

||ИД|Серийный номер|Дата|Блок|IUCR|Основной тип|ОПИСАНИЕ|Описание расположения|Фикс.|Дом|...|Административный район|Жилой микрорайон|Код ФБР|Координата X|Координата Y|Год|Обновлено|Широта|Долгота|Расположение|
|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|
|0|10140490|HY329907|05.07.2015 23:50:00|050XX N NEWLAND AVE|0820|КРАЖА|500 ДОЛЛ. США И МЕНЬШЕ|УЛИЦА|false|false|...|41|10|06|1129230|1933315|2015|12.07.2015 |12:42:46|41.973309466|-87.800174996|(41.973309466, -87.800174996)|
|1|10139776|HY329265|05.07.2015 23:30:00|011XX W MORSE AVE|0460|АККУМУЛЯТОР|ПРОСТОЙ|УЛИЦА|false|Да|...|49|1|08B|1167370|1946271|2015|12.07.2015 12:42:46|42.008124017|-87.65955018|(42.008124017, -87.65955018)|
|2|10140270|HY329253|05.07.2015 23:20:00|121XX S FRONT AVE|0486|АККУМУЛЯТОР|ПРОСТОЙ АККУМУЛЯТОР ДЛЯ ДОМАШНЕГО ПОЛЬЗОВАНИЯ|УЛИЦА|false|Да|...|9|53|08B|||2015|12.07.2015 12:42:46|



Используйте выражение `substring(start, length)` для извлечения кода из столбца из столбца "Номер регистра" и помещения этих данных в новый столбец "Категория регистра".

```
substring_expression = dprep.col('Case Number').substring(0, 2)
case_category = dataflow.add_column(new_column_name='Case Category',
                                    prior_column='Case Number',
                                    expression=substring_expression)
case_category.head(3)
```

||ИД|Серийный номер|Категория регистра|Дата|Блок|IUCR|Основной тип|ОПИСАНИЕ|Описание расположения|Фикс.|...|Административный район|Жилой микрорайон|Код ФБР|Координата X|Координата Y|Год|Обновлено|Широта|Долгота|Расположение|
|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|------|
|0|10140490|HY329907|HY|05.07.2015 23:50:00|050XX N NEWLAND AVE|0820|КРАЖА|500 ДОЛЛ. США И МЕНЬШЕ|УЛИЦА|false|false|...|41|10|06|1129230|1933315|2015|12.07.2015 |12:42:46|41.973309466|-87.800174996|(41.973309466, -87.800174996)|
|1|10139776|HY329265|HY|05.07.2015 23:30:00|011XX W MORSE AVE|0460|АККУМУЛЯТОР|ПРОСТОЙ|УЛИЦА|false|Да|...|49|1|08B|1167370|1946271|2015|12.07.2015 12:42:46|42.008124017|-87.65955018|(42.008124017, -87.65955018)|
|2|10140270|HY329253|HY|05.07.2015 23:20:00|121XX S FRONT AVE|0486|АККУМУЛЯТОР|ПРОСТОЙ АККУМУЛЯТОР ДЛЯ ДОМАШНЕГО ПОЛЬЗОВАНИЯ|УЛИЦА|false|Да|...|9|53|08B|||2015|12.07.2015 12:42:46|



Используйте выражение`substring(start)`, чтобы извлечь номер из столбца "Номер регистра", а затем преобразовать его в числовой тип данных и поместить в новый столбец "Идентификатор регистра".
```
substring_expression2 = dprep.col('Case Number').substring(2)
case_id = dataflow.add_column(new_column_name='Case Id',
                              prior_column='Case Number',
                              expression=substring_expression2)
case_id = case_id.to_number('Case Id')
case_id.head(3)
```

||ИД|Серийный номер|Идентификатор|Дата|Блок|IUCR|Основной тип|ОПИСАНИЕ|Описание расположения|Фикс.|...|Административный район|Жилой микрорайон|Код ФБР|Координата X|Координата Y|Год|Обновлено|Широта|Долгота|Расположение|
|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|------|
|0|10140490|HY329907|329907.0|05.07.2015 23:50:00|050XX N NEWLAND AVE|0820|КРАЖА|500 ДОЛЛ. США И МЕНЬШЕ|УЛИЦА|false|false|...|41|10|06|1129230|1933315|2015|12.07.2015 |12:42:46|41.973309466|-87.800174996|(41.973309466, -87.800174996)|
|1|10139776|HY329265|329265.0|05.07.2015 23:30:00|011XX W MORSE AVE|0460|АККУМУЛЯТОР|ПРОСТОЙ|УЛИЦА|false|Да|...|49|1|08B|1167370|1946271|2015|12.07.2015 12:42:46|42.008124017|-87.65955018|(42.008124017, -87.65955018)|
|2|10140270|HY329253|329253.0|05.07.2015 23:20:00|121XX S FRONT AVE|0486|АККУМУЛЯТОР|ПРОСТОЙ АККУМУЛЯТОР ДЛЯ ДОМАШНЕГО ПОЛЬЗОВАНИЯ|УЛИЦА|false|Да|...|9|53|08B|||2015|12.07.2015 12:42:46|

## <a name="impute-missing-values"></a>Добавление отсутствующих значений

Пакет SDK для подготовки данных машинного обучения Azure может добавлять отсутствующие значения в указанных столбцах. В этом примере вы загрузите значения широты и долготы, а затем попытаетесь ввести отсутствующие значения во входные данные.

```
import azureml.dataprep as dprep

# loading input data
df = dprep.read_csv(r'data\crime0-10.csv')
df = df.keep_columns(['ID', 'Arrest', 'Latitude', 'Longitude'])
df = df.to_number(['Latitude', 'Longitude'])
df.head(5)
```

||ИД|Фикс.|Широта|Долгота|
|-----|------|-----|------|-----|
|0|10140490|false|41.973309|-87.800175|
|1|10139776|false|42.008124|-87.659550|
|2|10140270|false|NaN|NaN|
|3|10139885|false|41.902152|-87.754883|
|4.|10140379|false|41.885610|-87.657009|

В третьей записи отсутствуют значения широты и долготы. Чтобы добавить отсутствующие значения, для изучения исправленной программы можно использовать `ImputeMissingValuesBuilder`. Он может добавлять в столбцы рассчитанное значение `MIN`, `MAX` или `MEAN`, либо значение `CUSTOM`. Если указано `group_by_columns`, то отсутствующие значения будут добавляться группой вместе с вычисленными `MIN`, `MAX` и `MEAN` для каждой группы.

Сначала быстро проверьте значение `MEAN` в столбце широты.
```
df_mean = df.summarize(group_by_columns=['Arrest'],
                       summary_columns=[dprep.SummaryColumnsValue(column_id='Latitude',
                                                                 summary_column_name='Latitude_MEAN',
                                                                 summary_function=dprep.SummaryFunction.MEAN)])
df_mean = df_mean.filter(dprep.col('Arrest') == 'false')
df_mean.head(1)
```

||Фикс.|Latitude_MEAN|
|-----|-----|----|
|0|false|41.878961|

Значение широты `MEAN` в порядке, поэтому его можно использовать, чтобы добавить широту. Мы заполним отсутствующее значение долготы числом 42, используя внешний источник данных.


```
# impute with MEAN
impute_mean = dprep.ImputeColumnArguments(column_id='Latitude',
                                          impute_function=dprep.ReplaceValueFunction.MEAN)
# impute with custom value 42
impute_custom = dprep.ImputeColumnArguments(column_id='Longitude',
                                            custom_impute_value=42)
# get instance of ImputeMissingValuesBuilder
impute_builder = df.builders.impute_missing_values(impute_columns=[impute_mean, impute_custom],
                                                   group_by_columns=['Arrest'])
# call learn() to learn a fixed program to impute missing values
impute_builder.learn()
# call to_dataflow() to get a data flow with impute step added
df_imputed = impute_builder.to_dataflow()

# check impute result
df_imputed.head(5)
```

||ИД|Фикс.|Широта|Долгота|
|-----|------|-----|------|-----|
|0|10140490|false|41.973309|-87.800175|
|1|10139776|false|42.008124|-87.659550|
|2|10140270|false|41.878961|42.000000|
|3|10139885|false|41.902152|-87.754883|
|4.|10140379|false|41.885610|-87.657009|

Как показано в приведенном выше результате, отсутствующая широта была добавлена с помощью значения `MEAN` группы `Arrest=='false'`. Для отсутствующего значения долготы было использовано число 42.
```
imputed_longitude = df_imputed.to_pandas_dataframe()['Longitude'][2]
assert imputed_longitude == 42
```

## <a name="derive-column-by-example"></a>Получение столбца по образцу
Одним из наиболее передовых инструментов пакета SDK для подготовки данных машинного обучения Azure является способность выводить столбцы, используя примеры желаемых результатов. Это позволяет дать пакету SDK пример, используя который он сможет сгенерировать код для получения нужного результата.

```
import azureml.dataprep as dprep
dataflow = dprep.read_csv(path='https://dpreptestfiles.blob.core.windows.net/testfiles/BostonWeather.csv')
df = dataflow.head(10)
df
```
||DATE|REPORTTPYE;|HOURLYDRYBULBTEMPF;|HOURLYRelativeHumidity;|HOURLYWindSpeed.|
|----|----|----|----|----|----|
|0|01.01.2015 0:54|FM-15|22|50|10|
|1|01.01.2015 1:00|FM-12|22|50|10|
|2|01.01.2015 1:54|FM-15|22|50|10|
|3|01.01.2015 2:54|FM-15|22|50|11|
|4.|01.01.2015 3:54|FM-15|24|46|13|
|5|01.01.2015 4:00|FM-12|24|46|13|
|6|01.01.2015 4:54|FM-15|22|52|15|
|7|01.01.2015 5:54|FM-15|23|48|17|
|8|01.01.2015 6:54|FM-15|23|50|14|
|9|01.01.2015 7:00|FM-12|23|50|14|

Как можно видеть, это довольно простой файл. Однако предположим, что вам потребуется объединить этот файл с набором данных, в которых формат даты и времени — "10 марта 2018 г. 2:00–4:00".

Данные можно преобразовать в необходимый формат.

```
builder = dataflow.builders.derive_column_by_example(source_columns=['DATE'], new_column_name='date_timerange')
builder.add_example(source_data=df.iloc[1], example_value='Jan 1, 2015 12AM-2AM')
builder.preview() 
```

||DATE|date_timerange|
|----|----|----|
|0|01.01.2015 0:54|1 января 2015 г. 00:00–2:00|
|1|01.01.2015 1:00|1 января 2015 г. 00:00–2:00|
|2|01.01.2015 1:54|1 января 2015 г. 00:00–2:00|
|3|01.01.2015 2:54|1 января 2015 г. 2:00–4:00|
|4.|01.01.2015 3:54|1 января 2015 г. 2:00–4:00|
|5|01.01.2015 4:00|1 января 2015 г. 4:00–6:00|
|6|01.01.2015 4:54|1 января 2015 г. 4:00–6:00|
|7|01.01.2015 5:54|1 января 2015 г. 4:00–6:00|
|8|01.01.2015 6:54|1 января 2015 г. 6:00–8:00|
|9|01.01.2015 7:00|1 января 2015 г. 6:00–8:00|

Приведенный выше код сначала создает для производного столбца построитель. Вы указали массив исходных столбцов, который учитывает (`DATE`) и имя нового столбца, который необходимо добавить.

Затем, как и в первом примере, вы переходите во вторую строку (индекс 1) и получаете ожидаемое значение для производного столбца.

Наконец, вы вызываете `builder.preview()` и видите производный столбец рядом с исходным. Кажется, что с форматом все в порядке, но вы видите только значения той же даты "1 января 2015 г.".

Теперь укажите, сколько строк сверху вы хотите `skip`, чтобы увидеть строки ниже.

```
preview_df = builder.preview(skip=30)
preview_df
```

||DATE|date_timerange|
|-----|-----|-----|
|30|1.11.2015 22:54|1 января 2015 г. 22:00–00:00|
|31|1.11.2015 2015 23:54|1 января 2015 г. 22:00–00:00|
|32|1.11.2015 23:59|1 января 2015 г. 22:00–00:00|
|33|2.11.2015 0:54|1 февраля 2015 г. 00:00–2:00|
|34|2.11.2015 1:00|1 февраля 2015 г. 00:00–2:00|
|35|2.11./2015 1:54|1 февраля 2015 г. 00:00–2:00|
|36|2.11.2015 2:54|1 февраля 2015 г. 2:00–4:00|
|37|2.11.2015 3:54|1 февраля 2015 г. 2:00–4:00|
|38|2.11.2015 4:00|1 февраля 2015 г. 4:00–6:00|
|11,9|2.11.2015 4:54|1 февраля 2015 г. 4:00–6:00|

Здесь вы можете увидеть проблему со сгенерированной программой. Поскольку она основана исключительно на одном примере, который вы указали выше, программа выбрала формат синтаксической проверки даты "день.месяц.год", который не является тем, который вам нужен в этом варианте.

Чтобы устранить эту проблему, необходимо предоставить другой пример.

```
builder.add_example(source_data=preview_df.iloc[3], example_value='Jan 2, 2015 12AM-2AM')
preview_df = builder.preview(skip=30, count=10)
preview_df
```

||DATE|date_timerange|
|-----|-----|-----|
|30|01.01.2015 22:54|1 января 2015 г. 22:00–00:00|
|31|01.01.2015 23:54|1 января 2015 г. 22:00–00:00|
|32|01.01.2015 23:59|1 января 2015 г. 22:00–00:00|
|33|02.01.2015 0:54|2 января 2015 г. 12:00–2:00|
|34|02.01.2015 1:00|2 января 2015 г. 12:00–2:00|
|35|02.01.2015 1:54|2 января 2015 г. 12:00–2:00|
|36|02.01.2015 2:54|2 января 2015 г. 2:00–4:00|
|37|02.01.2015 3:54|2 января 2015 г. 2:00–4:00|
|38|02.01.2015 4:00|2 января 2015 г. 4:00–6:00|
|11,9|02.01.2015 4:54|2 января 2015 г. 4:00–6:00|


Теперь строки "02.01.2015" правильно обрабатываются как "2 января 2015 г.", но если вы посмотрите на производный столбец, вы увидите, что в итоге в нем нет значений. Чтобы устранить эту проблему, необходимо предоставить другой пример для записи 66.

```
builder.add_example(source_data=preview_df.iloc[66], example_value='Jan 29, 2015 8PM-10PM')
builder.preview(count=10)
```

||DATE|date_timerange|
|-----|-----|-----|
|0|01.01.2015 22:54|1 января 2015 г. 22:00–00:00|
|1|01.01.2015 23:54|1 января 2015 г. 22:00–00:00|
|2|01.01.2015 23:59|1 января 2015 г. 22:00–00:00|
|3|02.01.2015 0:54|2 января 2015 г. 12:00–2:00|
|4.|02.01.2015 1:00|2 января 2015 г. 12:00–2:00|
|5|02.01.2015 1:54|2 января 2015 г. 12:00–2:00|
|6|02.01.2015 2:54|2 января 2015 г. 2:00–4:00|
|7|02.01.2015 3:54|2 января 2015 г. 2:00–4:00|
|8|02.01.2015 4:00|2 января 2015 г. 4:00–6:00|
|9|02.01.2015 4:54|2 января 2015 г. 4:00–6:00|

Все кажется хорошо, но вы замечаете, что это не совсем то, чего мы хотели. Чтобы создать правильный формат, вам нужно отделить дату и время с помощью символа "|".

Чтобы исправить эту ошибку, можно добавить еще один пример. На этот раз вместо того, чтобы использовать строку предварительной версии, создайте словарь имени столбца, чтобы задать значения для параметра `source_data`.

```
builder.add_example(source_data={'DATE': '11/11/2015 0:54'}, example_value='Nov 11, 2015 | 12AM-2AM')
builder.preview(count=10)
```
||DATE|date_timerange|
|-----|-----|-----|
|0|01.01.2015 22:54|None|
|1|01.01.2015 23:54|None|
|2|01.01.2015 23:59|None|
|3|02.01.2015 0:54|None|
|4.|02.01.2015 1:00|None|
|5|02.01.2015 1:54|None|
|6|02.01.2015 2:54|None|
|7|02.01.2015 3:54|None|
|8|02.01.2015 4:00|None|
|9|02.01.2015 4:54|None|

Это явно имело отрицательные последствия, так как теперь единственными строками со значениями в производном столбце, являются те, которые точно соответствуют предоставленным примерам.

Рассмотрим несколько примеров.
```
examples = builder.list_examples()
examples
```

| |DATE|пример|example_id|
| -------- | -------- | -------- | -------- |
|0|01.01.2015 1:00|1 января 2015 г. 00:00–2:00|-1|
|1|02.01.2015 0:54|2 января 2015 г. 12:00–2:00|-2|
|2|29.01.2015 20:54|29 января 2015 г. 20:00–22:00|-3|
|3|11.11.2015 0:54|11 ноября 2015 г. \| 00:00–2:00|-4|

Вы видите, что примеры несогласованны. Чтобы устранить эту проблему, нам необходимо сделать исправления в первых трех примерах (включая символ "|" между датой и временем).

Этого можно достичь, удалив примеры, которые являются неправильными (с помощью перехода из Pandas DataFrame в `example_row` или путем перехода в значение `example_id`), и внеся в примеры новые изменения.

```
builder.delete_example(example_id=-1)
builder.delete_example(example_row=examples.iloc[1])
builder.delete_example(example_row=examples.iloc[2])
builder.add_example(examples.iloc[0], 'Jan 1, 2015 | 12AM-2AM')
builder.add_example(examples.iloc[1], 'Jan 2, 2015 | 12AM-2AM')
builder.add_example(examples.iloc[2], 'Jan 29, 2015 | 8PM-10PM')
builder.preview()
```

| | DATE | date_timerange |
| -------- | -------- | -------- |
| 0 | 01.01.2015 0:54 | 1 января 2015 г. \| 00:00–2:00 |
| 1 | 01.01.2015 1:00 | 1 января 2015 г. \| 00:00–2:00 |
| 2 | 01.01.2015 1:54 | 1 января 2015 г. \| 00:00–2:00 |
| 3 | 01.01.2015 2:54 | 1 января 2015 г. \| 2:00–4:00 |
| 4. | 01.01.2015 3:54 | 1 января 2015 г. \| 2:00–4:00 |
| 5 | 01.01.2015 4:00 | 1 января 2015 г. \| 4:00–6:00|
| 6 | 01.01.2015 4:54 | 1 января 2015 г. \| 4:00–6:00|
| 7 | 01.01.2015 5:54 | 1 января 2015 г. \| 4:00–6:00|
| 8 | 01.01.2015 6:54 | 1 января 2015 г. \| 6:00–8:00|
| 9 | 01.01.2015 7:00 | 1 января 2015 г. \| 6:00–8:00|

Теперь формат данных правильный и мы можем наконец вызвать `to_dataflow()` построителя, который возвратит поток данных с добавлением требуемых производных столбцов.

```
dataflow = builder.to_dataflow()
df = dataflow.to_pandas_dataframe()
df
```

## <a name="filtering"></a>Фильтрация

Пакет SDK содержит методы `Dataflow.drop_columns` и `Dataflow.filter`, которые позволяют отфильтровать столбцы или строки.

### <a name="initial-setup"></a>Начальная настройка
```
import azureml.dataprep as dprep
from datetime import datetime
dataflow = dprep.read_csv(path='https://dprepdata.blob.core.windows.net/demo/green-small/*')
dataflow.head(5)
```
||lpep_pickup_datetime|Lpep_dropoff_datetime|store_and_fwd_flag|RateCodeID|Pickup_longitude|Pickup_latitude|Dropoff_longitude|Dropoff_latitude|Passenger_count|Trip_distance|Tip_amount|Tolls_amount|Total_amount|
|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|
|0|None|None|None|None|None|None|None|None|None|None|None|None|None|
|1|2013-08-01 08:14:37|2013-08-01 09:09:06|Нет|1|0|0|0|0|1|.00|0|0|21.25|
|2|2013-08-01 09:13:00|2013-08-01 11:38:00|Нет|1|0|0|0|0|2|.00|0|0|75|
|3|2013-08-01 09:48:00|2013-08-01 09:49:00|Нет|5|0|0|0|0|1|.00|0|1|2,1|
|4.|2013-08-01 10:38:35|2013-08-01 10:38:51|Нет|1|0|0|0|0|1|.00|0|0|3.25|

### <a name="filtering-columns"></a>Фильтрация столбцов

Для фильтрации столбцов используйте `Dataflow.drop_columns`. Этот метод использует список столбцов для удаления или более сложный аргумент под названием `ColumnSelector`.

#### <a name="filtering-columns-with-list-of-strings"></a>Фильтрация столбцов со списком строк

В этом примере `drop_columns` использует список строк. Каждая строка должна точно соответствовать необходимому столбцу, который подлежит удалению.

``` 
dataflow = dataflow.drop_columns(['Store_and_fwd_flag', 'RateCodeID'])
dataflow.head(5)
```
||lpep_pickup_datetime|Lpep_dropoff_datetime|Pickup_longitude|Pickup_latitude|Dropoff_longitude|Dropoff_latitude|Passenger_count|Trip_distance|Tip_amount|Tolls_amount|Total_amount|
|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|
|0|None|None|None|None|None|None|None|None|None|None|None|
|1|2013-08-01 08:14:37|2013-08-01 09:09:06|0|0|0|0|1|.00|0|0|21.25|
|2|2013-08-01 09:13:00|2013-08-01 11:38:00|0|0|0|0|2|.00|0|0|75|
|3|2013-08-01 09:48:00|2013-08-01 09:49:00|0|0|0|0|1|.00|0|1|2,1|
|4.|2013-08-01 10:38:35|2013-08-01 10:38:51|0|0|0|0|1|.00|0|0|3.25|

#### <a name="filtering-columns-with-regex"></a>Фильтрация столбцов с регулярным выражением
Кроме того, для удаления столбцов, соответствующих регулярному выражению, можно использовать выражение `ColumnSelector`. В этом примере мы удаляем все столбцы, соответствующие выражению `Column*|.*longitude|.*latitude`.

```
dataflow = dataflow.drop_columns(dprep.ColumnSelector('Column*|.*longitud|.*latitude', True, True))
dataflow.head(5)
```
||lpep_pickup_datetime|Lpep_dropoff_datetime|Passenger_count|Trip_distance|Tip_amount|Tolls_amount|Total_amount|
|-----|-----|-----|-----|-----|-----|-----|-----|
|0|None|None|None|None|None|None|None|
|1|2013-08-01 08:14:37|2013-08-01 09:09:06|1|.00|0|0|21.25|
|2|2013-08-01 09:13:00|2013-08-01 11:38:00|2|.00|0|0|75|
|3|2013-08-01 09:48:00|2013-08-01 09:49:00|1|.00|0|1|2,1|
|4.|2013-08-01 10:38:35|2013-08-01 10:38:51|1|.00|0|0|3.25|

## <a name="filtering-rows"></a>Фильтрация строк

Для фильтрации строк используйте `DataFlow.filter`. Этот метод принимает в качестве аргумента выражение пакета SDK для подготовки данных машинного обучения Azure и возвращает новый поток данных со строками, которые выражение оценивает как True. Выражения строятся с использованием построителя выражений (`col`, `f_not`, `f_and`, `f_or`) и обычных операторов (>, <, >=, <=, ==,!=).

### <a name="filtering-rows-with-simple-expressions"></a>Фильтрация строк с помощью простых выражений

С помощью построителя выражений `col` укажите имя столбца в качестве строкового аргумента `col('column_name')` и создайте выражение в сочетании с одним из перечисленных ниже стандартных операторов >, <, >=, <=, ==, !=, например `col('Tip_amount') > 0`. Наконец, используйте созданные выражения в функции `Dataflow.filter`.

В этом примере `dataflow.filter(col('Tip_amount') > 0)` возвращает поток данных со строками, в которых значение `Tip_amount` больше 0.

> [!NOTE] 
> `Tip_amount` сначала преобразуется в число, которое позволяет нам построить выражение, сравнивая его с другими числовыми значениями.

```
dataflow = dataflow.to_number(['Tip_amount'])
dataflow = dataflow.filter(dprep.col('Tip_amount') > 0)
dataflow.head(5)
```
||lpep_pickup_datetime|Lpep_dropoff_datetime|Passenger_count|Trip_distance|Tip_amount|Tolls_amount|Total_amount|
|-----|-----|-----|-----|-----|-----|-----|-----|
|0|2013-08-01 19:33:28|2013-08-01 19:35:21|5|.00|0.08|0|4.58|
|1|2013-08-05 13:16:38|2013-08-05 13:18:24|1|.00|0,30|0|3.8|
|2|2013-08-05 14:11:42|2013-08-05 14:12:47|1|.00|1.05|0|4.55|
|3|2013-08-05 14:15:56|2013-08-05 14:18:04|5|.00|2.22|0|5.72|
|4.|2013-08-05 14:42:14|2013-08-05 14:42:38|1|.00|0.88|0|4.38|

### <a name="filtering-rows-with-complex-expressions"></a>Фильтрация строк с помощью сложных выражений

Для фильтрации с помощью сложных выражений, объедините одно или несколько простых выражений с помощью построителя выражений `f_not`, `f_and`, или `f_or`.

В этом примере `Dataflow.filter` возвращает поток данных со строками, в которых значение `'Passenger_count'` меньше 5, а значение `'Tolls_amount'` больше 0.

```
dataflow = dataflow.to_number(['Passenger_count', 'Tolls_amount'])
dataflow = dataflow.filter(dprep.f_and(dprep.col('Passenger_count') < 5, dprep.col('Tolls_amount') > 0))
dataflow.head(5)
```
||lpep_pickup_datetime|Lpep_dropoff_datetime|Passenger_count|Trip_distance|Tip_amount|Tolls_amount|Total_amount|
|-----|-----|-----|-----|-----|-----|-----|-----|
|0|2013-08-08 12:16:00|2013-08-08 12:16:00|1.0|.00|2.25|5.00|19.75|
|1|2013-08-12 14:43:53|2013-08-12 15:04:50|1.0|5.28|6.46|5.33|32.29|
|2|2013-08-12 19:48:12|2013-08-12 20:03:42|1.0|5.50|1.00|10.66|30.66|
|3|2013-08-13 06:11:06|2013-08-13 06:30:28|1.0|9.57|7.47|5.33|44.8|
|4.|2013-08-16 20:33:50|2013-08-16 20:48:50|1.0|5.63|3.00|5.33|27.83|

Вы также можете отфильтровать строки, объединив несколько построителей выражений и создав вложенное выражение.

> [!NOTE]
> `lpep_pickup_datetime` и `Lpep_dropoff_datetime` сначала преобразуются в дату и время (DateTime), что позволяет нам построить выражение, сравнивая его с другими значениями даты и времени (DateTime).

```
dataflow = dataflow.to_datetime(['lpep_pickup_datetime', 'Lpep_dropoff_datetime'], ['%Y-%m-%d %H:%M:%S'])
dataflow = dataflow.to_number(['Total_amount', 'Trip_distance'])
mid_2013 = datetime(2013,7,1)
dataflow = dataflow.filter(
    dprep.f_and(
        dprep.f_or(
            dprep.col('lpep_pickup_datetime') > mid_2013,
            dprep.col('Lpep_dropoff_datetime') > mid_2013),
        dprep.f_and(
            dprep.col('Total_amount') > 40,
            dprep.col('Trip_distance') < 10)))
dataflow.head(5)
```

||lpep_pickup_datetime|Lpep_dropoff_datetime|Passenger_count|Trip_distance|Tip_amount|Tolls_amount|Total_amount|
|-----|-----|-----|-----|-----|-----|-----|-----|
|0|2013-08-13 06:11:06+00:00|2013-08-13 06:30:28+00:00|1.0|9.57|7.47|5.33|44.80|
|1|2013-08-23 12:28:20+00:00|2013-08-23 12:50:28+00:00|2,0|8,22|8.08|5.33|40.41|
|2|2013-08-25 09:12:52+00:00|2013-08-25 09:34:34+00:00|1.0|8.80|8.33|5.33|41.66|
|3|2013-08-25 16:46:51+00:00|2013-08-25 17:13:55+00:00|2,0|9.66|7.37|5.33|44.20|
|4.|2013-08-25 17:42:11+00:00|2013-08-25 18:02:57+00:00|1.0|9.60|6.87|5.33|41.20|

## <a name="custom-python-transforms"></a>Пользовательские преобразования Python 

Существуют сценарии, в которых самое простое решение — это написать код Python. Пакет SDK позволяет решить эту задачу с помощью 3 разных расширений.

- Столбец нового скрипта
- Фильтр нового скрипта
- Преобразование секции

Каждое из расширений поддерживает вертикальное и горизонтальное масштабирование среды выполнения. Ключевым преимуществом использования этих точек расширения является то, что для создания кадра данных вам не требуется извлекать все данные. Ваш пользовательский код Python будет работать так же, как и другие преобразования, в масштабе, по разделам и, как правило, параллельно.

### <a name="initial-data-preparation"></a>Начальная подготовка данных

Начните с загрузки некоторых данных из хранилища Blob-объектов Azure.

```
import azureml.dataprep as dprep
col = dprep.col

df = dprep.read_csv(path='https://dpreptestfiles.blob.core.windows.net/testfiles/read_csv_duplicate_headers.csv', skip_rows=1)
df.head(5)
```
| |stnam|fipst|leaid|leanm10|ncessch|MAM_MTH00numvalid_1011|
|-----|-------|---------| -------|------|-----|------|-----|
|0|АЛАБАМА|1|101710|Округ Хейл|10171002158| |
|1|АЛАБАМА|1|101710|Округ Хейл|10171002162| |
|2|АЛАБАМА|1|101710|Округ Хейл|10171002156| |
|3|АЛАБАМА|1|101710|Округ Хейл|10171000588|2|
|4.|АЛАБАМА|1|101710|Округ Хейл|10171000589| |

Сократите набор данных и выполните базовые преобразования.

```
df = df.keep_columns(['stnam', 'leanm10', 'ncessch', 'MAM_MTH00numvalid_1011'])
df = df.replace_na(columns=['leanm10', 'MAM_MTH00numvalid_1011'], custom_na_list='.')
df = df.to_number(['ncessch', 'MAM_MTH00numvalid_1011'])
df.head(5)
```
| |stnam|leanm10|ncessch|MAM_MTH00numvalid_1011|
|-----|-------|---------| -------|------|-----|
|0|АЛАБАМА|Округ Хейл|1,017100e+10|None|
|1|АЛАБАМА|Округ Хейл|1,017100e+10|None|
|2|АЛАБАМА|Округ Хейл|1,017100e+10|None|
|3|АЛАБАМА|Округ Хейл|1,017100e+10|2|
|4.|АЛАБАМА|Округ Хейл|1,017100e+10|None|

С помощью фильтра найдите значения NULL. Если таковы имеются, то вам нужно их заполнить.

```
df.filter(col('MAM_MTH00numvalid_1011').is_null()).head(5)
```

| |stnam|leanm10|ncessch|MAM_MTH00numvalid_1011|
|-----|-------|---------| -------|------|-----|
|0|АЛАБАМА|Округ Хейл|1,017100e+10|None|
|1|АЛАБАМА|Округ Хейл|1,017100e+10|None|
|2|АЛАБАМА|Округ Хейл|1,017100e+10|None|
|3|АЛАБАМА|Округ Хейл|1,017100e+10|None|
|4.|АЛАБАМА|Округ Хейл|1,017100e+10|None|

### <a name="transform-partition"></a>Преобразование секции

Для замены всех значений NULL на 0 вы можете использовать удобную функцию Pandas. Этот код будет выполняться по секциям, но не во всем наборе одновременно. Это значит, что для большого набора данных этот код можно выполнять в параллельном режиме по мере выполнения обработки данных, то есть секция за секцией.

```
df = df.transform_partition("""
def transform(df, index):
    df['MAM_MTH00numvalid_1011'].fillna(0,inplace=True)
    return df
""")
h = df.head(5)
h
```
||stnam|leanm10|ncessch|MAM_MTH00numvalid_1011|
|-----|-------|---------| -------|------|-----|
|0|АЛАБАМА|Округ Хейл|1,017100e+10|0,0|
|1|АЛАБАМА|Округ Хейл|1,017100e+10|0,0|
|2|АЛАБАМА|Округ Хейл|1,017100e+10|0,0|
|3|АЛАБАМА|Округ Хейл|1,017100e+10|2,0|
|4.|АЛАБАМА|Округ Хейл|1,017100e+10|0,0|

### <a name="new-script-column"></a>Столбец нового скрипта

Вы можете использовать код Python для создания нового столбца с именами округа и штата, а также для капитализации имени штата. Чтобы сделать это, используйте в потоке данных метод `new_script_column()`.

```
df = df.new_script_column(new_column_name='county_state', insert_after='leanm10', script="""
def newvalue(row):
    return row['leanm10'] + ', ' + row['stnam'].title()
""")
h = df.head(5)
h
```
||stnam|leanm10|county_state|ncessch|MAM_MTH00numvalid_1011|
|-----|-------|---------| -------|------|-----|
|0|АЛАБАМА|Округ Хейл|Округ Хейл, штат Алабама|1,017100e+10|0,0|
|1|АЛАБАМА|Округ Хейл|Округ Хейл, штат Алабама|1,017100e+10|0,0|
|2|АЛАБАМА|Округ Хейл|Округ Хейл, штат Алабама|1,017100e+10|0,0|
|3|АЛАБАМА|Округ Хейл|Округ Хейл, штат Алабама|1,017100e+10|2,0|
|4.|АЛАБАМА|Округ Хейл|Округ Хейл, штат Алабама|1,017100e+10|0,0|
### <a name="new-script-filter"></a>Фильтр нового скрипта

Теперь создайте выражение Python для фильтрации набора данных только в тех строках, где слово "Хейл" не указано в новом столбце `county_state`. Выражение возвращает `True`, если строку можно сохранить, и `False`, если ее нужно удалить.

```
df = df.new_script_filter("""
def includerow(row):
    val = row['county_state']
    return 'Hale' not in val
""")
h = df.head(5)
h
```

||stnam|leanm10|county_state|ncessch|MAM_MTH00numvalid_1011|
|-----|-------|---------| -------|------|-----|
|0|АЛАБАМА|Округ Джефферсон|Округ Джефферсон, штат Алабама|1.019200e+10|1.0|
|1|АЛАБАМА|Округ Джефферсон|Округ Джефферсон, штат Алабама|1.019200e+10|0,0|
|2|АЛАБАМА|Округ Джефферсон|Округ Джефферсон, штат Алабама|1.019200e+10|0,0|
|3|АЛАБАМА|Округ Джефферсон|Округ Джефферсон, штат Алабама|1.019200e+10|0,0|
|4.|АЛАБАМА|Округ Джефферсон|Округ Джефферсон, штат Алабама|1.019200e+10|0,0|
