<properties
	pageTitle="Обзор обработки и анализа данных с помощью платформы Spark в Azure HDInsight | Microsoft Azure"
	description="Набор средств Spark MLlib предоставляет широкие возможности моделирования машинного обучения в распределенной среде HDInsight."
	services="machine-learning"
	documentationCenter=""
	authors="bradsev"
	manager="paulettm"
	editor="cgronlun"  />

<tags
	ms.service="machine-learning"
	ms.workload="data-services"
	ms.tgt_pltfrm="na"
	ms.devlang="na"
	ms.topic="article"
	ms.date="04/18/2016"
	ms.author="bradsev" />

# Общие сведения об обработке и анализе данных с помощью платформы Spark в Azure HDInsight

[AZURE.INCLUDE [machine-learning-spark-modeling](../../includes/machine-learning-spark-modeling.md)]

## Введение

Платформа Spark в Azure HDInsight может управлять данными в памяти во время их распределенной обработки в кластере HDInsight (Hadoop). Таким образом, этот кластер Spark сочетает скорость и емкость. Он также поддерживает записные книжки Jupyter в кластере Spark, которые могут выполнять интерактивные запросы Spark SQL для преобразования, фильтрации и визуализации данных, хранящихся в больших двоичных объектах Azure (WASB).

[Spark](http://spark.apache.org/) — это платформа параллельной обработки с открытым исходным кодом, которая поддерживает обработку в памяти, повышая производительность приложений для анализа данных большого объема. Подсистема обработки Spark призвана ускорить разработку, повысить удобство использования и реализовать сложную аналитику. Возможности вычисления в памяти Spark отлично подходят для итеративных алгоритмов в машинном обучении и графовых вычислениях. [MLlib](http://spark.apache.org/mllib/) — это масштабируемая библиотека машинного обучения Spark.

HDInsight Spark представляет собой версию платформы Spark с открытым исходным кодом, размещенную в Azure. Действия по настройке и код, указанные в этом пошаговом руководстве, предназначены для HDInsight Spark. Однако код является универсальным и должен работать в любом кластере Spark. Действия по настройке кластера и управлению им могут немного отличаться от приведенных здесь, если вы не используете HDInsight Spark.

Набор средств Spark MLlib предоставляет для этой распределенной среды широкие возможности моделирования для машинного обучения (ML). В наборе тем, ссылки на которые содержатся в меню, это демонстрируется путем выполнения задач двоичной классификации и регрессии на примере набора данных о поездке на такси в Нью-Йорке и стоимости проезда в 2013 году с последующим сохранением результатов модели в WASB. Построенные модели включают логистическую и линейную регрессию, случайные леса и деревья с градиентным повышением. Они также показывают, как использовать результаты этих моделей для их оценки и анализа относительно других наборов данных, хранящихся в WASB. В более расширенных разделах рассматриваются способы обучения моделей с помощью перекрестной проверки и корректировки гиперпараметров.

Этапы моделирования, описанные в этих разделах, содержат код, который демонстрирует способ обучения, анализа, сохранения и использования каждого типа модели. Для кодировки решения и отображения соответствующих графиков использовался язык Python. Также были предусмотрены записные книжки Python, которые можно запускать в записных книжках Jupyter, установленных в кластерах Spark.


## Предварительные требования

1\. Для изучения этих разделов необходимо иметь подписку Azure. Если у вас ее нет, см. раздел [о получении бесплатной пробной версии Azure](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/).

2\. Чтобы создать кластер HDInsight Spark, см. статью [Начало работы. Создание кластера Apache Spark в Azure HDInsight и выполнение интерактивных запросов с помощью SQL Spark](../hdinsight/hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql.md).


[AZURE.INCLUDE [delete-cluster-warning](../../includes/hdinsight-delete-cluster-warning.md)]


## Данные о такси в Нью-Йорке, 2013 г.

Сведения о поездках на такси в Нью-Йорке заключены в сжатые файлы данных с разделителями-запятыми (CSV) объемом около 20 ГБ (~48 ГБ без сжатия), которые содержат более 173 миллионов записей о поездках и о стоимости каждой из них. Каждая запись о поездке содержит расположение пунктов отправления и назначения и время, анонимизированный номер лицензии водителя и номер медальона (уникальный идентификатор такси). Данные включают в себя все поездки за 2013 год и предоставляются в виде следующих двух наборов данных за каждый месяц:

1. В CSV-файлах trip\_data содержатся сведения о поездках, например количество пассажиров, места посадки и высадки, продолжительность и дальность поездок. Вот несколько примеров записей:

		medallion,hack_license,vendor_id,rate_code,store_and_fwd_flag,pickup_datetime,dropoff_datetime,passenger_count,trip_time_in_secs,trip_distance,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude
		89D227B655E5C82AECF13C3F540D4CF4,BA96DE419E711691B9445D6A6307C170,CMT,1,N,2013-01-01 15:11:48,2013-01-01 15:18:10,4,382,1.00,-73.978165,40.757977,-73.989838,40.751171
		0BD7C8F5BA12B88E0B67BED28BEA73D8,9FD8F69F0804BDB5549F40E9DA1BE472,CMT,1,N,2013-01-06 00:18:35,2013-01-06 00:22:54,1,259,1.50,-74.006683,40.731781,-73.994499,40.75066
		0BD7C8F5BA12B88E0B67BED28BEA73D8,9FD8F69F0804BDB5549F40E9DA1BE472,CMT,1,N,2013-01-05 18:49:41,2013-01-05 18:54:23,1,282,1.10,-74.004707,40.73777,-74.009834,40.726002
		DFD2202EE08F7A8DC9A57B02ACB81FE2,51EE87E3205C985EF8431D850C786310,CMT,1,N,2013-01-07 23:54:15,2013-01-07 23:58:20,2,244,.70,-73.974602,40.759945,-73.984734,40.759388
		DFD2202EE08F7A8DC9A57B02ACB81FE2,51EE87E3205C985EF8431D850C786310,CMT,1,N,2013-01-07 23:25:03,2013-01-07 23:34:24,1,560,2.10,-73.97625,40.748528,-74.002586,40.747868

2. В CSV-файлах trip\_fare содержатся сведения о плате за каждую поездку, например тип оплаты, стоимость поездки, добавочная стоимость и налоги, чаевые и специальные тарифы, а также общая сумма оплаты. Вот несколько примеров записей:

		medallion, hack_license, vendor_id, pickup_datetime, payment_type, fare_amount, surcharge, mta_tax, tip_amount, tolls_amount, total_amount
		89D227B655E5C82AECF13C3F540D4CF4,BA96DE419E711691B9445D6A6307C170,CMT,2013-01-01 15:11:48,CSH,6.5,0,0.5,0,0,7
		0BD7C8F5BA12B88E0B67BED28BEA73D8,9FD8F69F0804BDB5549F40E9DA1BE472,CMT,2013-01-06 00:18:35,CSH,6,0.5,0.5,0,0,7
		0BD7C8F5BA12B88E0B67BED28BEA73D8,9FD8F69F0804BDB5549F40E9DA1BE472,CMT,2013-01-05 18:49:41,CSH,5.5,1,0.5,0,0,7
		DFD2202EE08F7A8DC9A57B02ACB81FE2,51EE87E3205C985EF8431D850C786310,CMT,2013-01-07 23:54:15,CSH,5,0.5,0.5,0,0,6
		DFD2202EE08F7A8DC9A57B02ACB81FE2,51EE87E3205C985EF8431D850C786310,CMT,2013-01-07 23:25:03,CSH,9.5,0.5,0.5,0,0,10.5


Уникальный ключ для соединения trip\_data и trip\_fare состоит из полей: medallion, hack\_licence и pickup\_datetime.


Мы взяли образец этих файлов размером 0,1 % и объединили их в один набор данных, который будет использоваться в качестве входного набора данных для этого пошагового руководства. Уникальный ключ для соединения trip\_data и trip\_fare состоит из полей: medallion, hack\_licence и pickup\_datetime. Каждая запись набора данных содержит следующие атрибуты, представляющие поездку на такси в Нью-Йорке:


|Поле| Краткое описание
|------|---------------------------------
| medallion |Обезличенный медальон такси (уникальный идентификатор такси)
| hack\_license |	Обезличенный номер лицензии такси
| vendor\_id |	Идентификатор поставщика услуг такси
| rate\_code | Тариф на такси в Нью-Йорке
| store\_and\_fwd\_flag | Отметка записи и дальнейшей передачи
| pickup\_datetime |	Дата и время посадки пассажира
| dropoff\_datetime | Дата и время высадки пассажира
| pickup\_hour |	Час посадки пассажира
| pickup\_week |	Неделя посадки пассажира
| weekday |	День недели (диапазон 1–7)
| passenger\_count |	Количество пассажиров во время поездки на такси
| trip\_time\_in\_secs | Длительность поездки в секундах
| trip\_distance | Расстояние поездки в милях
| pickup\_longitude | Долгота посадки пассажира
| pickup\_latitude |	Широта посадки пассажира
| dropoff\_longitude | Долгота высадки пассажира
| dropoff\_latitude | Широта высадки пассажира
| direct\_distance |	Расстояние по прямой между местом посадки и местом высадки
| payment\_type | Тип оплаты (наличные, кредитная карта и т. д.)
| fare\_amount | Сумма к оплате
| surcharge | Доплата
| mta\_tax |	Налог MTA
| tip\_amount | Сумма чаевых
| tolls\_amount | Дорожные пошлины
| total\_amount | Общая сумма
| tipped | Чаевые (0 — нет, 1 — да)
| tip\_class | Класс чаевых (0: 0 долларов, 1: 0–5 долларов, 2: 6–10 долларов, 3: 11–20 долларов, 4: >20 долларов)


## Выполнение кода из записной книжки Jupyter в кластере Spark 

Записную книжку Jupyter можно запустить с портала Azure. Для этого найдите кластер Spark и щелкните его для входа на страницу подробных сведений об управлении кластером. На странице щелкните элемент **Панели мониторинга кластера** со значком для записной книжки Jupyter, связанной с кластером Spark.

![](./media/machine-learning-data-science-spark-overview/spark-jupyter-on-portal.png)

Для доступа к записной книжке Jupyter также можно перейти по ссылке ***https://CLUSTERNAME.azurehdinsight.net/jupyter***. Для доступа к записным книжкам необходим пароль к учетной записи администратора.

![](./media/machine-learning-data-science-spark-overview/spark-jupyter-notebook.png)

Откройте раздел Python для просмотра имеющихся записных книжек, работающих на скриптах Python. Отобразится каталог, содержащий несколько примеров предварительно упакованных записных книжек. Записные книжки, содержащие примеры кода в этом разделе, доступны на сайте [Github](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/Spark/Python).

Записные книжки можно отправить непосредственно из Github на сервер записных книжек Jupyter в кластере Spark. На домашней странице записной книжки Jupyter в правой части экрана нажмите кнопку **Отправить**. Откроется обозреватель файлов. Здесь вставьте URL-адрес Github (необработанное содержимое) для записной книжки и нажмите кнопку **Открыть**. В списке файлов Jupyter отобразится имя файла с кнопкой **Отправить**. Нажмите кнопку **Отправить**. Записная книжка импортирована. Повторите эти действия для отправки следующих записных книжек из этого пошагового руководства.

1.	machine-learning-data-science-spark-data-exploration-modeling.ipynb
2.	machine-learning-data-science-spark-model-consumption.ipynb
3.	machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb

Теперь вы можете:

- щелкнуть записную книжку, чтобы просмотреть код;
- выполнить каждую ячейку нажатием сочетания клавиш **SHIFT + ВВОД**;
- выполнить всю записную книжку, последовательно щелкнув элементы **Ячейка** -> **Выполнить**;


## Что дальше?

Теперь, когда выполнена настройка кластера HDInsight Spark и отправлены записные книжки Jupyter, все готово для работы с разделам, соответствующими этим трем записным книжкам. В них показано, как создавать, использовать и моделировать с помощью перекрестной проверки и очистки гиперпараметров.

**Создание модели**. Создайте модели машинного обучения для оценки и анализа, выполнив инструкции из статьи, посвященной [созданию моделей двоичной классификации и регрессии для данных с помощью набора средств Spark MLlib](machine-learning-data-science-spark-data-exploration-modeling.md).

**Использование модели**. Дополнительные сведения об оценке и анализе моделей классификации и регрессии, созданных в этой статье, см. в разделе [Score Spark-built machine learning models](machine-learning-data-science-spark-model-consumption.md) (Оценка и анализ моделей машинного обучения, созданных с помощью Spark).

<!---HONumber=AcomDC_0420_2016-->