<properties
   pageTitle="Максимальное увеличение производительности приема данных с помощью Elasticsearch в Azure | Microsoft Azure"
   description="Руководство по максимальному увеличению производительности приема данных с помощью Elasticsearch в Azure."
   services=""
   documentationCenter="na"
   authors="mabsimms"
   manager="marksou"
   editor=""
   tags=""/>

<tags
   ms.service="guidance"
   ms.devlang="na"
   ms.topic="article"
   ms.tgt_pltfrm="na"
   ms.workload="na"
   ms.date="02/05/2016"
   ms.author="mabsimms"/>

# Максимальное увеличение производительности приема данных с помощью Elasticsearch в Azure

Этот материал входит в [цикл статей, посвященных вопросам Elasticsearch](guidance-elasticsearch-introduction.md).

## Обзор

При создании любой базы данных поиска важно определить оптимальную структуру системы, которая позволит быстро и эффективно принимать данные для поиска. При этом важно выбрать не только оптимальную инфраструктуру, на базе которой будет реализована система, но и различные средства оптимизации, благодаря которым система сможет справиться с ожидаемым притоком данных. В этом документе описаны параметры развертывания и конфигурации, необходимые для реализации кластера Elasticsearch с высокой скоростью приема данных. Для иллюстративных целей здесь также приведены результаты тестирования производительности при рабочей нагрузке приема больших объемов данных в различных конфигурациях. Дополнительные сведения об этой рабочей нагрузке описаны в [приложении](#appendix-the-bulk-load-data-ingestion-performance-test) в конце статьи.

Мы провели эти тесты не для того, чтобы определить абсолютные показатели производительности для работы Elasticsearch или предложить определенную топологию. Наша цель — продемонстрировать методы оценки производительности, изменения размера узлов данных и реализации кластеров в соответствии с собственными требованиями к производительности. При выборе размера систем важно тщательно протестировать производительность с учетом собственных нагрузок. Соберите данные телеметрии, чтобы узнать оптимальную конфигурацию оборудования и факторы горизонтального масштабирования, которые следует учесть. В частности вам необходимо:

* Определить общий объем отправляемых полезных данных, а не только количество элементов в каждом запросе массовой вставки. В зависимости от ресурсов, доступных для обработки запроса, наличие меньшего числа элементов массовой операции в каждом запросе может быть более оптимально.

  > [AZURE.NOTE] Отслеживать влияние различных запросов массовой вставки можно с помощью Marvel, счетчиков операций ввода-вывода JMeter *readbytes** и writebytes* и средств операционной системы, таких как *iostat* и *vmstat* на Ubuntu.

* Выполнить тестирование производительности и собрать данные телеметрии, чтобы определить время ожидания обработки ЦП и операций ввода-вывода, задержку диска, пропускную способность и время отклика. Эта информация поможет вам определить потенциальные узкие места и оценить затраты и преимущества использования хранилища класса Premium. Следует помнить, что загрузка ЦП и диска может быть неодинаковой на всех узлах в зависимости от способа распределения сегментов и реплик в кластере (некоторые узлы могут содержать больше сегментов, чем другие).

* Подумать о том, как одновременные запросы в рамках рабочей нагрузки будут распределяться в кластере, и оценить влияние использования разного количества узлов для обработки этой нагрузки.

* Подумать о том, как могут увеличиться рабочие нагрузки с расширением бизнеса. Оцените влияние этого роста на затраты, связанные с виртуальными машинами и хранилищами, которые используют узлы.

* Осознать, что использование обычных дисков для кластера с большим количеством узлов может быть более экономно при условии, что сценарий требует большого числа запросов и инфраструктура диска поддерживает пропускную способность, предусмотренную соглашением об уровне обслуживания. Однако при увеличении числа узлов может увеличиться нагрузка: возникнут дополнительные связи и синхронизации между узлами.

* Понимать, что дисковый трафик может увеличиться при наличии большего количества ядер на узел, так как это позволяет обработать больше документов. В этом случае необходимо измерить показатели использования диска, чтобы оценить, сможет ли подсистема ввода-вывода стать узким местом, и определить преимущества использования хранилища класса Premium.

* Протестировать производительность при использовании большего количества узлов с меньшим числом ядер и меньшего количества узлов с большим числом ядер, а также проанализировать компромиссы в обоих случаях. Следует помнить, что увеличение числа реплик повышает нагрузку на кластер, из-за чего могут понадобиться дополнительные узлы.

* Учитывать, что при использовании временных дисков необходимая частота восстановления индексов может быть выше.

* Оценить используемую емкость и недостаточное использование ресурсов хранилища на основе оценки используемого объема хранилища. Например, в нашем сценарии для хранения 1,5 млрд документов используется 350 ГБ дискового пространства.

* Измерить скорость передачи для рабочих нагрузок и оценить вероятность достижения общего лимита передачи операций ввода-вывода для любой заданной учетной записи хранения, в которой созданы виртуальные диски.

Далее приводится более подробное описание этих рекомендаций.

## Рекомендации по структуре индексов и узлам

В системе, которая должна поддерживать прием данных большего объема, необходимо учитывать следующие аспекты:

* **Данные быстро меняющиеся или относительно статичные?** Чем более динамические данные, тем выше издержки на обслуживание для Elasticsearch. Если данные реплицируются, каждая реплика обслуживается синхронно. Для быстро меняющихся данных, которые имеют ограниченный срок существования или которые можно легко воссоздать, иногда лучше полностью отключить репликацию. Этот вариант подробно описан в разделе [Рекомендации по настройке приема данных большего объема](#_Considerations_for_Tuning).

* **Насколько актуальными должны быть данные, обнаруживаемые при поиске?** Для поддержания уровня производительности Elasticsearch помещает в буфер максимально возможный объем данных. Это означает, что не все изменения становятся сразу доступны для поисковых запросов. Процесс сохранения и отображения изменений в Elasticsearch описан в статье [Making Changes Persistent](https://www.elastic.co/guide/en/elasticsearch/guide/current/translog.html#translog) (Сохранения изменений). Скорость, с которой данные становятся доступными, зависит от значения параметра *refresh\_interval* соответствующего индекса. По умолчанию этот интервал составляет 1 секунду. Однако не всегда требуется, чтобы обновления происходили так быстро. Например, для индексов записи данных журналов может потребоваться обработка быстрого и постоянного притока данных, которые нужно быстро принимать, однако при этом не нужно, чтобы они сразу же были доступны для запросов. В этом случае следует уменьшить частоту обновления. Эта функция подробно описана в разделе [Рекомендации по настройке приема данных большего объема](#_Considerations_for_Tuning).

* **Какая ожидаемая скорость увеличения объема данных?** Емкость индекса зависит от количества сегментов, указанных при его создании. Чтобы обеспечить возможность роста, укажите достаточное количество сегментов (по умолчанию — 5). Если индекс изначально создан на одном узле, на этом узле будут размещены все пять сегментов. Но по мере увеличения объема данных можно добавлять дополнительные узлы, и Elasticsearch будет динамически распределять между ними сегменты. Однако каждый сегмент имеет отдельную нагрузку. При поиске в индексе будут запрашиваться все сегменты, поэтому создание большого числа сегментов для небольшого объема данных может замедлить получение данных (см. сведения о сценарии с [сегментами Kagillion](https://www.elastic.co/guide/en/elasticsearch/guide/current/kagillion-shards.html), которого следует избегать).

    При некоторых рабочих нагрузках (например, ведении журнала) необходимо создавать по индексу каждый день, и если вы заметили, что существующего количества сегментов недостаточно для объема данных, следует изменить их число перед созданием следующего индекса (это никак не повлияет на существующие индексы). Если необходимо распределить существующие данные между большим количеством сегментов, можно выполнить повторное индексирование информации, создать новый индекс с соответствующей конфигурацией и скопировать в него данные. Чтобы этот процесс поддерживали приложения, следует использовать [псевдонимы индексов](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-aliases.html).

* **Нужно ли секционировать данные между пользователями в мультитенантном сценарии?** Для каждого пользователя можно создавать отдельные индексы, но это может быть затратно, если у каждого из них средний объем данных. Вместо этого можно создать [общие индексы](https://www.elastic.co/guide/en/elasticsearch/guide/current/shared-index.html) и направлять запросы к данным пользователя, используя [псевдонимы на основе фильтров](https://www.elastic.co/guide/en/elasticsearch/guide/current/faking-it.html). Чтобы хранить данные пользователя в одном сегменте, переопределите конфигурацию маршрутизации по умолчанию для индекса и направляйте данные на основе определяющего атрибута пользователя.

* **Данные временные или постоянные?** Если для реализации кластера Elasticsearch используется набор виртуальных машин Azure, вместо подключенного диска для хранения временных данных можно использовать системный диск локальных ресурсов. Использование для виртуальной машины номера SKU, который предусматривает использование в качестве диска ресурса диска SSD, может повысить производительность ввода-вывода. Однако любая информация, которая хранится на диске ресурсов, временная и может быть потеряна при перезапуске виртуальной машины. (Дополнительные сведения см. в разделе о потере данных на временном диске в статье [Understanding the temporary drive on Microsoft Azure Virtual Machines](http://blogs.msdn.com/b/mast/archive/2013/12/07/understanding-the-temporary-drive-on-windows-azure-virtual-machines.aspx) (Основные сведения о временном диске на виртуальных машинах Microsoft Azure).) Если требуется предотвратить потерю данных при перезапусках, создайте для их хранения постоянные виртуальные жесткие диски и присоедините их к виртуальной машине.

* **Насколько активны данные?** Если число операций чтения и записи превышает заданные значения параметров (в настоящее время 500 операций ввода-вывода для диска, подключенного к виртуальной машине уровня Standard, и 5000 операций ввода-вывода для диска хранилища класса Premium), виртуальные жесткие диски Azure подвергаются регулированию. Чтобы снизить вероятность регулирования и повысить производительность ввода-вывода, можно создать несколько виртуальных жестких дисков данных для каждой виртуальной машины и настроить чередование данных между этими дисками для Elasticsearch, как описано в разделе о [требованиях к файловой системе и диску](#disk-and-file-system-requirements) в документе о реализации Elasticsearch в Azure.

    > [AZURE.NOTE] Следует выбрать такую конфигурацию оборудования, которая позволяет свести к минимуму число операций чтения дискового ввода-вывода, гарантируя доступность достаточного объема памяти для кэширования часто запрашиваемых данных. Это описано в разделе о [требованиях к памяти](#memory-requirements) документа о реализации Elasticsearch в Azure.

* **Какой тип рабочей нагрузки должен поддерживать каждый узел?** Наличие доступной памяти для кэширования данных (в форме кэша файловой системы) и для кучи виртуальной машины Java (как описано в разделе о [требованиях к памяти](#memory-requirements) документа о реализации Elasticsearch в Azure) предоставляет определенные преимущества в Elasticsearch. Кроме того, благодаря потоковой модели Elasticsearch более эффективно использовать многоядерные процессоры, а не мощные процессоры с меньшим числом ядер.

    > [AZURE.NOTE] Объем памяти, число ядер ЦП и количество доступных дисков ограничиваются SKU виртуальной машины. Дополнительные сведения см. на странице [Цены на виртуальные машины](http://azure.microsoft.com/pricing/details/virtual-machines/) на веб-сайте Azure.

### Характеристики виртуальной машины

Для подготовки виртуальных машин в Azure можно использовать различные номера SKU. Ресурсы, доступные для виртуальной машины Azure, зависят от выбранного SKU. Каждый номер SKU предусматривает различное количество ядер, объема памяти и ресурсов хранения. Для виртуальной машины необходимо выбрать такой размер, чтобы она не только могла обработать ожидаемую рабочую нагрузку, но и была экономичной. Начните с конфигурации, которая будет соответствовать вашим текущим требованиям (выполните тест производительности, как описано далее в этой статье). Кластер можно масштабировать позже, добавив дополнительные виртуальные машины, на которых запущены узлы Elasticsearch.

На странице [Размеры виртуальных машин](virtual-machines-size-specs/) на веб-сайте Azure приведены различные варианты и SKU, доступные для виртуальных машин.

Размер и ресурсы виртуальной машины должны соответствовать роли узлов, запущенных на виртуальной машине.

Рекомендации для узла данных

* Выделите до 30 ГБ или 50 % доступной памяти ОЗУ для кучи Java (в зависимости от того, какое значение ниже). Оставшуюся память оставьте операционной системе для кэширования файлов. Если вы используете ОС Linux, объем памяти, который нужно выделить для кучи Java, можно указать задав переменную среды ES\_HEAP\_SIZE перед запуском Elasticsearch. Кроме того, если вы используете Windows или Linux, можно указать объем памяти с помощью параметров *Xmx* и *Xms* при запуске Elasticsearch.

    > [AZURE.NOTE]  В зависимости от рабочей нагрузки использование большего количества виртуальных машин среднего размера может оказаться более эффективным для производительности, чем использование меньшего числа больших виртуальных машин. Необходимо выполнить тесты, чтобы оценить компромисс между дополнительным сетевым трафиком, издержками обслуживания и затратами на увеличение количества доступных ядер, а также сокращением конфликтов дисков на каждом узле.

* Используйте для хранения данных Elasticsearch высокоскоростные диски, в идеале диски SSD с низкой задержкой. Это более подробно рассматривается в разделе [Варианты хранилищ](#storage-options).

* Используйте несколько дисков (одинакового размера) и чередуйте между ними данные. Максимальное количество дисков данных, которые можно присоединить, зависит от SKU виртуальных машин. Дополнительные сведения см. в разделе о [требованиях к файловой системе и диску](#disk-and-file-system-requirements).

* Используйте ЦП с несколькими ядрами (по крайней мере 2 ядра, предпочтительно 4 и более). Выбирайте процессоры, опираясь на количество ядер, а не на мощность ЦП. Потоковая модель, которую Elasticsearch использует для обработки одновременных запросов, более эффективна при использовании ЦП с большим количеством ядер, а не мощных ЦП с небольшим числом ядер.

Рекомендации для клиентского узла

* Не выделяйте дисковое пространство для данных Elasticsearch. Выделенные клиенты не сохраняют данные на диске.

* Убедитесь в наличии достаточного объема памяти для обработки рабочих нагрузок. Запросы на массовую вставку считываются в памяти до отправки данных в различные узлы данных, а результаты агрегирования и запросов накапливаются в памяти перед возвращением клиентскому приложению. Протестируйте производительность собственных рабочих нагрузок и оцените оптимальные требования за счет мониторинга использования памяти с помощью Marvel или [сведений JVM](https://www.elastic.co/guide/en/elasticsearch/guide/current/_monitoring_individual_nodes.html#_jvm_section), возвращаемых при выполнении API *node/stats*:

    ```http
    GET _nodes/stats
    ```

    Отслеживайте метрику *heap\_used\_percent* для каждого узла. Размер кучи должен составлять менее 75 % доступного места.

* Обеспечьте доступность достаточного количества ядер ЦП для получения и обработки ожидаемого объема запросов. Перед обработкой получаемые запросы помещаются в очередь, а количество элементов, которые могут быть поставлены в очередь, зависит от числа ядер ЦП на каждом узле. Отслеживать длину очереди можно на основе [сведений Threadpool](https://www.elastic.co/guide/en/elasticsearch/guide/current/_monitoring_individual_nodes.html#_threadpool_section), которые возвращаются при выполнении API node/stats. Если значение параметра *rejected* для очереди указывает на то, что запросы отклоняются, это означает, что производительность кластера снижается. Причиной может быть пропускная способность ЦП, а также другие факторы, например нехватка памяти или низкая производительность операций ввода-вывода. Поэтому для определения основной причины следует учитывать не только эту информацию, но и другие статистические данные.

    > [AZURE.NOTE]  Клиентские узлы могут не потребоваться в зависимости от рабочих нагрузок. Рабочие нагрузки приема данных, как правило, не выигрывают от использования выделенных клиентов. Однако при их использовании некоторые поисковые операции и операции агрегирования могут выполняться быстрее. Будьте готовы к тестированию производительности в рамках собственных сценариев.

    > Клиентские узлы особенно эффективны для приложений, использующих API транспортного клиента для подключения к кластеру. Можно также использовать API клиента узла, который динамически создает выделенный клиент для приложения, используя ресурсы среды размещения приложения. Если ваше приложение использует API клиента узла, кластеру необязательно содержать предварительно настроенные выделенные клиентские узлы. Однако имейте в виду, что узел, созданный с помощью API узла клиента, — это один из основных компонентов кластера. Следовательно, он участвует в сети Chatter вместе с другими узлами. Зачастую запуск и остановка работы клиентских узлов могут привести к ошибочным показателям по всему кластеру.

Рекомендации для главного узла

* Не выделяйте дисковое пространство для данных Elasticsearch. Выделенные главные узлы не сохраняют данные на диске.

* Требования к ЦП должны быть минимальными.

* Требования к памяти зависят от размера кластера. Сведения о состоянии кластера хранятся в памяти. Для небольших кластеров требуемый объем памяти минимальный, но для больших и очень активных кластеров, где часто создаются индексы и перемещаются сегменты, объем сведений о состоянии может существенно расти. Выполните мониторинг размера кучи виртуальной машины Java, чтобы определить, необходимо ли увеличить объем памяти.

    > [AZURE.NOTE]  Для обеспечения надежности кластера всегда создавайте несколько главных узлов и настраивайте остальные узлы во избежание возможного разделения вычислительных мощностей. В идеале число главных узлов должно быть нечетным. Это более подробно описано в документе Configuring, Testing, and Analyzing Elasticsearch Resilience and Recovery (Настройка, тестирование и анализ устойчивости и восстановления Elasticsearch).

### Варианты хранилищ

Виртуальные машины Azure предоставляют несколько вариантов хранения. Каждый из них отличается различными компромиссами, которые влияют на стоимость, производительность, доступность и восстановление. Их нужно тщательно рассмотреть.

Обратите внимание, что данные Elasticsearch следует хранить на тех дисках, которые не используются операционной системой. Это позволяет уменьшить количество конфликтов с операционной системой и избежать "конкуренции" между ресурсоемкими операциями ввода-вывода Elasticsearch и функциями ОС за ресурсы ввода-вывода.

На диски Azure распространяются ограничения производительности. Если в кластере наблюдаются периодические всплески активности, возможно, выполняется регулирование запросов ввода-вывода. Чтобы предотвратить это, сбалансируйте размер документа в Elasticsearch с учетом объема запросов, который может получать каждый диск.

Максимальная поддерживаемая частота запросов для дисков на основе хранилища класса Standard составляет 500 операций ввода-вывода, а для дисков на основе хранилища класса Premium — до 5000 операций ввода-вывода (хранилище класса Standard использует вращающийся носитель, а хранилище класса Premium — твердотельные накопители, которые имеют меньшую задержку и более высокую пропускную способность). Диски хранилища класса Premium доступны только для виртуальных машин серии DS и GS. Максимальную частоту операций ввода-вывода для дисков виртуальных машин Azure [можно просмотреть здесь](virtual-machines-size-specs/).

> [AZURE.NOTE] В зависимости от объема данных, возвращаемых запросами, может не удаться достичь максимального числа операций ввода-вывода для диска, так как каждая виртуальная машина регулируется в соответствии с максимальной пропускной способностью, которая зависит от ее размера. Например, диск данных в виртуальной машине Standard\_GS5 может обработать до 5000 операций ввода-вывода, но только если общая пропускная способность при передаче данных не превышает 2000 МБ/с на всех дисках, подключенных к виртуальной машине.

**Постоянные диски данных**

Постоянные диски данных — это виртуальные жесткие диски, которые поддерживает служба хранилища Azure. Если виртуальную машину необходимо повторно создать после аварийного отказа, существующие виртуальные жесткие диски можно с легкостью присоединить к новой виртуальной машине. Виртуальный жесткий диск можно создать на основе хранилища класса Standard (вращающиеся носители) и на основе хранилища класса Premium (SSD). Если требуется использовать SSD, необходимо создать виртуальные машины серии DS или лучшей. Виртуальные машины серии DS и эквивалентные им виртуальные машины серии D стоят одинаково, но за использование хранилища класса Premium взимается дополнительная плата.

Если максимальная скорость передачи на одном диске недостаточная для поддержки ожидаемой рабочей нагрузки, можно создать нескольких дисков данных и разрешить Elasticsearch [чередовать данные между этими дисками](#disk-and-file-system-requirements) либо реализовать [чередование RAID 0 с использованием виртуальных дисков](virtual-machines-linux-configure-raid/) на уровне системы.

> [AZURE.NOTE] Работа с продуктами Microsoft показала, что использование RAID 0 позволяет существенно сгладить влияние на операции ввода-вывода *неравномерных* рабочих нагрузок, которые вызывают частые всплески активности.

Для учетной записи хранения, содержащей диски, следует использовать локально избыточные реплики (или локально избыточное реплики класса Premium). Репликация между регионами и областями необязательна для Elasticsearch высокой доступности.

**Временные диски**

При использовании постоянных дисков на основе SSD необходимо создать виртуальные машины, которые поддерживают хранилище класса Premium. Это влечет за собой материальные затраты. Более экономичным решением по хранению данных Elasticsearch для узлов среднего размера, требующих хранилища размером около 800 ГБ, являются локальные временные диски. На виртуальных машинах серии D уровня Standard временные диски реализуются на основе SSD, которые обеспечивают гораздо большую производительность и более низкую задержку, чем обычные диски. Производительность при использовании Elasticsearch можно сравнить с производительностью при использовании хранилища класса Premium, причем вы за это не доплачиваете. Дополнительные сведения см. в разделе [Решение проблемы задержки диска](#addressing-disk-latency-issues).

Размер виртуальной машины ограничивает объем места во временном хранилище, как описано в статье [D-Series Performance Expectations](https://azure.microsoft.com/blog/d-series-performance-expectations/) (Ожидаемая производительность серии D). Например, виртуальная машина Standard\_D1 предоставляет 50 ГБ временного хранилища, виртуальная машина Standard\_D2 — 100 ГБ, а виртуальная машина Standard\_D14 — 800 ГБ. Использование временного хранилища виртуальных машин серии D может быть выгодно для кластеров, узлам которых достаточно такого объема: на момент написания статьи расчетная стоимость виртуальных машин серии D4 под управлением Linux составляла 458 долл./месяц. Эквивалентная виртуальная машина DS4 под управлением Linux с одним диском SSD P30 предоставляет 1024 ГБ пространства и стоит 645 долл./месяц (виртуальная машина — 509 долл., SSD — 136 долл.). Для хранения 1024 ГБ данных во временном хранилище требуется 3 виртуальные машины серии D4. Их стоимость — 1374 долл./месяц. Тем не менее не стоит учитывать только объем хранилища. Одна виртуальная машина DS4 предоставляет 8 ядер ЦП и 28 ГБ памяти, в то время как 3 виртуальные машины D4 вместе предоставляют 24 ядра ЦП и 84 ГБ памяти. Если рабочие нагрузки требуют интенсивного использования ЦП, производительность будет лучшей, если распределить нагрузки между 3 виртуальными машинами, а не использовать для их обработки одну виртуальную машину. Кроме того, использование одной виртуальной машины (или всего нескольких виртуальных машин) может повлиять на устойчивость и восстанавливаемость кластера.

> [AZURE.NOTE] Цены указаны здесь только для иллюстрации. С момента публикации этой статьи они могли измениться. Текущие цены см. на странице [Калькулятор цен](https://azure.microsoft.com/pricing/calculator/).

Необходимо привести пропускную способность, увеличенную за счет временного хранилища, в соответствие с временем и затратами на восстановление данных после перезапуска компьютера. В случае перемещения виртуальной машины на другой сервер узла, обновления узла или сбоя оборудования на узле содержимое временного диска будет утеряно. Если данные сами по себе имеют ограниченное время существования, такая потеря данных может быть приемлемой. Если данные долгосрочного хранения, можно попытаться перестроить индекс или восстановить отсутствующие данные из резервной копии. Чтобы максимально снизить вероятность потерь, можно использовать реплики, которые хранятся на других виртуальных машинах.

> [AZURE.NOTE] Не рекомендуется хранить важные рабочие данные на **одной** виртуальной машине. В случае сбоя узла все данные будут недоступны. Критически важные данные следует реплицировать по крайней мере на еще один узел.

**Файлы Azure**

[Служба файлов Azure](http://blogs.msdn.com/b/windowsazurestorage/archive/2014/05/12/introducing-microsoft-azure-file-service.aspx) предоставляет доступ к общим файлам через службу хранилища Azure. Вы можете создать общие папки, к которым затем можно подключаться на виртуальных машинах Azure. К одной общей папке можно подключить несколько виртуальных машин, тем самым предоставив им общий доступ к данным.

Из соображений производительности общие папки не рекомендуется использовать для хранения данных Elasticsearch, которые не должны совместно использоваться узлами. Для этого лучше использовать обычные диски данных. Общие папки можно использовать для создания [теневых индексов реплик](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-shadow-replicas.html) Elasticsearch. Пока это только экспериментальная функция и ее не следует применять в рабочей среде. По этой причине теневые индексы не рассматриваются в этом руководстве более подробно.

**Характеристики сети**

Azure использует общую сетевую схему. Виртуальные машины, которые используют одни и те же аппаратные стойки, конкурируют за сетевые ресурсы. Поэтому доступная пропускная способность сети может различаться в зависимости от времени суток и ежедневного цикла выполняемых заданий на виртуальных машинах, которые используют одну физическую сетевую инфраструктуру. Вы практически никак не можете повлиять на эти факторы. Производительность сети со временем меняется, поэтому параметры сети необходимо настраивать так, чтобы они соответствовали ожиданиям пользователей.

## Рекомендации по увеличению масштаба узлов для поддержки приема данных больших объемов

Кластеры Elasticsearch можно создать, используя среднее количество аппаратных компонентов. По мере увеличения объема данных и числа запросов их можно масштабировать. Для увеличения масштаба в Azure можно использовать более дорогие виртуальные машины большего размера, а для развертывания дополнительные недорогие виртуальные машины меньшего размера. Можно использовать сочетание этих двух решений. Универсального решения, которое подходит для всех сценариев, не существует. Поэтому, чтобы определить оптимальный подход в конкретной ситуации, необходимо выполнить несколько тестов производительности.

В этом разделе описывается подход с увеличением масштаба. Развертывание рассматривается в разделе [Рекомендации по увеличению масштаба кластеров для поддержки приема данных больших объемов](#scaling-out-clusters). В этом разделе описаны результаты тестов производительности, выполненных для набора кластеров Elasticsearch, содержащих виртуальные машины различных размеров. Кластеры были разделены на кластеры небольшого, среднего и большого размеров. В следующей таблице перечислены ресурсы, выделенные для виртуальных машин в каждом кластере.

| HDInsight | SKU виртуальной машины | Количество ядер | Количество дисков данных | ОЗУ |
|---------|-------------|-----------------|----------------------|------|
| Малый | Standard D2 | 2 | 4\. | 7 ГБ |
| Средний | Standard D3 | 4\. | 8 | 14 ГБ |
| Крупный | Standard D4 | 8 | 16 | 28 ГБ |

Каждый рассматриваемый кластер Elasticsearch содержал 3 узла данных. Эти узлы данных использовались для выполнения клиентских запросов и обработки данных. Некоторые клиентские узлы не использовались, так как от них не было особой пользы для сценария приема данных, используемого в тестах. Кластер также содержал три главных узла, один из которых использовался Elasticsearch для управления кластером.

Тесты проводились с использованием ElasticSearch версии 1.7.3 Сначала они были выполнены на кластерах под управлением Ubuntu Linux 14.0.4, а затем в Windows Server 2012. Дополнительные сведения о рабочей нагрузке, используемой для тестов, см. в [приложении](#appendix-the-bulk-load-data-ingestion-performance-test).

> [AZURE.IMPORTANT] Как описано в разделе "Характеристики сети", на статистику производительности распределенных служб в облачной среде в значительной степени влияет пропускная способность сети при физической передаче данных в эти службы и их получение. При создании и развертывании системы, такой как кластер Elasticsearch, вы можете самостоятельно настроить параметры ЦП, объем памяти и размер доступных ресурсов диска, выбрав размер и SKU виртуальной машины. Степень контроля над доступными сетевыми ресурсами гораздо меньше, так как их совместно используют виртуальные машины, расположенные в одной аппаратной стойке. Кроме того, сетевые ресурсы также зависят от объема входящего и исходящего трафика центра обработки данных. Поэтому крайне важно выполнять тесты сравнения производительности с использованием одно и того же центра обработки данных в примерно одно и то же время суток в течение рабочей недели. Результаты тестов, выполненных для виртуальных машин в разных центрах обработки данных или в разное время, могут значительно отличаться.

### Производительность приема данных для Ubuntu Linux 14.0.4

В следующей таблице приведены общие результаты тестов для каждой конфигурации, которые выполнялись в течение двух часов:

| Конфигурация | Количество образцов | Среднее время отклика (мс) | Пропускная способность (операций в секунду) |
|---------------|-----------|----------------------------|---------------------------|
| Малый | 67 057 | 636 | 9,3 |
| Средний | 123 482 | 692 | 17,2 |
| Крупный | 197 085 | 839 | 27,4 |

Приблизительное соотношение пропускной способности и количества обработанных выборок для трех конфигураций — 1:2:3. Однако соотношение объема ресурсов памяти, числа ядер ЦП и дисков — 1:2:4. Помимо прочего, мы проанализировали сведения о низкой производительности узлов в кластере, чтобы определить причину ухудшения. На основе этих сведений можно определить, есть ли ограничения увеличения масштаба и когда лучше применять развертывание.

### Определение ограничивающих факторов. Использование сети

Возможность поддержки притока клиентских запросов, а также сведений о синхронизации, передаваемой между узлами в кластере, в Elasticsearch зависит от наличия достаточной пропускной способности сети. Как отмечено ранее, у вас ограниченный контроль над пропускной способностью, которая зависит от многих факторов, например от используемого центра обработки данных и текущей сетевой нагрузки остальных виртуальных машин, которые совместно используют одну инфраструктуру сети. Тем не менее все же стоит проверять активность сети для каждого кластера, чтобы убедиться, что объем трафика не чрезмерный. На графике ниже представлено сравнение объемов сетевого трафика, полученного узлом 2 в каждом кластере (объемы трафика для других узлов в каждом кластере были схожи).

![](media/guidance-elasticsearch-data-ingestion-image1.png)

Узел 2 в каждой конфигурации кластера за два часа получал следующее среднее число байтов в секунду:

| Конфигурация | Среднее число полученных байтов в секунду |
|---------------|--------------------------------------|
| Малый | 3 993 640,346 |
| Средний | 7 311 689,897 |
| Крупный | 11 893 874,2 |

> [AZURE.NOTE] При проведении тестов система находилась в устойчивом состоянии. При повторной балансировке индексов или восстановлении узла передача данных между узлами, содержащими основные сегменты и сегменты реплики, может привести к увеличению объема сетевого трафика. Влияние этого процесса более подробно описано в документе Configuring, Testing, and Analyzing Elasticsearch Resilience and Recovery (Настройка, тестирование и анализ устойчивости и восстановления Elasticsearch).

### Определение ограничивающих факторов. Загрузка ЦП

Скорость обработки запросов по крайней мере частично регулируется доступной мощностью обработки. Elasticsearch принимает запросы на массовую вставку в очередь массовой вставки. Каждый узел содержит ряд очередей массовой вставки, число которых зависит от количества доступных процессоров. По умолчанию для каждого процессора выделяется одна очередь. Каждая такая очередь может содержать до 50 запросов, ожидающих обработки, пока они не будут отклонены. Приложения должны отправлять запросы с такой скоростью, которая не приведет к переполнению очередей. Количество элементов в каждой очереди в любой момент времени будет зависеть от скорости отправки запросов клиентскими приложениями и скорости их получения и обработки в Elasticsearch. Именно поэтому в следующей таблице представлены важные статистические данные по частоте ошибок.

| Конфигурация | Общее число образцов | Количество ошибок | Частота ошибок |
|---------------|---------------|-----------|------------|
| Малый | 67 057 | 0 | 0,00 % |
| Средний | 123 483 | 1 | 0,0008 % |
| Крупный | 200 702 | 3617 | 1,8 % |

Каждая из этих ошибок вызвана следующим исключением Java:

```
org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase$1@75a30c1b]; ]
[219]: index [systembase], type [logs], id [AVEAioKb2TRSNcPa_8YG], message [RemoteTransportException[[esdatavm2][inet[/10.0.1.5:9300]][indices:data/write/bulk[s]]]; nested: EsRejectedExecutionException[rejected execution (queue capacity 50)
```

Чтобы снизить число ошибок, можно увеличить число очередей и/или длину каждой очереди. Однако такой подход применим только при кратковременных скачках. Если применить его при выполнении ряда непрерывных задач по приему данных, ошибки начнут возникать позже. Кроме того, производительность не улучшится, и, скорее всего, это негативно скажется на времени отклика клиентских приложений, так как запросы будут ожидать обработки в очереди дольше.

На следующих графиках продемонстрированы данные по загрузке ЦП в самых загруженных узлах в каждом кластере.

> [AZURE.NOTE] При использовании структуры индекса по умолчанию с 5 сегментами и 1 репликой (всего 10 сегментов) нагрузка между узлами в кластере распределяется слегка неравномерно. Два узла будут содержать 3 сегмента, а один узел — 4 сегмента. Скорее всего, максимально загруженный узел будет больше всего ограничивать пропускную способность. Поэтому в каждом случае выбран именно этот узел:

![](media/guidance-elasticsearch-data-ingestion-image2.png)

![](media/guidance-elasticsearch-data-ingestion-image3.png)

![](media/guidance-elasticsearch-data-ingestion-image4.png)

В кластерах небольшого, среднего и большого размеров средняя загрузка ЦП для этих узлов составляла 75,01 %, 64,93 % и 64,64 %. ЦП редко бывает загружен на 100 %. Загрузка уменьшается с увеличением размера узлов и доступной мощности ЦП. Поэтому маловероятно, что мощность ЦП ограничивает производительность кластеров большого размера.

### Определение ограничивающих факторов. Память

Использование памяти — еще один важный аспект, который может влиять на производительность. С целью проведения тестов Elasticsearch выделено 50 % доступной памяти. Это соответствует [рекомендациям в документации](https://www.elastic.co/guide/en/elasticsearch/guide/current/heap-sizing.html#_give_half_your_memory_to_lucene). При тестировании выполнялся мониторинг виртуальной машины Java для выявления чрезмерной активности сбора мусора (что указывает на недостаток динамической памяти). Во всех случаях размер кучи был стабильным, а показатели операций по сбору мусора в виртуальной машине Java были низкими. На снимке экрана на рисунке 1 представлен моментальный снимок Marvel, на котором выделена ключевая статистика виртуальной машины Java за короткий период при выполнении тестов в большом кластере.

![](media/guidance-elasticsearch-data-ingestion-image5.png)

***Рис. 1. Память и операции по сбору мусора в виртуальной машине Java в большом кластере***

### Определение ограничивающих факторов. Число операций ввода-вывода на диске.
Последняя физическая характеристика сервера, которая может ограничить производительность — производительность подсистемы ввода-вывода диска. На графике ниже представлено сравнение активности дисков по количеству байтов, записанных на самых загруженных узлах в каждом кластере.

![](media/guidance-elasticsearch-data-ingestion-image6.png)

В следующей таблице содержатся сведения о среднем числе байтов, записываемых в секунду, на узле 2 в каждой конфигурации кластера за два часа:

| Конфигурация | Среднее число записанных байтов в секунду |
|---------------|-------------------------------------|
| Малый | 25 502 361,94 |
| Средний | 48 856 124,5 |
| Крупный | 88 137 675,46 |

Объем записываемых данных увеличивается с ростом числа запросов, обрабатываемых кластером, а скорость операций ввода-вывода находится в пределах, определенных для службы хранилища Azure (диски, созданные с помощью хранилища Azure, могут поддерживать постоянную скорость от 10 до 100 с для показателя числа МБ в секунду, в зависимости от используемого хранилища (класса Standard или Premium)). Если отследить количество времени, затраченное на ожидание операций ввода-вывода диска, можно объяснить, почему пропускная способность диска значительно меньше теоретического максимального значения. На графиках и в таблице ниже показана статистика по этим показателям для одних и тех самых трех узлов:

> [AZURE.NOTE] Время ожидания диска измеряется путем определения процента времени ЦП, в течение которого процессоры блокируются в ожидании завершения операций ввода-вывода.

![](media/guidance-elasticsearch-data-ingestion-image7.png)

![](media/guidance-elasticsearch-data-ingestion-image8.png)

![](media/guidance-elasticsearch-data-ingestion-image9.png)

| Конфигурация | Среднее время ожидания ЦП для диска (%) |
|---------------|--------------------------------|
| Малый | 21,04 |
| Средний | 14,48 |
| Крупный | 15,84 |

Исходя из этих данных можно сказать, что значительная часть времени ЦП (от 16 до 21 %) тратится на ожидание завершения операций ввода-вывода на диске. Это ограничивает возможность обработки запросов и хранения данных Elasticsearch.

> [AZURE.NOTE]  При тестовом запуске в большом кластере выполнена вставка более пятисот миллионов документов. Затем при выполнении теста выявлено, что время ожидания существенно увеличилось, когда в базу данных поступило более шестисот миллионов документов. Причины такого поведения не были полностью исследованы. Возможно, это произошло из-за фрагментации диска, что привело к увеличению задержки. Чтобы устранить последствия такого поведения, можно увеличить размер кластера на несколько узлов. В исключительных случаях может потребоваться выполнить дефрагментацию диска, на котором операции ввода-вывода выполняются слишком долго. Однако дефрагментация диска большого объема может занять много времени (возможно, более 48 часов для виртуального жесткого диска объемом 2 ТБ). Более экономичный подход — переформатировать диск и разрешить Elasticsearch восстановить отсутствующие данные из сегментов реплики.

### Решение проблемы задержки диска

Изначально тесты проводились на виртуальных машинах со стандартными дисками. Стандартный диск создан на основе вращающегося носителя и поэтому подвержен возникновению задержки поиска нужного сектора и других проблем, которые могут ограничивать скорость ввода-вывода. Кроме того, Azure предоставляет хранилище класса Premium, в котором диски создаются с помощью устройств SSD. Эти устройства не подвержены возникновению задержки поиска нужного сектора и поэтому должны обеспечивать повышенную скорость ввода-вывода. В следующей таблице сравниваются результаты замены стандартных дисков дисками класса Premium в большом кластере. Виртуальные машины уровня Standard серии D4 в большом кластере заменены аналогичными виртуальными машинами серии DS4. В обоих случаях количество ядер, объема памяти и число дисков были одинаковыми. Единственное отличие заключалось в том, что в виртуальных машинах DS4 использовались диски SSD.

| Конфигурация | Количество образцов | Среднее время отклика (мс) | Пропускная способность (операций в секунду) |
|------------------|-----------|----------------------------|---------------------------|
| Большой кластер, диски класса Standard | 197 085 | 839 | 27,4 |
| Большой кластер, диски класса Premium | 255 985 | 581 | 35,6 |

Время отклика значительно улучшилось, в результате чего средняя пропускная способность почти в 4 раза превысила пропускную способность для кластера небольшого размера. Такие показатели соответствуют ресурсам, доступным в виртуальной машине уровня Standard серии DS4. Средняя загрузка ЦП на самом загруженном узле кластера (в этом случае узел 1) увеличилась, так как время ожидания завершения операций ввода-вывода сократилось:

![](media/guidance-elasticsearch-data-ingestion-image10.png)

Уменьшение времени ожидания диска становится очевидным, если рассмотреть приведенный ниже график, на котором показано, что для самого загруженного узла этот статистический показатель упал в среднем на 1 %:

![](media/guidance-elasticsearch-data-ingestion-image11.png)

Однако за такое улучшение приходится платить. Количество ошибок приема увеличилось в 10 раз до 35 797 (12,3 %). Опять же большинство этих ошибок произошли из-за переполнения очереди массовой вставки. Учитывая, что теперь оборудование работает на пределе своих возможностей, чтобы уменьшить количество ошибок, может потребоваться добавить дополнительные узлы или урегулировать скорость выполнения операций массовой вставки в сторону уменьшения. Эти проблемы рассматриваются далее в этом документе.

### Тестирование с использованием временного хранилища

Те же тесты повторно выполнены в кластере с виртуальными машинами D4 при использовании временного хранилища. На виртуальных машинах D4 временное хранилище реализовано в виде одного SSD объемом 400 ГБ. Количество обработанных образцов, время отклика и пропускная способность были очень похожи на показатели для кластера с виртуальными машинами DS14 и хранилищем класса Premium.

| Конфигурация | Количество образцов | Среднее время отклика (мс) | Пропускная способность (операций в секунду) |
|-----------------------------------|-----------|----------------------------|---------------------------|
| Большой кластер, диски класса Premium | 255 985 | 581 | 35,6 |
| Большой кластер, диски класса Standard (временный диск) | 255 626 | 585 | 35,5 |

Частота ошибок тоже была аналогичной (33 862 сбоев на 289 488 запросов, т. е. 11,7 %).

На следующих графиках представлены статистические данные по загрузке ЦП и времени ожидания диска для самого загруженного узла в кластере (в этом случае узел 2):

![](media/guidance-elasticsearch-data-ingestion-image12.png)

![](media/guidance-elasticsearch-data-ingestion-image13.png)

В этом случае, если оценивать только производительность, использовать временное хранилище гораздо экономичнее, чем использовать хранилище класса Premium.

### Производительность приема данных для Windows Server 2012

Те же тесты выполнены повторно в ряде кластеров Elasticsearch с узлами под управлением Windows Server 2012. В рамках этих тестов нужно было установить влияние (если таковое имелось) операционной системы на производительность кластера.

Чтобы продемонстрировать масштабируемость Elasticsearch в Windows, в таблице ниже приведены значения пропускной способности и времени отклика, полученные для конфигураций кластеров небольшого, среднего и большого размеров. Обратите внимание, что при выполнении всех этих тестов для Elasticsearch настроено временное хранилище SSD, так как в результате проведения тестов на компьютерах с ОС Ubuntu выяснилось, что задержка диска, скорее всего, является ключевым фактором в достижении максимальной производительности:

| Конфигурация | Количество образцов | Среднее время отклика (мс) | Пропускная способность (операций в секунду) |
|---------------|-----------|----------------------------|---------------------------|
| Малый | 90 295 | 476 | 12,5 |
| Средний | 169 243 | 508 | 23,5 |
| Крупный | 257 115 | 613 | 35,6 |

Эти результаты показывают, как Elasticsearch масштабируется с учетом размера виртуальной машины и доступных ресурсов в Windows.

В следующих таблицах сравниваются результаты для больших кластеров в ОС Ubuntu и Windows:

| Операционная система | Количество образцов | Среднее время отклика (мс) | Пропускная способность (операций в секунду) | Частота ошибок (%) |
|------------------|-----------|----------------------------|---------------------------|----------------|
| Ubuntu | 255 626 | 585 | 35,5 | 11,7 |
| Windows | 257 115 | 613 | 35,6 | 7,2 |

Пропускная способность соответствовала показателю для больших кластеров Ubuntu, хотя время отклика было немного выше. Это можно объяснить меньшей частотой ошибок (об ошибках сообщается быстрее, чем об успешных операциях, поэтому время отклика для ошибок ниже).

Значение загрузки ЦП, полученное с помощью средств мониторинга Windows, немного больше, чем для Ubuntu. Тем не менее сравнивать показатели разных операционных систем следует с особой осторожностью, так как в каждой из них эти показатели представляются по-разному. Кроме того, получение доступа к сведениям о задержке диска с точки зрения времени ЦП, затраченного на ожидание ввода-вывода, в Ubuntu и других ОС отличается. Обратите внимание, что загрузка ЦП была высокой. Это означает, что время, затраченное на ожидание ввода-вывода, было низким:

![](media/guidance-elasticsearch-data-ingestion-image14.png)

### Увеличение масштаба. Выводы

Производительность Elasticsearch для правильно настроенного кластера, скорее всего, будет одинаковой в Windows и Ubuntu. Увеличение масштаба в обоих операционных системах происходит по аналогичной схеме. Чтобы добиться лучшей производительности, используйте SSD для хранения данных Elasticsearch.

## <a name="scaling-out-clusters"></a> Рекомендации по увеличению масштаба кластеров для поддержки приема больших объемов данных

Кроме увеличения масштаба, рассмотренного в предыдущем разделе, также используется развертывание. Важная характеристика Elasticsearch — встроенная горизонтальная масштабируемость программного обеспечения. Увеличивая размер кластера, вы просто добавляете дополнительные узлы. Для повторного распространения индексов или сегментов ничего не нужно делать вручную, так как эти задачи выполняются автоматически. Однако на этот процесс можно повлиять, использовав некоторые параметры конфигурации. При добавлении дополнительных узлов повышается производительность за счет распределения нагрузки по нескольким устройствам. При этом можно также выполнить повторное индексирование данных, чтобы увеличить количество доступных сегментов. Этот процесс можно в некоторой степени выполнить заранее. Для этого нужно создать индексы, количество сегментов в которых превышает количество изначально доступных узлов. При добавлении дополнительных узлов сегменты можно распределить.

Помимо получения преимуществ горизонтального масштабирования Elasticsearch индексы, количество сегментов в которых превышает количество узлов, следует использовать и по другим причинам. Каждый сегмент реализуется как отдельная структура данных (индекс [Lucene](https://lucene.apache.org/)). В нем предусмотрен собственный внутренний механизм для поддержки согласованности и обеспечения параллелизма. Создание нескольких сегментов позволяет повысить уровень параллелизма в узле и увеличить производительность. Однако поддержка определенного уровня производительности при масштабировании сопровождается компромиссами. Чем больше узлов и сегментов содержит кластер, тем сложнее синхронизировать выполняемые им операции. Это может привести к снижению пропускной способности. Для любой заданной рабочей нагрузки существует "золотая середина", при достижении которой производительность приема максимально увеличивается, а дополнительные затраты на обслуживание снижаются до минимума. Эта "золотая середина" сильно зависит от характера рабочей нагрузки и кластера, в частности от объема, размера и содержимого документов, скорости приема и оборудования системы.

В этом разделе представлены сводные результаты исследований по определению размера кластеров, которые должны поддерживать рабочую нагрузку, использованную в тестах производительности, описанных выше. Тестирование выполнено в кластерах с виртуальными машинами больших размеров (Standard серии D4 с 8 ядрами ЦП, 16 дисками данных и 28 ГБ ОЗУ) под управлением Ubuntu Linux 14.0.4, для которых установлено разное число узлов и сегментов. Результаты не должны быть окончательными, так как они применимы только в одном конкретном сценарии. Однако они могут послужить в качестве хорошей отправной точки для анализа горизонтальной масштабируемости кластеров и формирования значений для оптимального соотношения сегментов и узлов, которое лучше всего удовлетворяет требованиям вашей системы.

### Результаты для базовой конфигурации из 3 узлов

Чтобы получить базовые показатели, тест производительности приема данных выполнен для кластера с 3 узлами, 5 сегментами и 1 репликой. Это конфигурация по умолчанию для индекса Elasticsearch. В этой конфигурации Elasticsearch распределяет 2 основных сегмента между 2 узлами, а оставшийся основной сегмент назначается третьему узлу. В таблице ниже приведены данные о пропускной способности на основе таких показателей, как количество операций массового приема в секунду и количество документов, успешно сохраненных в ходе теста.

> [AZURE.NOTE] В последующих таблицах распределение основных сегментов представлено в виде чисел для каждого узла, разделенных дефисом. Например, структура из 5 сегментов и 3 узлов обозначается как 2-2-1. Структура сегментов реплики не включена. Они распределяются аналогично основным сегментам.

| Конфигурация | Количество документов | Пропускная способность (операций в секунду) | Структура сегментов |
|---------------|--------------|-----------------------------|--------------|
| 5 сегментов | 200 560 412 | 27,86 | 2-2-1 |

### Результаты для конфигурации из 6 узлов

Тест проведен повторно для кластера из 6 узлов. В рамках этих тестов нужно было точнее выяснить влияние, оказываемое при хранении нескольких сегментов на узле.

| Конфигурация | Количество документов | Пропускная способность (операций в секунду) | Структура сегментов |
|---------------|--------------|-----------------------------|--------------|
| 4 сегмента | 227 360 412 | 31,58 | 1-1-0-1-1-0 |
| 7 сегментов | 268 013 252 | 37,22 | 2-1-1-1-1-1 |
| 10 сегментов | 258 065 854 | 35,84 | 1-2-2-2-1-2 |
| 11 сегментов | 279 788 157 | 38,86 | 2-2-2-1-2-2 |
| 12 сегментов | 257 628 504 | 35,78 | 2-2-2-2-2-2 |
| 13 сегментов | 300 126 822 | 41,68 | 2-2-2-2-2-3 |

Предположительно, эти результаты указывают на следующие тенденции:

* Добавление дополнительных сегментов на каждый узел позволяет повысить пропускную способность. При использовании небольшого числа сегментов на узел, созданных для этих тестов, это явление было предсказуемым по причинам, описанным выше.

* При добавлении нечетного числа сегментов прирост производительности выше, чем при добавлении четного числа. Причины этого явления не совсем понятны. *Возможно*, в этом случае алгоритм маршрутизации, используемый Elasticsearch, лучше справляется с распределением данных по сегментам, поэтому нагрузка на каждом узле распределена более равномерно.

Чтобы проверить эти гипотезы, проведено несколько дополнительных тестов с большим числом сегментов. Решено применить такую тактику к Elasticsearch: для каждого теста использовать количество сегментов, представляющее простое число, так как в результате получается приемлемое распределение показателей, представленных нечетными числами, в рассматриваемом диапазоне.

| Конфигурация | Количество документов | Пропускная способность (операций в секунду) | Структура сегментов |
|---------------|--------------|-----------------------------|-------------------|
| 23 сегмента | 312 844 185 | 43,45 | 4-4-4-3-4-4 |
| 31 сегмент | 309 930 777 | 43,05 | 5-5-5-5-6-5 |
| 43 сегмента | 316 357 076 | 43,94 | 8-7-7-7-7-7 |
| 61 сегмент | 305 072 556 | 42,37 | 10-11-10-10-10-10 |
| 91 сегмент | 291 073 519 | 40,43 | 15-15-16-15-15-15 |
| 119 сегментов | 273 596 325 | 38,00 | 20-20-20-20-20-19 |

Исходя из этих результатов, переломный момент достигнут при использовании примерно 23 сегментов. С этого момента увеличение числа сегментов приводило к незначительному ухудшению производительности (пропускная способность для 43 сегментов, возможно, представляет собой аномалию).

### Результаты для конфигурации из 9 узлов

Тесты проведены повторно для кластера с 9 узлами с использованием количества сегментов, представляющих простое число.

| Конфигурация | Количество документов | Пропускная способность (операций в секунду) | Структура сегментов |
|---------------|--------------|-----------------------------|----------------------------|
| 17 сегментов | 325 165 364 | 45,16 | 2-2-2-2-2-2-2-2-1 |
| 19 сегментов | 331 272 619 | 46,01 | 2-2-2-2-2-2-2-2-3 |
| 29 сегментов | 349 682 551 | 48,57 | 3-3-3-4-3-3-3-4-3 |
| 37 сегментов | 352 764 546 | 49,00 | 4-4-4-4-4-4-4-4-5 |
| 47 сегментов | 343 684 074 | 47,73 | 5-5-5-6-5-5-5-6-5 |
| 89 сегментов | 336 248 667 | 46,70 | 10-10-10-10-10-10-10-10-9 |
| 181 сегмент | 297 919 131 | 41,38 | 20-20-20-20-20-20-20-20-21 |

В этих результатах наблюдалась аналогичная закономерность. Переломный момент наступил при использовании около 37 сегментов.

### Развертывание. Выводы.

Учитывая результаты тестов для кластеров с 6 и 9 узлами и применив грубую экстраполяцию можно сказать, что в этой конкретной ситуации оптимальное количество сегментов, которое позволяет увеличить производительность, определяется формулой 4n+/-1, где n — количество узлов. Это количество, *возможно*, зависит от числа доступных потоков операций массовой вставки, которое в свою очередь зависит от числа ядер ЦП. Этому можно дать такое разумное объяснение (дополнительные сведения см. в статье [Multidocument Patterns](https://www.elastic.co/guide/en/elasticsearch/guide/current/distrib-multi-doc.html#distrib-multi-doc) (Шаблоны для нескольких документов)):

* Каждый запрос на массовую вставку, отправленный клиентским приложением, приходится на один узел данных.

* Узел данных формирует новый запрос на массовую вставку для каждого основного сегмента, задействованного в обработке исходного запроса, и параллельно отправляет их на другие узлы.

* Так как каждый основной сегмент записывается, каждой реплике указанного сегмента отправляется другой запрос. Основной сегмент ожидает выполнения запроса, отправленного реплике.

По умолчанию Elasticsearch создает по одному потоку операций массовой вставки для каждого доступного ядра ЦП на виртуальной машине. При проведении теста на виртуальных машинах D4 каждый ЦП содержал 8 ядер, поэтому создано 8 потоков операций массовой вставки. Индекс использовал 4 связанных основных сегмента (в одном случае 5) на каждом узле. Кроме того, на каждом узле содержалось по 4 (5) реплики. При вставке данных в этих сегментах и репликах могло использоваться до 8 потоков на каждом узле на запрос. Это количество совпадает с количеством доступных потоков. Увеличение или уменьшение количества сегментов может привести к неэффективному использованию потоков, так как, возможно, потоки не используются или запросы ставятся в очередь. Тем не менее без дальнейших исследований это всего лишь теория, поэтому невозможно прийти к конкретным выводам.

Тесты демонстрируют еще один важный момент. В этом сценарии увеличение числа узлов может способствовать повышению пропускной способности приема данных. Однако результаты не всегда поддаются линейному масштабированию. При проведении дополнительных тестов с кластерами на 12 и 15 узлов можно было бы определить момент, при достижении которого увеличение масштаба приносит небольшие дополнительные преимущества. Если такого числа узлов недостаточно для хранения, возможно, необходимо вернуться к стратегии увеличения масштаба и начать использовать дополнительные диски или диски большего объема на основе хранилища класса Premium.

> [AZURE.IMPORTANT] Не рассматривайте соотношение 4n+/-1 как идеальную формулу, которая подойдет для каждого кластера. Если доступных ядер ЦП больше или меньше, оптимальная конфигурация сегментов может отличаться. Результаты получены на основе определенной рабочей нагрузки, при которой выполнялся только прием данных. Для рабочих нагрузок, при которых выполняются запросы и агрегирование, результаты могут существенно отличаться.

> Кроме того, при рабочей нагрузке приема данных использовался один индекс. Во многих случаях данные чаще всего распределяются по нескольким индексам. Поэтому применяются различные шаблоны и ресурсы используются по-разному.

> При выполнении этого задания важно понять используемый метод, а не полученные результаты. Чтобы получить сведения, которые лучше всего подходят для конкретной ситуации, нужно быть готовым провести оценку масштабируемости на основе собственных рабочих нагрузок.

<span id="_Considerations_for_Tuning" class="anchor"></span>
## Рекомендации по настройке приема больших объемов данных

Elasticsearch предоставляет множество возможностей для настройки, а также параметров, с помощью которых можно оптимизировать производительность для конкретных вариантов использования и сценариев. В этом разделе описаны некоторые распространенные примеры. Помните, что гибкость, обеспечиваемая Elasticsearch в этом отношении, имеет свою цену. Очень просто нарушить настройку Elasticsearch и ухудшить производительность. Настраивайте только один показатель за раз и всегда оценивайте влияние любых изменений, чтобы убедиться, что они не оказывают негативного влияния на систему.

### Оптимизация ресурсов для операций индексирования

В следующем списке описаны некоторые рекомендации, которые следует учитывать при настройке кластера Elasticsearch для поддержки приема больших объемов данных. При выполнении первых двух повышение производительности, скорее всего, будет заметно сразу. При выполнении оставшихся рекомендаций повышение производительности будет не очевидным в зависимости от рабочей нагрузки:

*  Новые документы, добавленные в индекс, становятся доступными для поиска при обновлении индекса. Обновление индекса — дорогостоящая операция, поэтому она выполняется периодически, а не при создании каждого документа. По умолчанию интервал обновления составляет 1 секунду. При выполнении массовых операций можно временно отключить обновление индекса. Для этого задайте для параметра индекса *refresh\_interval* значение -1.

	```http
	PUT /my_busy_index
	{
		"settings" : {
			"refresh_interval": -1
		}
	}
	```

	Активируйте обновление вручную с помощью API [*\_refresh*](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-refresh.html) по завершении операции, чтобы сделать данные доступными. Дополнительные сведения см. в разделе [Bulk Indexing Usage](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-update-settings.html#bulk) (Использование массового индексирования). Более подробные сведения о [влиянии изменения интервала обновления на прием данных](#the-impact-of-changing-the-index-refresh-interval-on-data-ingestion-performance) описаны ниже.

* При репликации индекса каждая операция индексирования (создание, обновление или удаление документов) повторно выполняется в сегментах реплики по мере их выполнения в основном сегменте. Отключите репликацию во время выполнения операций массового импорта и снова включите ее после завершения импорта:

    ```http
	PUT /my_busy_index
	{
		"settings" : {
			"number_of_replicas": 0
		}
	}
	```

	При повторном включении репликации Elasticsearch выполняет передачу данных по сети в побайтовом режиме из индекса в каждую реплику. Это более эффективно, чем повторять индексирование документ за документом на каждом узле. Риск заключается в том, что данные могут быть утеряны, если в основном узле произойдет сбой при выполнении массового импорта. Однако для восстановления понадобится просто запустить импорт еще раз. Подробные сведения о [влиянии репликации на производительность приема данных](#the-impact-of-replicas-on-data-ingestion-performance) представлены далее.

* Поисковой сервер Elasticsearch пытается равномерно распределить доступные ресурсы, необходимые для выполнения запросов и приема данных. В результате он может регулировать производительность приема данных (события регулирования записываются в журнал Elasticsearch). Это ограничение позволяет предотвратить создание большого количества сегментов индекса одновременно, для чего требуется слияние и сохранение на диск. Такой процесс может монополизировать ресурсы. Если сейчас на компьютере не выполняются запросы, можно отключить регулирование приема данных. Таким образом, использование индексирования позволит увеличить производительность до максимума. Вот как можно отключить регулирование для всего кластера:

	```http
	PUT /_cluster/settings
	{
		"transient" : {
			"indices.store.throttle.type": "none"
		}
	}
	```

  > [AZURE.IMPORTANT] По завершении приема измените тип регулирования кластера на *merge*. Кроме того, обратите внимание, что отключение регулирования может привести к нестабильной работе кластера. Поэтому убедитесь в наличии процедур, необходимых для восстановления кластера при необходимости.

* Elasticsearch резервирует часть динамической памяти для выполнения операций индексирования. Остальная часть в основном используется для запросов и операций поиска. Эти части памяти предназначены для уменьшения числа операций ввода-вывода на диске и выполнения небольшого количества операций записи с большим объемом данных, а не множества операций записи с небольшим объемом данных. По умолчанию выделяется 10 % динамической памяти. Если индексируется большой объем данных, такого объема памяти может быть недостаточно. Для систем с поддержкой операций приема больших объемов данных следует выделить до 512 МБ памяти для каждого активного сегмента в узле. Например, если при запуске Elasticsearch на виртуальных машинах D4 (28 ГБ ОЗУ) выделить 50 % доступной памяти виртуальной машине Java (14 ГБ), операции индексирования смогут использовать 1,4 ГБ. Если узел содержит 3 активных сегмента, этой конфигурации, вероятно, будет достаточно. Однако если узел содержит больше 3 сегментов, следует увеличить значение параметра *indices.memory.index\_buffer\_size* в файле конфигурации elasticsearch.yml. Дополнительные сведения см. в статье [Performance Considerations for Elasticsearch Indexing](https://www.elastic.co/blog/performance-considerations-elasticsearch-indexing) (Рекомендации по производительности индексирования Elasticsearch).

  > [AZURE.NOTE] Если выделить более 512 МБ на активный сегмент, скорее всего, производительность индексирования не улучшится, а ухудшится, так как в таком случае для выполнения других задач будет доступен меньший объем памяти. Помимо этого, имейте в виду, что при выделении большего объема памяти буферам индексов не остается памяти для выполнения других операций (поиск и агрегирование данных). Кроме того, это может привести к снижению производительности операций запроса.

* Elasticsearch ограничивает количество потоков (значение по умолчанию — 8), в которых могут одновременно выполняться операции индексирования в сегменте. Если узел содержит небольшое количество сегментов, можно увеличить значение параметра *index\_concurrency* для индекса с большим количеством операций индексирования или индекса, с помощью которого необходимо выполнить массовую вставку. Это можно сделать следующим образом:

	```http
	PUT /my_busy_index
	{
		"settings" : {
			"index_concurrency": 20
		}
	}
	```

* При выполнении множества операций индексирования и массовых операций за короткий период времени можно увеличить число потоков для *операций индексирования* и *массовых операций*, доступных в пуле потоков, и увеличить размер очереди *массовой вставки* для каждого узла данных. Таким образом больше запросов будут помещаться в очередь, а не удаляться. Дополнительные сведения см. в статье [Thread Pool](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-threadpool.html) (Пул потоков). При регулярном выполнении операций приема больших объемов данных не рекомендуется увеличивать число потоков для массовых операций. Вместо этого создайте дополнительные узлы и используйте сегментирование для распределения нагрузки индексирования по этим узлам. Кроме того, можно отправлять пакеты операций массовой вставки последовательно, а не параллельно. Это послужит естественным механизмом регулирования, который позволит снизить вероятность возникновения ошибок из-за переполнения очереди массовой вставки.

### Влияние изменения интервала обновления индекса на производительность приема данных

Интервал обновления определяет скорость, с которой полученные данные становятся доступными для запросов и агрегирования. Однако частые обновления могут повлиять на производительность операций приема данных. По умолчанию интервал обновления составляет 1 секунду. Обновление можно отключить полностью, но это может быть не применимо к рабочей нагрузке. Вы можете поэкспериментировать, задавая разные интервалы, и определить "золотую середину", при достижении которой можно достичь равновесия между требуемой производительностью приема и необходимостью предоставлять актуальные сведения.

Чтобы продемонстрировать пример такого влияния, несколько раз проведен тест производительности приема данных для кластера Elasticsearch, состоящего из 7 сегментов, которые распределены между 3 узлами данных. Индекс содержал одну реплику. Каждый узел данных был создан на основе виртуальной машины D4 (28 ГБ ОЗУ, 8 ядер процессора) с использованием временного хранилища с поддержкой SSD для хранения данных. Каждый тест выполнялся в течение часа.

В этом тесте для частоты обновления установлено значение по умолчанию (1 секунда). В таблице ниже представлено сравнение значений пропускной способности и времени отклика для этого теста со значениями этих показателей в тесте, выполненном отдельно, для которого значение частоты обновления уменьшено (каждые 30 секунд).

| Частота обновления | Количество образцов | Среднее время отклика для успешных операций (мс) | Пропускная способность для успешных операций (операций в секунду) |
|--------------|------------|----------------------------------------------------|---------------------------------------------------|
| 1 с | 93 755 | 460 | 26,0 |
| 30 секунд | 117 758 | 365 | 32,7 |

В этом тесте при уменьшении частоты обновления пропускная способность увеличилась на 18 %, а среднее время отклика уменьшилось на 21 %. На следующих графиках, созданных с помощью Marvel, можно увидеть, что является основной причиной такой разницы. На рис. 2 представлен уровень активности операций слияния индекса с интервалом обновления в 1 секунду, а на рис. 3 — уровень активности с интервалом обновления в 30 секунд. Слияние индекса выполняется, чтобы в памяти не создавалось слишком много сегментов индекса. При интервале обновления равном 1 секунде создается множество небольших сегментов, которые необходимо быстро объединять, а при интервале обновления равном 30 секундам создается меньшее количество больших сегментов, которые можно объединить более оптимально.

![](media/guidance-elasticsearch-data-ingestion-image15.png)

***Рис. 2. Активность слияния индекса для частоты обновления индекса равной 1 секунде***

![](media/guidance-elasticsearch-data-ingestion-image16.png)

***Рис. 3. Активность слияния индекса для частоты обновления индекса равной 30 секундам***

### Влияние реплик на производительность приема данных

Реплики являются важным компонентом любого отказоустойчивого кластера. Без них существует риск потери данных в случае сбоя узла. Однако реплики увеличивают объем операций ввода-вывода, выполняемых на дисках и в сети, что может отрицательно сказаться на скорости поступления данных. По причинам, описанным выше, может быть полезно временно отключить реплики во время выполнения операций передачи больших объемов данных.

Тесты производительности приема данных выполнены повторно с использованием трех конфигураций:

* кластер без реплик;

* кластер с одной репликой;

* кластер с двумя репликами.

Во всех случаях кластер содержал 7 сегментов, распределенных по 3 узлам, и выполнялся на виртуальных машинах, настроенных согласно предыдущим тестам. В тесте использовался интервал обновления индекса равный 30 секундам.

В следующей таблице перечислены значения времени отклика и пропускной способности в каждом тесте для сравнения:

| Конфигурация | Количество образцов | Среднее время отклика для успешных операций (мс) | Пропускная способность для успешных операций (операций в секунду) | Количество ошибок при приеме данных |
|---------------|------------|----------------------------------------------------|---------------------------------------------------|--------------------------|
| Без реплик | 215 451 | 200 | 59,8 | 0 |
| 1 реплика | 117 758 | 365 | 32,7 | 0 |
| 2 реплики | 94 218 | 453 | 26,1 | 194 262 |


Снижение производительности с ростом числа реплик вполне очевидно. Однако следует также обратить внимание на большое количество ошибок приема данных в третьем тесте. Согласно сообщениям об ошибках причиной послужило переполнение очереди массовой вставки, что вызвало отклонение запросов. Эти отклонения происходили очень быстро, поэтому ошибок так много.

> [AZURE.NOTE]  Учитывая результаты третьего теста, следует подчеркнуть важность применения интеллектуальной стратегии повторов при возникновении подобных временных ошибок. Она заключается в том, чтобы на короткий период прекратить выполнение операций массовой вставки, прежде чем выполнять их повторно. Таким образом очередь массовой вставки немного освободится.

На следующих графиках сравниваются значения времени отклика во время тестов. В каждом случае на первом графике показано общее время отклика, а на втором представлены подробные сведения о времени отклика для операций, которые выполнялись быстрее всего (обратите внимание, что показатели на первом графике в десять раз больше показателей на втором графике). Заметно, как изменяется профиль времени отклика во всех трех тестах.

В кластере без реплик продолжительность большинства операций составляла от 75 до 750 мс (самая быстрая операция — около 25 мс):

![](media/guidance-elasticsearch-data-ingestion-image17.png)

В кластере с 1 репликой при выполнении максимального количества операций оперативное время отклика составило от 125 до 1250 мс. Самое короткое время отклика составило 75 мс, несмотря на то, что операций с таким временем отклика было меньше, чем при использовании кластера без реплик. Кроме того, выявлено гораздо больше операций с временем отклика, сильно отличавшимся от наиболее распространенных случаев (свыше 1250 мс):

![](media/guidance-elasticsearch-data-ingestion-image18.png)

В кластере с 2 репликами при выполнении максимального количества операций время отклика составило от 200 до 1500 мс. Однако в этом случае получено гораздо меньше результатов ниже минимального диапазона, чем в тесте с 1 репликой. Тем не менее тенденции в результатах, превышающих верхний предел, были аналогичны тенденциям в результатах теста с 1 репликой. Это, скорее всего, вызвано переполнением очереди массовой вставки (превышение длины очереди в 50 запросов). Так как для поддержки 2 реплик требуются дополнительные ресурсы, очередь переполняется гораздо чаще, и, как результат, время отклика операций приема находится в разумных пределах. Операции быстро отклоняются, а не выполняются продолжительное время, что, возможно, приводит к возникновению исключений времени ожидания или влияет на скорость реагирования клиентских приложений (именно для этого предназначен механизм очереди массовой вставки):

![](media/guidance-elasticsearch-data-ingestion-image19.png)

<span id="_The_Impact_of_1" class="anchor"><span id="_Impact_of_Increasing" class="anchor"></span></span>Использовав Marvel, можно заметить влияние числа реплик на очередь массового индексирования. На рис. 4 представлены данные Marvel, по которым видно, как заполняется очередь массовой вставки во время теста. Средняя длина очереди составляла примерно 40 запросов, но периодические скачки количества запросов вызвали переполнение очереди, в результате чего запросы отклонялись:

![](media/guidance-elasticsearch-data-ingestion-image20.png)

***Рис. 4 Размер очереди массового индексирования и число отклоненных запросов с 2 репликами***

Эти результаты можно сравнить с результатами для одной реплики на рис. 5. Механизм Elasticsearch обрабатывал запросы достаточно быстро. Таким образом средняя длина очереди составляла около 25 запросов. Так как длина очереди ни разу не превысила 50 запросов, они не отклонялись.

![](media/guidance-elasticsearch-data-ingestion-image21.png)

***Рис. 5. Размер очереди массового индексирования и число отклоненных запросов с 1 репликой***

## Рекомендации по отправке данных клиентами в Elasticsearch

Многие аспекты производительности связаны не только с внутренними характеристиками системы, но и с тем, как ее используют клиентские приложения. В Elasticsearch представлено множество функций, которые могут использоваться при приеме данных, например создание уникальных идентификаторов для документов, анализ документов и даже использование сценариев для преобразования хранящихся данных. Тем не менее их выполнение приводит к увеличению нагрузки на механизм Elasticsearch. Зачастую до передачи данных с ними могут более эффективно справиться клиентские приложения. Кроме того, при необходимости можно применить следующие рекомендации.

> [AZURE.NOTE] Этот список рекомендаций в основном касается приема новых данных, а не изменения существующих данных, хранящихся в индексе. В Elasticsearch рабочие нагрузки приема выполняются как операции добавления, а изменение данных — как операции добавления и удаления. Это обусловлено тем, что документы в индексе неизменяемы, поэтому при изменении весь документ заменяется новой версией. Можно выполнить HTTP-запрос PUT для перезаписи существующего документа или воспользоваться API *обновлениями* Elasticsearch, который абстрагирует запрос для получения существующего документа, объединяет изменения, а затем выполняет запрос PUT для сохранения нового документа.

* Отключите анализ текста для полей индекса, которые не нужно анализировать. Анализ предполагает разбивку текста на лексемы с помощью маркеров для выполнения запросов на поиск конкретных терминов. Тем не менее эта задача может сильно нагружать ЦП, поэтому выполняйте ее с осторожностью. Если Elasticsearch используется для хранения данных журнала, может оказаться полезным разбить подробные сообщения журнала на лексемы для обеспечения сложного поиска. Другие поля, например поля с кодами ошибок или идентификаторами, вероятно, не следует разбивать на лексемы. (Вряд ли часто понадобится запрашивать сведения всех запросов, код ошибки которых содержит цифру 3.) Следующий код отключает анализ полей *name* и *hostip* в индексе *systembase* типа *logs*:

	```http
	PUT /systembase
	{
		"settings" : {
			...
		},
		"logs" : {
			...
			"name": {
				"type": "string",
				"index" : "not_analyzed"
			},
			"hostip": {
				"type": "string",
				"index" : "not_analyzed"
			},
			...
		}
	}
	```

* Отключите поле *\_all* индекса, если оно не является обязательным. Поле *\_all* сцепляет значения других полей в документе для анализа и индексирования. Это полезно для выполнения запросов, которые могут выполнять сопоставление с любым полем в документе. Если ожидается, что клиенты будут выполнять сопоставление с именованными полями, включение поля *\_all* приведет к увеличению нагрузки на ЦП и хранилище. В следующем примере показано, как отключить поле *\_all* для типа *logs* в индексе *systembase*.

	```http
	PUT /systembase
	{
		"settings" : {
			...
		},
		"logs" : {
			"_all": {
				"enabled" : false
			},
			...,
		...
		}
	}
	```

    Обратите внимание, что можно создать выборочную версию поля *\_all*, которая содержит сведения только из определенных полей. Дополнительные сведения см. в разделе [Disabling the \_all Field](https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-all-field.html#disabling-all-field) (Отключение поля \_all).

* Не выполняйте динамическое сопоставление в индексах. Динамическое сопоставление — эффективная функция. Однако при добавлении новых полей в существующий индекс требуется координировать изменения в структуре индекса на узлах, из-за чего индекс может быть временно заблокирован. Если не выполнять динамическое сопоставление с особой тщательностью, это может также привести к резкому росту количества полей и последующему увеличению объема метаданных индекса. Это в свою очередь приведет к увеличению требований к хранилищу и повышению числа операций ввода-вывода как при приеме данных, так и при выполнении запросов. Обе эти проблемы будут влиять на производительность. Рекомендуется отключить динамическое сопоставление и явно определить структуру индекса. Дополнительные сведения см. в статье [Dynamic Field Mapping](https://www.elastic.co/guide/en/elasticsearch/reference/current/dynamic-field-mapping.html#dynamic-field-mapping) (Динамическое сопоставление полей).

* Выясните способ распределения рабочей нагрузки в соответствии с конфликтующими требованиями. Всегда следует учитывать, что прием данных может существенно влиять на производительность других параллельных операций, например выполнение запросов. При приеме данных может наблюдаться резкое увеличение нагрузки, и, если система пытается немедленно использовать все поступающие данные, приток данных может привести к значительному сокращению скорости выполнения запросов. Во избежание этого Elasticsearch регулирует скорость обработки запросов на прием с помощью очереди массовой вставки (дополнительные сведения см. в разделе [Определение ограничивающих факторов. Загрузка ЦП](#determining-limiting-factors-cpu-utilization)). Однако этот механизм следует рассматривать только в качестве последнего средства. Если код приложения не подготовлен для обработки отклоненных запросов, появляется риск потери данных. Вместо этого рекомендуется использовать шаблон, например [выравнивание нагрузки на основе очереди](https://msdn.microsoft.com/library/dn589783.aspx), для управления скоростью передачи данных в Elasticsearch.

* Убедитесь, что в кластере достаточно ресурсов для обработки рабочей нагрузки, особенно если для индексов настроено несколько реплик.

* Используйте API массовой вставки для передачи больших пакетов документов. Устанавливайте размер массовых запросов соответствующим образом. Иногда большие пакеты ухудшают производительность и могут привести к перегруженности потоков и других ресурсов Elasticsearch, что в свою очередь приводит к задержке выполнения других параллельных операций. При выполнении операции документы в пакете массовой вставки хранятся в памяти на координирующем узле. Физический размер каждого пакета имеет более важное значение, чем количество документов. Не существует непреложных правил относительно идеального размера пакета. Однако в документации Elasticsearch в качестве отправной точки для собственного исследования рекомендуется использовать от 5 до 15 МБ. Проведите тестирование производительности, чтобы установить оптимальный размер пакета для собственных сценариев и рабочих нагрузок.

* Убедитесь, что запросы массовой вставки распределяются по узлам, а не направляются на один узел. В противном случае это может привести к нехватке памяти, так как каждый обрабатываемый запрос на массовую вставку хранится в памяти на узле. Кроме того, может увеличиться задержка в сети, так как запросы перенаправляются на другие узлы.

* При записи данных Elasticsearch использует кворум, состоящий из большинства основных узлов и узлов реплики. Операция записи не завершится, пока кворум не сообщит об успешном выполнении. Такой подход позволяет гарантировать, что данные не записываются, если большинство узлов недоступны из-за нарушения связности сети (сбой). При использовании кворума эффективность выполнения операций записи может снизиться. Запись с использованием кворума можно отключить, установив для параметра *consistency* значение *one* при записи данных. В следующем примере добавляется новый документ, но выполнение операции завершается сразу же после завершения записи в основной сегмент.

	```http
	PUT /my_index/my_data/104?consistency=one
	{
		"name": "Bert",
		"age": 23
	}
	```

	Обратите внимание, что, подобно асинхронной репликации, отключение записи с использованием кворума может привести к несогласованности между основным сегментом и каждой из реплик.

* Если при использовании кворумов узлов недостаточно, Elasticsearch будет ожидать, прежде чем определить, что операцию записи нужно отменить из-за того, что невозможно получить доступ к кворуму. Этот период ожидания определяется параметром запроса timeout (значение по умолчанию — 1 минута). Его можно изменить, использовав параметр запроса timeout. В приведенном ниже примере создается документ и до прерывания операции ответ кворума ожидается в течение 5 секунд:

	```http
	PUT /my_index/my_data/104?timeout=5s
	{
		"name": "Sid",
		"age": 27
	}
	```

	В Elasticsearch также можно использовать собственные номера версий, [созданные с помощью внешних средств](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-index_.html#_version_types).

* Рекомендуется отключить поле индекса *\_source*. Это поле содержит копию исходного документа JSON, использованную при сохранении документа. Если сохранить это поле, это приведет к дополнительным затратам на хранение и увеличению числа операций ввода-вывода на диске. Тем не менее в зависимости от структуры документа эти затраты могут быть незначительными. Кроме того, следует иметь в виду, что в случае отключения поля *\_source* клиент не сможет выполнять следующие операции:

	* изменение документа с помощью API обновления;
	* выделение в режиме реального времени при выполнении запросов;
	* повторное индексирование данных;
	* отладка запросов и агрегирований путем просмотра исходного документа.

	В следующем примере отключается поле *\_source* для типа *logs* в индексе *systembase*.

  ```http
  PUT /systembase
  {
		"settings" : {
			...
		},
		"logs" : {
			"_source": {
				"enabled": false
			},
			...,
		...
		}
  }
  ```

## Общие рекомендации по проведению тестирования производительности приема данных с помощью Elasticsearch

Далее приведены рекомендации, которые следует учесть при выполнении тестов производительности с помощью Elasticsearch и анализе результатов.

* Тестирование производительности всегда отнимает много времени и средств. Поэтому по возможности следует по крайней мере собрать статистику с измерением скорости передачи на диске и в сети, загрузки ЦП, времени ожидания ЦП и задержки диска. Таким образом, вы быстро получите результаты тестирования и сможете возвратить вложенные средства.

* Используйте любые возможности написания сценариев, предоставляемые в средстве тестирования нагрузки, для сбора метрик, которые невозможно получить другим образом. Например, Linux предоставляет множество надежных и эффективных статистических данных по производительности, которые можно собрать с помощью служебных программ, таких как *vmstat* и *iostat*. Для сбора этих данных в рамках плана тестирования можно создавать сценарии с помощью JMeter.

* При управлении производительностью выполняется анализ статистики на основе надежных повторяемых данных. Не рассматривайте одни лишь общие показатели, так как с их помощью невозможно получить нужное аналитическое представление. Анализируйте данные, выполняйте управление производительностью в ходе разработки и мгновенно реагируйте на возникающие проблемы. Всегда просматривайте статистику и сравнивайте тенденции с прошлыми результатами и конфигурациями. Если делать это регулярно, вы сможете создавать нужные данные, которые можно применить к рабочим нагрузкам и использовать для оценки влияния изменений в конфигурации и развертывании.

* Чтобы получить дополнительные сведения, используйте при тестировании средство для мониторинга производительности кластера и узлов, например Marvel. JMeter подходит для записи необработанных данных для последующего анализа. Однако с помощью Marvel можно получить сведения о производительности в режиме реального времени и узнать причины возникновения ошибок и задержек. Кроме того, многие средства проверки нагрузки не позволяют просматривать внутренние метрики Elasticsearch. Применяйте и сравнивайте показатели пропускной способности индексирования, счетчиков операций слияния сегментов, статистики сборки мусора и времени регулирования, доступные в статистике по индексу. Выполняйте этот анализ регулярно.

* Сравнивайте статистику средств проверки нагрузки со статистикой узла в Marvel (трафик диска и сетевой трафик, загрузка ЦП, использование памяти и пула потоков), чтобы понять, как показатели, предоставляемые инфраструктурой, и определенные статистические данные Elasticsearch коррелируют между собой.

* Как правило, для тестирования производительности рекомендуется использовать базовую конфигурацию *один узел на один сегмент*, а затраты на приложения оценивать путем добавления узлов. Тем не менее не следует полностью полагаться на экстраполяцию производительности по нескольким узлам и сегментам. Чем больше количество узлов и сегментов, тем неожиданнее влияние на затраты, связанные с синхронизацией и обменом данными в кластере.

* Для сравнения статистических данных просмотрите распределение сегментов по узлам. Некоторые узлы будут содержать меньше реплик и сегментов, что приведет к неравномерному использованию ресурсов.

* При выполнении нагрузочного тестирования увеличьте число потоков, используемых средством проверки для отправки данных кластера до возникновения ошибок. Для тестирования устойчивой пропускной способности рекомендуется не превышать согласованное *пороговое значение*. Если частота ошибок превышает пороговое значение, из-за ошибок потребуется восстанавливать внутренние ресурсы, что приведет к дополнительным затратам. В таких случаях пропускная способность будет неизбежно снижаться.

* Чтобы сымитировать поведение системы при неожиданном скачке активности, рекомендуется выполнять тесты, позволяющие смоделировать частоту ошибок при достижении порогового значения. В результате вы получите показатели пропускной способности, которые позволят определить не только объем необходимых ресурсов, но и стоимость восстановления.

* Оценивайте профиль производительности по числу документов и удаляйте документы в соответствии с характером рабочих нагрузок. Учтите, что при добавлении дополнительных документов профиль производительности может измениться.

* Учитывайте соглашения об уровне обслуживания касательно количества операций ввода-вывода в секунду и ограничения скорости передачи для используемого хранилища. Скорость передачи зависит от типа хранилища (SSD, вращающийся носитель).

* Помните, что производительность ЦП может ухудшиться не только из-за активности на диске и в сети, но и из-за того, что при распределенной обработке серверные приложения могут использовать механизмы блокировки и обмена данными, что может привести к неэффективному использованию процессора.

* Выполняйте тесты производительности в течение по крайней мере двух часов (а не нескольких минут). Влияние индексирования на производительность проявляется не сразу. Например, статистика сборки мусора виртуальной машины Java и слияние индексов может постепенно изменить профиль производительности.

* Обратите внимание на то, насколько сильно обновление индекса может повлиять на пропускную способность приема данных и регулирование в кластере.

## Сводка

Важно понимать, как масштабировать решение с увеличением объема данных и числа запросов. Сервер Elasticsearch, выполняемый в среде Azure, позволяет выполнять вертикальное и горизонтальное масштабирование. Можно использовать виртуальные машины большого размера с дополнительными ресурсами, а также распределить ресурсы кластера Elasticsearch по сети виртуальных машин. Диапазон вариантов может быть бесконечным. Что лучше с точки зрения расходов — развернуть кластер на множестве виртуальных машин небольшого размера, в кластере с несколькими виртуальными машинами большого размера или выбрать компромиссное решение? Кроме того, сколько сегментов должен содержать каждый индекс и какое негативное влияние может оказать прием данных на производительность запросов? Способ распределения сегментов между узлами может оказывать существенное влияние на пропускную способность приема данных. Добавление сегментов может сократить количество внутренних конфликтов, возникающих в сегменте. Однако при этом следует также учитывать дополнительную нагрузку на кластер, возникающую при использовании большого количества сегментов. Чтобы получить исчерпывающие ответы на эти вопросы и определить наиболее подходящую стратегию, следует протестировать систему.

Для рабочих нагрузок приема данных производительность подсистемы ввода-вывода диска является ключевым фактором. Твердотельные накопители позволяют повысить производительность за счет снижения задержки при выполнении операций записи на диск. Если не требуется большой объем дискового пространства на узле, рекомендуется использовать стандартные виртуальные машины с временным хранилищем вместо более дорогостоящих виртуальных машин с поддержкой хранилища класса Premium.

## Приложение. Тест производительности приема данных при массовой загрузке

В этом приложении описан тест производительности, выполненный для кластера Elasticsearch. Тесты выполнены с помощью средства JMeter на отдельных виртуальных машинах. Подробные сведения о конфигурации тестовой среды описаны в документе How-To: Create a Performance Testing Environment for Elasticsearch (Практическое руководство. Создание среды тестирования производительности для Elasticsearch). Чтобы провести тест, можно создать план тестирования JMeter вручную или использовать автоматические сценарии тестов, которые доступны отдельно. Дополнительные сведения см. в документе How-To: Run the Automated Elasticsearch Ingestion Tests (Практическое руководство. Запуск автоматизированных тестов приема Elasticsearch).

Рабочая нагрузка приема данных выполнила передачу большого количества документов с помощью API массовой вставки. С помощью этого индекса нужно было сымитировать получение данных журнала репозиторием, что представляет собой системные события для последующего анализа и поиска. Каждый документ сохранен в одном индексе с именем *systembase* типа *logs*. Все документы имели фиксированную схему, описанную в следующей таблице:

| Поле | Тип данных | Пример |
|---------------|---------------------|-----------------------------------|
| @timestamp | datetime; | 2013-12-11T08:01:45.000Z |
| name | string | checkout.payment |
| message | string | Входящее сообщение запроса |
| severityCode | целое число | 1 |
| severity | string | info |
| hostname | string | sixshot |
| hostip | строка (IP-адрес) | 10\.0.0.4 |
| pid | int | 123 |
| tid | int | 4325 |
| appId | строка (UUID) | {00000000-0000-0000-000000000000} |
| appName | string | mytestapp |
| appVersion | string | 0\.1.0.1234 |
| type | int | 5 |
| subtype | int | 1 |
| correlationId | guid | {00000000-0000-0000-000000000000} |
| ОС | string | Linux |
| osVersion | string | 4\.1.1 |
| parameters | [ ] | {ключ:значение,ключ:значение} |

Следующий запрос можно использовать для создания индекса. Во многих тестах значения параметров *number\_of\_replicas*, *refresh\_interval* и *number\_of\_shards* отличались от представленных ниже.

> [AZURE.IMPORTANT] Каждый раз перед выполнением теста индекс удалялся и создавался заново.

```http
PUT /systembase
{
	"settings" : {
		"number_of_replicas": 1,
		"refresh_interval": "30s",
		"number_of_shards": "5"
	},
	"logs" : {
		"properties" : {
			"@timestamp": {
			"type": "date",
			"index" : "not_analyzed"
			},
			"name": {
				"type": "string",
				"index" : "not_analyzed"
			},
			"message": {
				"type": "string",
				"index" : "not_analyzed"
			},
			"severityCode": {
				"type": "integer",
				"index" : "not_analyzed"
			},
			"severity": {
				"type": "string",
				"index" : "not_analyzed"
			},
			"hostname": {
				"type": "string",
				"index" : "not_analyzed"
			},
			"hostip": {
				"type": "string",
				"index" : "not_analyzed"
			},
			"pid": {
				"type": "integer",
				"index" : "not_analyzed"
			},
			"tid": {
				"type": "integer",
				"index" : "not_analyzed"
			},
			"appId": {
				"type": "string",
				"index" : "not_analyzed"
			},
			"appName": {
				"type": "string",
				"index" : "not_analyzed"
			},
			"appVersion": {
				"type": "integer",
				"index" : "not_analyzed"
			},
			"type": {
				"type": "integer",
				"index" : "not_analyzed"
			},
			"subtype": {
				"type": "integer",
				"index" : "not_analyzed"
			},
			"correlationId": {
				"type": "string",
				"index" : "not_analyzed"
			},
			"os": {
				"type": "string",
				"index" : "not_analyzed"
			},
			"osVersion": {
				"type": "string",
				"index" : "not_analyzed"
			},
			"parameters": {
				"type": "string",     
				"index" : "not_analyzed"
			}
		}
	}
}
```

Каждый пакет массовой вставки содержал 1000 документов. При создании каждого документа для полей *severityCode*, *hostname*, *hostip*, *pid*, *tid*, *appName*, *appVersion*, *type*, *subtype* и *correlationId* устанавливались случайные значения в разных сочетаниях, а также выбирался произвольный текст из фиксированного набора значений для полей *name*, *message*, *severity*, *os*, *osVersion*, *parameters*, *data1* и *data2*. Количество экземпляров клиентских приложений для передачи данных было тщательно подобрано, чтобы максимально увеличить количество успешных циклов передачи. Тесты выполнялись в течение двух часов, чтобы обеспечить переход кластера к нормальному режиму работы и уменьшить влияние любых временных сбоев на общие результаты. За это время в ходе некоторых тестов передано около 1,5 миллиардов документов.

С помощью дискретизатора запросов JUnit выполнено динамическое создание данных. Он добавлен в группу потоков в плане тестирования JMeter. В Eclipse IDE с помощью шаблона тестовых случаев JUnit создан код JUnit.

> [AZURE.NOTE] Дополнительные сведения о создании теста JUnit для JMeter см. в статье How-To: Create and Deploy a JMeter JUnit Sampler for Testing Elasticsearch Performance (Практическое руководство. Создание и развертывание дискретизатора JUnit JMeter для тестирования производительности Elasticsearch).

В следующем фрагменте показан код Java для тестирования Elasticsearch 1.7.3. Обратите внимание, что в этом примере класс теста JUnit называется *ElasticSearchLoadTest2*:

```java
/* Java */
package elasticsearchtest2;

	import static org.junit.Assert.*;

	import org.junit.*;

	import java.util.*;

	import java.io.*;

	import org.elasticsearch.action.bulk.*;
	import org.elasticsearch.common.transport.*;
	import org.elasticsearch.client.transport.*;
	import org.elasticsearch.common.settings.*;
	import org.elasticsearch.common.xcontent.*;

	public class ElasticSearchLoadTest2 {

		private String [] names={"checkout","order","search","payment"};
		private String [] messages={"Incoming request from code","incoming operation succeeded with code","Operation completed time","transaction performed"};
		private String [] severity={"info","warning","transaction","verbose"};
		private String [] apps={"4D24BD62-20BF-4D74-B6DC-31313ABADB82","5D24BD62-20BF-4D74-B6DC-31313ABADB82","6D24BD62-20BF-4D74-B6DC-31313ABADB82","7D24BD62-20BF-4D74-B6DC-31313ABADB82"};

		private String hostname = "";
		private String indexstr = "";
		private String typestr = "";
		private int port = 0;
		private int itemsPerInsert = 0;
		private String clustername = "";
		private static Random rand=new Random();

		@Before
		public void setUp() throws Exception {
		}

		public ElasticSearchLoadTest2(String paras) {
		* Paras is a string containing a set of comma separated values for:
			hostname
			indexstr
			typestr
			port
			clustername
			node
			itemsPerInsert
		*/

			// Note: No checking/validation is performed

			String delims = "[ ]*,[ ]*"; // comma surrounded by zero or more spaces
			String[] items = paras.split(delims);

			hostname = items[0];
			indexstr = items[1];
			typestr = items[2];
			port = Integer.parseInt(items[3]);
			clustername = items[4];
			itemsPerInsert = Integer.parseInt(items[5]);

			if (itemsPerInsert == 0)
				itemsPerInsert = 1000;
			}

		@After
		public void tearDown() throws Exception {
		}

		@Test
		public void BulkBigInsertTest() throws IOException {

			Settings settings = ImmutableSettings.settingsBuilder().put("cluster.name", clustername).build();

			TransportClient client;
			client = new TransportClient(settings);

			try {
				client.addTransportAddress(new InetSocketTransportAddress(hostname, port));
				BulkRequestBuilder bulkRequest = client.prepareBulk();
				Random random = new Random();
				char[] exmarks = new char[12000];
				Arrays.fill(exmarks, 'x');
				String dataString = new String(exmarks);

				for(int i=1; i &lt; itemsPerInsert; i++){
					random.nextInt(10);
					int host=random.nextInt(20);

					bulkRequest.add(client.prepareIndex(indexstr, typestr).setSource(XContentFactory.jsonBuilder().startObject()
						.field("@timestamp", new Date())
						.field("name", names[random.nextInt(names.length)])
						.field("message", messages[random.nextInt(messages.length)])
						.field("severityCode", random.nextInt(10))
						.field("severity", severity[random.nextInt(severity.length)])
						.field("hostname", "Hostname"+host)
						.field("hostip", "10.1.0."+host)
						.field("pid",random.nextInt(10))
						.field("tid",random.nextInt(10))
						.field("appId", apps[random.nextInt(apps.length)])
						.field("appName", "application" + host)
						.field("appVersion", random.nextInt(5))
						.field("type", random.nextInt(6))
						.field("subtype", random.nextInt(6))
						.field("correlationId", UUID.randomUUID().toString())
						.field("os", "linux")
						.field("osVersion", "14.1.5")
						.field("parameters", "{key:value,key:value}")
						.field("data1",dataString)
						.field("data2",dataString)
					.endObject()));
				}

				BulkResponse bulkResponse = bulkRequest.execute().actionGet();
				assertFalse(bulkResponse.hasFailures());
			}
			finally {
				client.close();
			}
		}

		@Test
		public void BulkDataInsertTest() throws IOException {
			Settings settings = ImmutableSettings.settingsBuilder().put("cluster.name", clustername).build();

			TransportClient client;
			client = new TransportClient(settings);

			try {
				client.addTransportAddress(new InetSocketTransportAddress(hostname, port));
				BulkRequestBuilder bulkRequest = client.prepareBulk();

				for(int i=1; i&lt; itemsPerInsert; i++){
					rand.nextInt(10);
					int host=rand.nextInt(20);

					bulkRequest.add(client.prepareIndex(indexstr, typestr).setSource(XContentFactory.jsonBuilder().startObject()
						.field("@timestamp", new Date())
						.field("name", names[rand.nextInt(names.length)])
						.field("message", messages[rand.nextInt(messages.length)])
						.field("severityCode", rand.nextInt(10))
						.field("severity", severity[rand.nextInt(severity.length)])
						.field("hostname", "Hostname" + host)
						.field("hostip", "10.1.0."+host)
						.field("pid",rand.nextInt(10))
						.field("tid",rand.nextInt(10))
						.field("appId", apps[rand.nextInt(apps.length)])
						.field("appName", "application"+host)
						.field("appVersion", rand.nextInt(5))
						.field("type", rand.nextInt(6))
						.field("subtype", rand.nextInt(6))
						.field("correlationId", UUID.randomUUID().toString())
						.field("os", "linux")
						.field("osVersion", "14.1.5")
						.field("parameters", "{key:value,key:value}")
					.endObject()));
				}

				BulkResponse bulkResponse = bulkRequest.execute().actionGet();
				assertFalse(bulkResponse.hasFailures());
			}
			finally {
				client.close();
			}
		}
	}
```

Закрытые массивы параметров *String* (*names*, *messages*, *severity* и *apps*) содержат небольшое количество значений, показатели которых выбираются случайным образом. Остальные данные создаются для каждого документа во время выполнения.

Конструктор, получающий параметр *String*, вызывается средством JMeter, а значения, передаваемые строке, указываются в ходе конфигурации дискретизатора запросов JUnit. В этом тесте JUnit предполагается, что параметр *String* должен содержать параметры, как указано ниже.

* **Hostname**. Это имя или IP-адрес балансировщика нагрузки Azure. Балансировщик нагрузки пытается распределить запросы по узлам данных в кластере. Если балансировщик нагрузки не используется, адрес узла можно указать в кластере. Однако в этом случае все запросы будут направляться на этот узел, который, как результат, может стать узким местом.

* **Indexstr**. Это имя индекса, в который добавляются данные, созданные при выполнении теста JUnit. Если индекс создан согласно указаниям выше, для этого параметра должно быть установлено значение *systembase*.

* **Typestr**. Это тип индекса, в котором хранятся данные. Если индекс создан согласно указаниям выше, для этого параметра должно быть установлено значение *logs*.

* **Port**. Это порт, к которому нужно подключиться на узле. В большинстве случаев для него устанавливается значение 9300. Это порт, используемый Elasticsearch для прослушивания запросов API клиента. Порт 9200 используется только для HTTP-запросов.

* **Clustername**. Это имя кластера Elasticsearch, содержащего индекс.

* **ItemsPerInsert**. Это числовой параметр, указывающий число документов, которые нужно добавлять в каждый пакет массовой вставки. Размер пакета по умолчанию — 1000 документов.

Данные для конструктора указываются на странице JUnit Request (Запрос JUnit), на которой можно настроить дискретизатор JUnit в JMeter. Пример приведен на следующем рисунке:

![](media/guidance-elasticsearch-data-ingestion-image22.png)

Методы *BulkInsertTest* и *BigBulkInsertTest* фактически выполняют создание и передачу данных. Эти методы очень похожи. Они подключаются к кластеру Elasticsearch, а затем создают пакет документов (их количество определяется параметром строки конструктора *ItemsPerInsert*). Документы добавляются в индекс с помощью API массовых операций Elasticsearch. Эти методы отличаются тем, что метод *BulkInsertTest* исключает строковые поля *data1* и *data2* в каждом документе при передаче данных, а метод *BigBulkInsertTest* заполняет эти поля строками длиной 12 000 символов. Обратите внимание, что выбрать один из этих методов можно на странице JUnit Request (Запрос JUnit) в JMeter в поле *Test Method* (Метод тестирования) (выделено на предыдущем рисунке).

> [AZURE.NOTE] В образце кода, указанном здесь, используется библиотека транспортного клиента Elasticsearch 1.7.3. Если используется Elasticsearch 2.0.0 или более поздней версии, необходимо использовать библиотеку, соответствующую выбранной версии. Дополнительные сведения о библиотеке транспортного клиента Elasticsearch 2.0.0 см. на странице [Transport Client](https://www.elastic.co/guide/en/elasticsearch/client/java-api/2.0/transport-client.html) (Транспортный клиент) на веб-сайте Elasticsearch.

<!---HONumber=AcomDC_0211_2016-->