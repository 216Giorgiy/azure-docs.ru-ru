<properties
   pageTitle="Использование ElasticSearch в Azure | Microsoft Azure"
   description="Как установить, настроить и запустить Elasticsearch в Azure."
   services=""
   documentationCenter="na"
   authors="mabsimms"
   manager="marksou"
   editor=""
   tags=""/>

<tags
   ms.service="guidance"
   ms.devlang="na"
   ms.topic="article"
   ms.tgt_pltfrm="na"
   ms.workload="na"
   ms.date="02/05/2016"
   ms.author="mabsimms"/>

# Работа Elasticsearch в Azure

Этот материал входит в [цикл статей, посвященных вопросам Elasticsearch](guidance-elasticsearch-introduction.md).

## Обзор

Elasticsearch — это высокомасштабируемая поисковая система и база данных с открытым кодом. Ее можно использовать в ситуациях, требующих быстрого анализа и обнаружения информации, хранящейся в больших наборах данных. Ниже приведены распространенные сценарии.

- Поиск в произвольном тексте большого объема, в котором можно быстро находить документы и сочетания, соответствующие условиям поиска, и извлекать их.
- Ведение журналов событий, в которых может регистрироваться информация, поступающая из различных источников. Необходимо проанализировать данные, чтобы выяснить, как та или иная последовательность событий привела к конкретному результату.
- Хранение данных, собранных с удаленных устройств и из других источников. Данные могут содержать различную информацию, но ее в любом случае необходимо представить на группе панелей мониторинга, чтобы оператор мог определить состояние всей системы. Приложения также могут использовать информацию для быстрого принятия решений о потоке данных и бизнес-операциях, которые должны быть выполнены в результате.
- Контроль запасов, при котором регистрируются изменения данных о наличии по мере продажи товаров. Бизнес-системы могут использовать эту информацию, чтобы сообщать пользователям об уровне запасов и пополнять запасы с помощью повторного заказа, если товара осталось мало. Аналитики могут изучать данные о тенденциях, чтобы определить, какие товары лучше продаются и при каких обстоятельствах.
- Финансовый анализ, когда информация о рынке поступает практически в режиме реального времени. Могут быть созданы панели мониторинга, отображающие актуальную производительность различных финансовых инструментов, которые можно использовать для принятия решений о покупке или продаже.

В этом документе приводится краткое введение в общую структуру Elasticsearch, а также объясняется, как реализовать кластер Elasticsearch с помощью Azure. Документ содержит рекомендации по развертыванию кластера Elasticsearch, в которых основное внимание уделяется функциональности и требованиям к управлению вашей системой. Также из этого документа вы узнаете, как выбрать конфигурацию и топологию в соответствии с определенными требованиями.

> [AZURE.NOTE] В этом руководстве предполагается, что у вас есть основные навыки работы с [Elasticsearch][].

## Структура Elasticsearch

Elasticsearch — это база данных документов, оптимизированная для работы в качестве поисковой системы. Документы сериализуются в формате JSON. Данные хранятся в индексах, реализованных с помощью библиотеки [Apache Lucene][]. При этом данные извлекаются из представления, и для использования Elasticsearch нет необходимости изучать Lucene.

### Кластеры, узлы, индексы и сегменты

Elasticsearch реализует кластеризованную архитектуру, которая использует сегментирование для распределения данных по нескольким узлам и репликацию для обеспечения высокой доступности.

Документы хранятся в индексах. Пользователь может указать, какие поля в документе используются для однозначной идентификации документа в индексе, или система может создать ключевое поле и значения автоматически. Индекс используется для физического упорядочивания документов и применяется в качестве основного средства поиска документов. Кроме того, Elasticsearch автоматически создает набор дополнительных структур, которые выступают в качестве инвертированных индексов для оставшихся полей. Такая структура обеспечивает быстрый просмотр и анализ данных в пределах коллекции.

Индекс состоит из набора сегментов. Документы равномерно распределяются по сегментам с помощью механизма хэширования, который основан на значениях ключа индекса и количестве сегментов в индексе. После того как документ помещается в сегмент, он не будет перемещен из этого сегмента, пока не поменяется ключ индекса. Система Elasticsearch распределяет сегменты по всем доступным узлам данных в кластере. Один узел изначально может хранить один или несколько сегментов, принадлежащих к одному индексу. Но, по мере того как в кластер добавляются новые узлы, Elasticsearch перемещает сегменты, чтобы обеспечить сбалансированную нагрузку в системе. Такое же перераспределение применяется при удалении узлов.

Индексы могут реплицироваться. В этом случае копируется каждый сегмент в индексе. Elasticsearch следит за тем, чтобы каждый исходный сегмент индекса (так называемый "первичный сегмент") и его реплика всегда находились на разных узлах.

> [AZURE.NOTE] Количество сегментов в индексе нельзя легко изменить после создания индекса, но в него можно добавлять реплики.

Когда добавляется или изменяется документ, все операции записи выполняются сначала в первичном сегменте, а затем в каждой реплике. По умолчанию этот процесс выполняется синхронно для обеспечения согласованности. Во время записи данных Elasticsearch использует оптимистичный параллелизм с управлением версиями. Операции чтения могут выполняться с помощью первичного сегмента или любой из его реплик.

На рисунке 1 показаны основные аспекты кластера Elasticsearch с тремя узлами. Создан индекс, состоящий из двух первичных сегментов с двумя репликами для каждого сегмента (всего шесть сегментов).

![](media/guidance-elasticsearch-general-cluster1.png)

**Рисунок 1.**
Простой кластер Elasticsearch, включающий в себя два первичных узла и два набора реплик

В этом кластере первичный сегмент 1 и первичный сегмент 2 расположены на отдельных узлах для балансировки нагрузки между ними. Точно так же распределяются реплики. При сбое одного узла остальные узлы содержат достаточно информации, чтобы позволить системе продолжать работу. При необходимости Elasticsearch повысит уровень реплики сегмента, и она станет первичным сегментом, если соответствующий первичный сегмент недоступен.
Когда запускается узел, он может инициировать новый кластер (если это первый узел в кластере) или присоединиться к существующему кластеру. Чтобы определить, к какому кластеру принадлежит узел, см. параметр *cluster.name* в файле elasticsearch.yml.

### Роли узлов

Узлы в кластере Elasticsearch могут выполнять следующие роли.

- Узел данных, в котором может храниться один или несколько сегментов, содержащих данные индекса.
- Узел клиента, который не хранит данные индекса, но обрабатывает входящие запросы, которые клиентские приложения отправляют на соответствующий узел данных.
- Главный узел, который не хранит данные индекса, но выполняет операции управления кластером, например хранит и распределяет информацию о маршрутизации в кластере (список узлов с сегментами, которые они содержат), определяет доступность узлов, перемещает сегменты по мере появления и исчезновения узлов и координирует восстановление после сбоя узла. Несколько узлов можно настроить как главные, но только один из них будет назначен для выполнения функций главного узла. В случае сбоя такого узла для выполнения этих функций назначается другой подходящий узел.

По умолчанию узлы Elasticsearch выполняют все три роли (чтобы вы могли собрать полностью работающий кластер для разработки и тестирования на одном компьютере). Но вы можете изменить операции узлов с помощью параметров *node.data* и *node.master* в файле *elasticsearch.yml*, как показано ниже.

```yaml
node.data: true
node.master: false
```

**Конфигурация для узла данных**

```yaml
node.data: false
node.master: false
```

**Конфигурация для клиентского узла**

```yaml
node.data: false
node.master: true
```

**Конфигурация для главного узла**

> [AZURE.NOTE] Назначенный главный узел имеет большое значение для правильной работы кластера. Другие узлы регулярно проверяют связь с ним, чтобы убедиться, что он доступен. Если назначенный главный узел также выполняет роль узла данных, существует вероятность того, что он может быть занят и не сможет ответить на проверку связи. В этом случае он ошибочно считается неработающим, и его функции начинает выполнять другой главный узел. Если исходный главный узел на самом деле по-прежнему работает, в результате получится кластер с двумя главными узлами, что приведет к проблеме "разделения вычислительных мощностей", которая может вызвать повреждение данных и другие проблемы. В документе Configuring, Testing, and Analyzing Elasticsearch Resilience and Recovery (Настройка, тестирование и анализ устойчивости и восстановления Elasticsearch) описывается, как настроить кластер, чтобы снизить вероятность такого сценария. Но лучше всего в работе со средними и большими кластерами использовать выделенные главные узлы, которые не отвечают за управление данными.

Узлы в кластере совместно используют информацию о других узлах в кластере (с помощью [обмена сведениями][]) и информацию о том, какие сегменты содержат другие узлы. Клиентские приложения, которые хранят и получают данные, могут подключаться к любому узлу в кластере, и запросы будут прозрачно перенаправлены на правильный узел. Когда клиентское приложение запрашивает данные из кластера, узел, который первым получает запрос, отвечает за руководство операцией. Он связывается с каждым узлом, из которого можно получить данные, а затем выполняет статистическое вычисление результата, прежде чем вернуть его клиентскому приложению. Использование клиентских узлов для обработки запросов освобождает узлы данных от выполнения этой задачи по распределению и сборке и позволяет им заниматься обслуживанием данных. Вы можете предотвратить случайное подключение клиентских приложений к узлам данных (что может заставить их выполнять роль клиентских узлов). Просто отключите транспорт HTTP для узлов данных:

```yaml
http.enabled: false
```

Узлы данных могут связываться с другими узлами данных, клиентскими узлами и выделенными главными узлами в одной сети с помощью модуля транспорта Elasticsearch (который использует сокеты TCP для подключения непосредственно между узлами), но клиентские приложения могут подключаться к клиентским узлам только по протоколу HTTP. На рисунке 2 показана топология, включающая в себя сочетание выделенного главного узла, клиентского узла и узла данных в кластере Elasticsearch.

![](media/guidance-elasticsearch-general-cluster2.png)

**Рис. 2.**
Кластер Elasticsearch с различными типами узлов

### Стоимость и преимущества использования клиентских узлов

Когда приложение отправляет запрос в кластер Elasticsearch, узел, к которому подключается приложение, отвечает за руководство обработкой запроса. Узел перенаправляет запрос на каждый узел данных и собирает результаты, а затем возвращает накопленную информацию приложению. Если запрос использует статистическую обработку и другие вычисления, узел, к которому подключается приложение, выполняет необходимые операции после извлечения данных из всех остальных узлов. Этот процесс распределения и сборки может потреблять значительные ресурсы памяти и обработки.

Использование выделенных клиентских узлов для выполнения этих задач позволяет узлам данных заниматься управлением и хранением данных. Поэтому выделенные клиентские узлы могут быть очень полезны во многих сценариях, в которых используются сложные запросы и статическая обработка. Тем не менее эффективность выделенных клиентских узлов зависит от конкретного сценария, рабочей нагрузки и размера кластера. Например, использование клиентских узлов для рабочих нагрузок по приему данных может быть менее эффективным из-за дополнительных сетевых переходов, которые являются обязательными при сохранении данных. Допустим, в системе не настроены выделенные клиентские узлы. При равных факторах среды и нагрузках на узлы в кластере с тремя узлами и шестью сегментами есть вероятность 1/3, что приложение, которое хранит и изменяет данные, будет подключаться напрямую к самому подходящему сегменту. Таким образом, в 1/3 случаев не нужно выполнять дополнительный сетевой переход. С другой стороны, в рабочих нагрузках, которые выполняют статистическую обработку, может быть выгодно использовать выделенные клиентские узлы в качестве одного узла, который будет отвечать за каждый набор операций распределения и сборки, который выполняют операции статической обработки. Вы должны быть готовы к запуску тестов производительности в смешанной среде, чтобы оценить влияние использования клиентских узлов на конкретные рабочие нагрузки.

> [AZURE.NOTE] Документ Maximizing Data Aggregation and Query Performance with Elasticsearch on Azure (Повышение производительности запросов и статистической обработки с помощью Elasticsearch в среде Azure) резюмирует тесты производительности, которые провела группа разработчиков шаблонов и рекомендаций Майкрософт, в том числе для этой цели.

### Подключение к кластеру

Elasticsearch предоставляет ряд интерфейсов REST API для создания клиентских приложений и отправки запросов к кластеру. Для разработки приложений с помощью .NET Framework доступны два API высокого уровня: [Elasticsearch.Net и NEST][].

Если вы создаете клиентские приложения с помощью Java, вы можете использовать [API Node Client][] для динамического создания клиентских узлов и их добавления в кластер. Динамическое создание клиентских узлов удобно, если ваша система использует относительно небольшое количество длительных подключений. Главный узел предоставляет клиентским узлам, созданным с помощью API Node, схему маршрутизации кластера (сведения о том, в каких узлах содержатся какие сегменты). Эта информация позволяет приложению Java напрямую подключаться к соответствующим узлам во время индексирования или запрашивания данных, уменьшая тем самым количество переходов, которые могут быть необходимы при использовании других интерфейсов API. Стоимость этого подхода равняется расходам на регистрацию клиентского узла в кластере. Если большое количество клиентских узлов быстро появляется и исчезает, постоянное обновление схемы маршрутизации кластера и ее распространение могут иметь большое значение.

**Распределение нагрузки подключения**

Elasticsearch включает несколько механизмов для распределения нагрузки подключения. В следующем списке перечислены некоторые типичные подходы.

**Распределение нагрузки на стороне клиента**. Если вы создаете клиентские приложения с помощью Elasticsearch.Net или API NEST, вы можете использовать пул подключений для циклического перебора запросов на подключение между узлами. Таким образом нагрузка будет распределена между запросами без внешней подсистемы балансировки нагрузки. В следующем фрагменте кода показано, как создать объект *ElasticsearchClient* с адресами трех узлов. Запросы из клиентского приложения будут распределены между этими узлами:

```csharp
// C#
var node1 = new Uri("http://node1.example.com:9200");
var node2 = new Uri("http://node2.example.com:9200");
var node3 = new Uri("http://node3.example.com:9200");

var connectionPool = new SniffingConnectionPool(new[] {node1, node2, node3});
var config = new ConnectionConfiguration(connectionPool);
var client = new ElasticsearchClient(config);
```

> [AZURE.NOTE] Похожая функциональность доступна для приложений Java. Она реализуется с помощью [API Transport Client][].

**Распределение нагрузки на сервере**. Вы можете использовать отдельный балансировщик нагрузки, чтобы распределять запросы к узлам. Преимущество этого подхода заключается в прозрачности адреса. Клиентские приложения не нужно настраивать с помощью информации о каждом узле. Это упрощает добавление, удаление или перемещение узлов без изменения кода клиента. На рисунке 3 показана конфигурация, которая использует балансировщик нагрузки для маршрутизации запросов к набору клиентских узлов. Такую же стратегию можно использовать для подключения непосредственно к узлам данных, если клиентские узлы не используются.

> [AZURE.NOTE] Вы можете использовать [балансировщик нагрузки Azure][], чтобы разместить кластер в общедоступном Интернете. Если клиентские приложения и кластер находятся в одной и той же виртуальной частной сети, используйте [внутреннюю подсистему балансировки нагрузки][].

![](media/guidance-elasticsearch-general-clientappinstances.png)

**Рис. 3.**
Экземпляры клиентского приложения, подключающиеся к кластеру Elasticsearch через балансировщик нагрузки Azure

**Произвольное распределение нагрузки**. Вы можете использовать [nginx][] в качестве обратного прокси-сервера вместо балансировщика нагрузки Azure. Сервер nginx предоставляет несколько методов распределения нагрузки, в том числе циклический перебор, наименее подключенный адресат (запрос направляется в назначение с наименьшим количеством текущих подключений) и хэширование на основе IP-адреса клиента.

> [AZURE.NOTE] Вы можете развернуть сервер nginx как виртуальную машину Azure. Для обеспечения доступности необходимо создать хотя бы два сервера nginx в одной группе доступности Azure.

Чтобы решить, следует ли использовать распределение нагрузки и какую реализацию использовать, учитывайте следующие факторы.

- Подключение к одному узлу для обработки всех запросов всех экземпляров приложения может привести к тому, что этот узел станет проблемным. Когда количество доступных на узле потоков исчерпается, запросы будут помещаться в очередь и могут быть отклонены, если очередь станет слишком длинной. Поэтому нельзя использовать фиксированные параметры подключения к одному узлу в коде приложения, которое могут развернуть много пользователей.
- Механизм циклического перебора Elasticsearch.Net, NEST и API-интерфейсов Transport Client обрабатывает завершившиеся сбоем запросы на подключение, повторяя попытку подключения к следующему доступному узлу в пуле подключений. Подключение к узлу, который не отвечает на запросы, можно временно пометить в пуле как *неактивное*. Этот узел может вернуться к работе позднее, а пул может проверять связь с узлом, чтобы определить, стал ли он снова активным.
- Балансировщик нагрузки Azure может прозрачно перенаправлять запросы к узлам в зависимости от ряда факторов (IP-адрес клиента, порт клиента, IP-адрес назначения, порт назначения, тип протокола). Следуя этой стратегии, экземпляр клиентского приложения, работающий на отдельно взятом компьютере, вероятнее всего, будет направляться на тот же узел Elasticsearch. В зависимости от конфигурации зонда балансировщика нагрузки, если на этом узле перестает работать служба Elasticsearch, но виртуальная машина продолжает работать, все подключения к этому узлу будут отложены, а подключения других экземпляров клиентского приложения к другим узлам могут продолжать работать.
- Балансировщик нагрузки Azure можно настроить так, чтобы он изымал узел из ротации, если тот не отвечает на запросы проверки работоспособности, выполняемой балансировщиком нагрузки.

### Обнаружение узла

Служба Elasticsearch основана на одноранговой связи, поэтому обнаружение других узлов в кластере является важной частью жизненного цикла узла. Благодаря обнаружению узлов в кластер могут динамически добавляться новые узлы данных, что в свою очередь позволяет кластеру прозрачно развертываться. Кроме того, если узел данных не отвечает на запросы связи от других узлов, главный узел может решить, что на узле данных произошел сбой. В таком случае главный узел предпримет необходимые действия, чтобы перераспределить сегменты, которые содержал неработающий узел, на исправные узлы данных.

Обнаружение узлов Elasticsearch обрабатывается с помощью модуля обнаружения. Модуль обнаружения — это подключаемый модуль, в котором можно выбирать различные механизмы обнаружения. Модуль обнаружения по умолчанию ([Zen][]) заставляет узел выдавать запросы проверки связи для поиска других узлов в той же сети. Если другой узел отвечает, они обмениваются информацией. Теперь главный узел может распределить сегменты на новый узел (если это узел данных), чтобы перераспределить нагрузку в кластере.

Модуль обнаружения Zen также обрабатывает процесс назначения главного узла и протокол обнаружения сбоев на узлах.

В версиях Elasticsearch, предшествующих версии 2.0, модуль обнаружения Zen использовал многоадресную связь, чтобы узлы контактировали друг с другом. Это позволяет легко вводить новый узел в кластер, но также может вызвать проблемы безопасности, если другая установка Elasticsearch в той же сети использует то же имя кластера. Новая установка считается частью того же кластера, и сегменты могут быть перенаправлены на узлы в этой установке. Кроме того, если узлы Elasticsearch работают как виртуальные машины Azure, многоадресный обмен сообщениями не поддерживается. Поэтому необходимо настроить модуль обнаружения Zen, чтобы он использовал одноадресный обмен сообщениями, и указать список допустимых для контакта узлов в файле конфигурации elasticsearch.yml.

> [AZURE.NOTE] В Elasticsearch 2.0 и более поздних версиях многоадресная рассылка больше не используется в качестве стандартного механизма обнаружения.

Если вы разместили кластер Elasticsearch в виртуальной сети Azure, вы можете указать, что закрытый IP-адрес, назначенный DHCP каждой виртуальной машине в кластере, должен оставаться выделенным (статическим). Вы можете настроить в модуле обнаружения Zen одноадресный обмен сообщениями с использованием этих статических IP-адресов. Если вы используете виртуальные машины с динамическими IP-адресами, помните: если виртуальная машина останавливается и перезапускается, ей может быть присвоен новый IP-адрес, что усложнит ее обнаружение. В таком случае вы можете заменить модуль обнаружения Zen на [облачный подключаемый модуль Azure][]. Этот подключаемый модуль использует API Azure для реализации механизма обнаружения, который основан на информации в подписке Azure.

> [AZURE.NOTE] Чтобы использовать текущую версию облачного подключаемого модуля Azure, необходимо установить сертификат управления для подписки Azure в хранилище ключей Java на узле Elasticsearch и указать в файле elasticsearch.yml расположение и учетные данные для доступа к хранилищу ключей. Этот файл хранится в виде открытого текста, поэтому важно проследить за тем, чтобы файл был доступен только для учетной записи, в которой выполняется служба Elasticsearch. Кроме того, этот подход может оказаться несовместимым с развертываниями диспетчера ресурсов Azure (ARM). Поэтому рекомендуется использовать статические IP-адреса для основных узлов и использовать эти узлы для одноадресного обмена сообщениями в кластере с помощью модуля обнаружения Zen. В следующей конфигурации, которая скопирована из файла elasticsearch.yml для примера узла данных, IP-адреса хоста ссылаются на главные узлы в кластере:

>`discovery.zen.ping.multicast.enabled: false`  
`discovery.zen.ping.unicast.hosts: ["10.0.0.10","10.0.0.11","10.0.0.12"]`

## Общие рекомендации по работе с системой

Elasticsearch может работать на разных компьютерах, от ноутбука до кластера мощных серверов. Однако чем больше ресурсов, таких как память, вычислительная мощность и высокоскоростные диски, доступно системе, тем выше ее производительность. В следующих разделах приведены основные требования к оборудованию и программному обеспечению для работы Elasticsearch.

### Требования к памяти
Elasticsearch пытается хранить данные в памяти для увеличения скорости. Рабочий сервер, на котором размещен узел, для стандартного корпоративного или коммерческого развертывания среднего размера в Azure должен иметь от 14 до 28 ГБ ОЗУ (для виртуальных машин размера D3 или D4). Распределяйте нагрузку между несколькими узлами, вместо того чтобы создавать узлы с большим объемом памяти. Эксперименты показали, что использование большего количества узлов с большим объемом памяти приводит к тому, что в случае сбоя требуется больше времени на восстановление. Тем не менее, несмотря на то что кластеры с очень большим количеством мелких узлов увеличивают доступность и пропускную способность, такая система также становится более требовательной к управлению и обслуживанию.

Распределите 50 % доступной на сервере памяти в куче Elasticsearch. Если вы используете Linux, укажите переменную среды ES\_HEAP\_SIZE, прежде чем запускать Elasticsearch. Кроме того, если вы используете Windows или Linux, можно указать размер памяти в параметрах *Xmx* и *Xms* при запуске Elasticseach. Укажите для этих двух параметров одно значение, чтобы виртуальная машина Java не могла изменить размер кучи в среде выполнения. Но не нужно выделять более 30 ГБ. Используйте оставшийся объем памяти для кэша файлов операционной системы.

> [AZURE.NOTE] Elasticsearch использует библиотеку Lucene для создания индексов и управления ими. Структуры Lucene используют формат диска. Кэширование этих структур в кэше файловой системы значительно увеличивает производительность.

> Кроме того, служба Elasticsearch написана на языке Java. Максимальный оптимальный размер кучи для Java на 64-разрядном компьютере — около 30 ГБ. Если куча превышает этот размер, Java переключается на использование расширенного механизма предоставления ссылок на объекты в куче. Но этот механизм увеличивает требования к памяти для каждого объекта и снижает производительность. Сборщик мусора Java по умолчанию (Concurrent Mark and Sweep, CMS) также может работать медленнее, если размер кучи превышает 30 ГБ. В данный момент не рекомендуется использовать другой сборщик мусора, так как служба Elasticsearch и библиотека Lucene были протестированы только со стандартным сборщиком.

Не перегружайте память: когда диск начинает выполнять функции оперативной памяти, производительность существенно снижается. Если возможно, полностью отключите эту замену (детали зависят от операционной системы). Если это невозможно, включите параметр *mlockall* в файле конфигурации Elasticsearch (elasticsearch.yml) следующим образом:

```yaml
bootstrap.mlockall: true
```

Этот параметр вызывает блокировку памяти виртуальной машины Java и предотвращает ее переключение операционной системой.

### Требования к дискам и системе

Для хранения сегментов используйте твердотельные накопители (SSD). Диски должны иметь соответствующий размер для хранения максимального объема данных, который ожидается в сегментах, хотя позднее можно добавить дополнительные диски. Сегмент можно расширить на несколько дисков на узле.

> [AZURE.NOTE] Elasticsearch сжимает данные для хранимых полей с помощью алгоритма LZ4, а в Elasticsearch 2.0 и более поздних версиях можно изменить тип сжатия. Выберите нужный алгоритм сжатия DEFLATE, используемый служебными программами *zip* и *gzip*. Этот метод сжатия может быть более ресурсоемким, зато его можно использовать для "холодных" индексов, таких как архивные данные журналов. Такой подход позволяет сократить размер индекса.

Не обязательно, чтобы все узлы в кластере имели одинаковую разметку и емкость диска. Тем не менее узел с очень большой емкостью диска по сравнению с другими узлами в кластере будет привлекать дополнительные данные и потребует повышения вычислительной мощности для обработки этих данных. Соответственно, нагрузка на этом узле может быть выше, чем на других узлах, а это, в свою очередь, может повлиять на производительность.

> [AZURE.NOTE] В Azure есть несколько вариантов дискового хранилища, в том числе стандартное хранилище, хранилище класса Premium и временное хранилище. Стандартное хранилище использует вращающиеся диски, а хранилище класса Premium — твердотельные накопители. В зависимости от номера SKU, который используется для реализации виртуальной машины, временное хранилище может быть реализовано на вращающемся диске (виртуальные машины серии A) или твердотельном накопителе (виртуальные машины серии D и выше). Рассматривая варианты реализации хранилища, подробное описание которых приведено в документе Maximizing Data Ingestion Performance with Elasticsearch on Azure (Максимальное увеличение производительности приема данных с помощью Elasticsearch в Azure), следует найти баланс между финансовыми и техническими аспектами.

По возможности используйте RAID 0 (чередование). Другие виды RAID, реализующие четность и зеркальное отображение, не нужны, так как Elasticsearch предоставляет собственное решение высокой доступности в виде реплик, а на дисках Azure в любом случае сохраняются три копии данных с дисков.

> [AZURE.NOTE] В версиях службы до Elasticsearch 2.0.0 можно было также реализовать чередование на программном уровне, указав несколько каталогов в параметре конфигурации *path.data*. В Elasticsearch 2.0.0 этот формат чередования больше не поддерживается. Вместо этого для разных путей могут выделяться разные сегменты, но все файлы в пределах одного сегмента будут записываться в одну и ту же папку. Если требуется чередование, данные следует чередовать на уровне оборудования или операционной системы.

Также следует помнить, что общая пропускная способность ввода-вывода для дисков Azure, подключенных к виртуальной машине, ограничивается группой хранения, к которой они принадлежат. Отдельная стандартная учетная запись хранения может обрабатывать запросы со скоростью до 20 000 операций ввода-вывода в секунду (это ограничение не применяется в хранилище класса Premium). Если это число меньше ожидаемой скорости трафика ввода-вывода на диске, распределите диски для виртуальных машин в кластере по нескольким учетным записям хранения. При этом следует помнить, что в одной подписке можно создать до 100 учетных записей хранения.

В библиотеке Lucene можно использовать большое количество файлов для хранения данных индекса, а в Elasticsearch можно открыть значительное количество сокетов для обмена данными между узлами и клиентами. Убедитесь, что в операционной системе настроена поддержка достаточного количества дескрипторов открытых файлов (до 64 000 при наличии достаточного объема памяти). Обратите внимание, что конфигурация по умолчанию для многих дистрибутивов Linux ограничивает количество дескрипторов открытых файлов значением 1024, а этого слишком мало.

Для оптимизации одновременного доступа к файлам данных и индексам Elasticsearch использует сочетание операций ввода-вывода MMIO и NIO (Java New). Если вы используете Linux, настройте операционную систему так, чтобы обеспечить достаточный объем виртуальной памяти и место для областей таблиц распределения памяти размером 256 КБ.

> [AZURE.NOTE] Многие дистрибутивы Linux по умолчанию используют планировщик Completely Fair Queuing (CFQ) при упорядочивании данных для записи на диск. Этот планировщик не оптимизирован для SSD. Попробуйте повторно настроить операционную систему, чтобы использовать планировщик NOOP или планировщик Deadline, которые более эффективны для SSD.

### Требования к ЦП

Используйте ЦП с несколькими ядрами. Лучше использовать ЦП с большим количеством ядер, чем более быстрые процессоры с меньшим количеством ядер. Это объясняется тем, что размер пула потоков Elasticsearch по умолчанию настраивается в соответствии с количеством доступных ядер ЦП. Алгоритмы, используемые Elasticsearch, оптимизируются на основе этих вычислений, к тому же изменять стандартные параметры пула потоков Elasticsearch не рекомендуется.

> [AZURE.NOTE] Виртуальные машины Azure доступны в различных конфигурациях ЦП и могут поддерживать от 1 до 32 ядер. На узле данных для начала рекомендуем использовать стандартную виртуальную машину серии D и выбирать SKU D3 (4 ядра) или D4 (8 ядер). Кроме того, для D3 предусмотрено 14 ГБ ОЗУ, а для D4 — 28 ГБ. Если вам требуется дисковое хранилище класса Premium, можно использовать виртуальную машину серии DS (рекомендуем DS3 или DS4). Если же вы считаете, что производительность ЦП является ключевым фактором при ограничении пропускной способности, выберите серию Dv2 с более мощными процессорами Intel Xeon с тактовой частотой 2,4 ГГц. В сериях G (для стандартного хранилища) и GS (для хранилища класса Premium) используются процессоры Xeon E5 V3, которые удобно использовать для рабочих нагрузок с высокой интенсивностью вычислений, например для крупномасштабных агрегатов. Актуальные сведения см. в [статье о размерах виртуальных машин][].

### Требования к сети

Для Elasticsearch требуется пропускная способность сети от 1 до 10 Гбит/с, в зависимости от размера и изменчивости реализуемых кластеров. При добавлении к кластеру дополнительных узлов Elasticsearch переносит сегменты между узлами. При этом предполагается, что время обмена данными между всеми узлами примерно равное, а относительное расположение сегментов на этих узлах не учитывается. Кроме того, репликация может привести к значительному увеличению количества сетевых операций ввода-вывода между сегментами. Поэтому не следует создавать кластеры на узлах, разбросанных по разным географическим регионам.

Обратите внимание, что пропускная способность виртуальных машин Azure ограничивается не только номером SKU, но и общим сетевым использованием физического оборудования, на котором работают виртуальные машины. Виртуальные машины, которые совместно используют оборудование, обладают более низкой пропускной способностью, чем виртуальные машины, работающие на выделенных устройствах, зато их развертывание обходится дешевле. Это означает, что кроме выбора серии и номера SKU для виртуальных машин, у вас практически нет инструментов влияния на доступную пропускную способность. Например, вы можете создавать виртуальные машины с несколькими виртуальными сетевыми адаптерами, но это не увеличит общую пропускную способность, доступную для виртуальных машин. Это просто означает, что за одни и те же физические сетевые ресурсы конкурирует больше виртуальных сетевых адаптеров.

### Требования к программному обеспечению

Elasticsearch можно запускать в ОС Windows или Linux. Служба Elasticsearch развертывается в виде библиотеки JAR-файлов Java и имеет зависимости от других библиотек Java, которые включены в пакет Elasticsearch. Для запуска Elasticsearch необходимо установить виртуальную машину Java 7 (обновление 55 или новее) или Java 8 (обновление 20 или новее).

> [AZURE.NOTE] Не изменяйте параметры конфигурации по умолчанию для виртуальной машины Java, кроме параметров памяти *Xmx* и *Xms* (заданы как параметры командной строки механизма Elasticsearch — см. раздел [Требования к памяти][]). Служба Elasticsearch рассчитана на использование параметров по умолчанию. Их изменение может привести к сбою настроек и снижению производительности Elasticsearch.

### Развертывание Elasticsearch в Azure

Развернуть отдельный экземпляр Elasticsearch несложно, однако создание нескольких узлов, установка и настройка Elasticsearch на каждом из них — довольно длительный процесс, в ходе которого могут возникать ошибки. Если вы собираетесь запускать Elasticsearch на виртуальных машинах Azure, есть два способа, которые помогают снизить вероятность ошибок.
- Создание кластера с помощью [шаблона диспетчера ресурсов Azure (ARM)](http://azure.microsoft.com/documentation/templates/elasticsearch/). Этот шаблон полностью параметризован и позволяет настроить размер и уровень производительности для виртуальных машин, на которых реализуются узлы, количество дисков и другие часто используемые параметры. Шаблон может создать кластер под управлением Windows Server 2012 или Ubuntu Linux 14.0.4.
- Использование сценариев, которые могут быть автоматизированными или выполняться автоматически. Сценарии, которые могут создать и развернуть кластер Elasticsearch, доступны на сайте [шаблонов быстрого запуска Azure][].

## Рекомендации по размерам и масштабируемости кластеров и узлов

Elasticsearch включает ряд топологий развертывания, предназначенных для поддержки различных требований и уровней масштабирования. В этом разделе рассматриваются некоторые общие топологии и рекомендации по реализации кластеров на основе этих топологий.

### Топологии Elasticsearch

На рисунке 4 показана отправная точка разработки топологии Elasticsearch для облака на основе виртуальных машин Azure.

![](media/guidance-elasticsearch-general-startingpoint.png)

**Рис. 4**
Рекомендуемая отправная точка для создания кластера Elasticsearch в Azure

Эта топология состоит из шести узлов данных с тремя клиентскими узлами и тремя главными узлами (выбран только один главный узел, два других можно выбрать в случае сбоя главного узла). Каждый узел реализован в виде отдельной виртуальной машины. Веб-приложения Azure направляются на клиентские узлы с помощью подсистемы балансировки нагрузки. В этом примере все узлы и веб-приложения находятся в одной виртуальной сети Azure, которая эффективно изолирует их от внешнего мира. Если кластер должен быть доступен извне (возможно, как часть гибридного решения, в которое входят локальные клиенты), можно использовать балансировщик нагрузки Azure для указания общедоступного IP-адреса. Однако в этом случае вам придется принять дополнительные меры безопасности для предотвращения несанкционированного доступа к кластеру. Дополнительное "поле перехода" — это виртуальная машина, доступная только для администраторов. Эта виртуальная машина имеет сетевое подключение к виртуальной сети Azure, а также обращенное наружу сетевое подключение, позволяющее администраторам выполнять вход из внешней сети (этот вход должен быть защищен с помощью надежного пароля или сертификата). Администратор может войти в "поле перехода" и подключиться оттуда напрямую к любому из узлов в кластере. К альтернативным подходам относится использование VPN типа "сеть-сеть" между организацией и виртуальной сетью или использование каналов [ExpressRoute][] для подключения к виртуальной сети. Эти механизмы предоставляют администраторам доступ к кластеру без отображения кластера в общедоступном Интернете.

Для обеспечения доступности виртуальной машины узлы данных объединяются в одну группу доступности Azure. Аналогичным образом клиентские узлы хранятся в другой группе доступности, а главные узлы — в третьей группе доступности.

Эту топологию относительно легко масштабировать. Просто добавьте дополнительные узлы соответствующего типа и убедитесь, что в них настроено имя кластера, указанное в файле elasticsearch.yml. Клиентские узлы также следует добавлять к пулу серверной части с помощью балансировщика нагрузки Azure.

**Определение географического расположения кластеров**

Не распределяйте узлы в кластере по разным регионам, так как это может повлиять на производительность обмена данными между узлами (см. раздел [Требования к сети][]). Для определения местонахождения данных рядом с пользователями в разных регионах нужно создать несколько кластеров. В этом случае необходимо учесть, как будут синхронизироваться кластеры (и будут ли вообще). Возможные варианты решения приведены ниже.

**Подключение к кластеру с помощью [узлов Tribe][].** Узел Tribe похож на клиентский узел, за исключением того, что он может участвовать в нескольких кластерах Elasticsearch и просматривать их все как один большой кластер. Данные по-прежнему управляются локально каждым кластером (обновления не выходят за границы кластеров), при этом отображаются все данные. Узел Tribe может запрашивать, создавать и контролировать документы в любом кластере. Основные ограничения: узел Tribe не может использоваться для создания нового индекса, а имена индексов должны быть уникальными для всех кластеров. Поэтому при создании кластеров, к которым будет предоставляться доступ с узлов Tribe, важно учитывать правила именования индексов.

Благодаря этому механизму каждый кластер может содержать данные, к которым с наибольшей вероятностью будут обращаться локальные клиентские приложения. Эти клиенты по-прежнему могут открывать и изменять удаленные данные, хотя при этом возможны значительные задержки. Пример этой топологии показан на рисунке 5. Узел Tribe кластера 1 выделен. В других кластерах также могут быть узлы Tribe, хотя они не показаны на схеме.

![](media/guidance-elasticsearch-general-tribenode.png)

**Рис. 5.**
Доступ клиентского приложения к нескольким кластерам через узел Tribe

В этом примере клиентское приложение подключается к узлу Tribe в кластере 1 (размещенному в том же регионе), но этот узел настроен для доступа к кластеру 2 и кластеру 3, которые могут находиться в разных регионах. Клиентское приложение может отправлять запросы для извлечения или изменения данных в любом из кластеров.

> [AZURE.NOTE] Чтобы подключаться к кластерам, которые могут представлять угрозу безопасности, узлам Tribe требуется обнаружение посредством многоадресной рассылки. Дополнительные сведения см. в разделе [Обнаружение узлов][].

*	Реализация георепликации между кластерами. При таком подходе изменения, внесенные в каждом кластере, распространяются на кластеры, расположенные в других центрах обработки данных, практически в режиме реального времени. Для службы Elasticsearch, поддерживающей этих функции, доступны cторонние модули, такие как [подключаемый модуль изменений PubNub][].
*	Использование [модуля моментальных снимков и восстановления в Elasticsearch][]. Если данные перемещаются очень медленно и изменяются только в одном кластере, можно использовать моментальные снимки для периодического копирования данных и последующего восстановления этих моментальных снимков в других кластерах (моментальные снимки могут храниться в хранилище BLOB-объектов Azure, если вы установили [подключаемый модуль облачных служб Azure][]). Однако это решение неэффективно для быстро изменяющихся данных или в случае, если данные могут изменяться в нескольких кластерах.

**Небольшие топологии**

Крупномасштабные топологии, включающие кластеры выделенных главных и клиентских узлов, а также узлов данных, подходят не для всех сценариев. При создании небольших рабочих систем или систем разработки рекомендуется использовать кластер с тремя узлами, показанный на рисунке 6. Клиентские приложения подключаются напрямую к любому доступному узлу данных в кластере. Кластер содержит три сегмента с именами P1–P3 (что предусматривает возможность добавления сегментов), а также реплики R1–R3. Использование трех узлов позволяет Elasticsearch распределять сегменты и реплики так, чтобы избежать потери данных в случае сбоя на одном узле.

![](media/guidance-elasticsearch-general-threenodecluster.png)

**Рис. 6.**
Кластер с тремя узлами, тремя сегментами и репликами

При выполнении установки для разработки на изолированном компьютере можно настроить кластер с одним узлом, который будет выполнять роль главного узла, клиентского узла и узла данных. Кроме того, можно запустить несколько узлов, работающих как кластер на одном компьютере, запустив несколько экземпляров Elasticsearch. На рисунке 7 приведен пример.

![](media/guidance-elasticsearch-general-developmentconfiguration.png)

**Рис. 7.**
Конфигурация разработки с несколькими узлами Elasticsearch, работающими на одном компьютере

Обратите внимание, что ни одну из этих изолированных конфигураций не рекомендуется использовать для рабочей среды, так как они вызывают конфликты, если на компьютере для разработки нет значительного объема памяти и нескольких высокоскоростных дисков. Кроме того, они не предоставляют никаких гарантий высокой доступности — в случае сбоя на компьютере все узлы будут потеряны.

### Рекомендации по масштабированию кластера и узлов данных

Elasticsearch можно масштабировать в двух направлениях: вертикально (используя более мощные компьютеры с большим объемом памяти) и горизонтально (распределяя нагрузку на несколько компьютеров).

**Вертикальное масштабирование узлов данных Elasticsearch**

Если вы размещаете кластер Elasticsearch с помощью виртуальных машин Azure, каждый узел может соответствовать виртуальной машине. Предел вертикального масштабирования для узла в значительной степени зависит от SKU виртуальной машины и общих ограничений, применяемых к отдельным учетным записям и подпискам Azure. На странице, посвященной [квотам и ограничениям для подписок и служб Azure](azure-subscription-service-limits/), эти ограничения описываются подробно, но в контексте создания кластера Elasticsearch наиболее важными являются пункты из приведенного ниже списка. Кроме того, не следует использовать виртуальные машины с объемом памяти более 64 ГБ без крайней необходимости. Как описано в разделе [Требования к памяти][], не следует выделять более 30 ГБ оперативной памяти на каждую виртуальную машину в виртуальной машине Java и использовать оставшийся объем памяти для буферизации операций ввода-вывода.
- Для каждой учетной записи хранения установлено ограничение на 20 000 операций ввода-вывода в секунду. Использование одной и той же учетной записи хранения для хранения нескольких виртуальных жестких дисков может ограничить их производительность.
- Количество узлов данных в виртуальной сети. Если вы не используете диспетчер ресурсов Azure (ARM), в одной виртуальной сети можно использовать не более 2048 экземпляров виртуальных машин. Этого достаточно во многих случаях, однако, если вы используете очень большие конфигурации с сотнями узлов, такое ограничение может создавать неудобства.
- Количество учетных записей хранения на подписку в одном регионе. В каждом регионе можно создать до 100 учетных записей хранения в пределах одной подписки Azure. Для хранения виртуальных дисков используются учетные записи хранения, и каждая из них ограничена 500 ТБ дискового пространства.
- Количество ядер в подписке. Ограничение по умолчанию составляет 20 ядер на подписку, но корпорация Майкрософт может увеличить его до 10 000 ядер. Помните, что виртуальные машины определенных размеров (A9, A11, D14 и DS14) могут содержать 16 ядер, а VM размера G5 имеет 32 ядра.
- Объем памяти для каждого размера виртуальной машины. Виртуальные машины меньшего размера обладают ограниченным объемом доступной памяти (в машинах размера D1 3,5 ГБ памяти, а в машинах размера D2 — 7 ГБ). Эти машины могут не подходить для сценариев, когда службе Elasticsearch требуется кэшировать значительный объем данных для достижения высокой производительности (например, при статистической обработке данных или анализе большого количества документов во время приема данных).
- Максимальное количество дисков на размер виртуальной машины. Это ограничение может влиять на размер и производительность кластера. Меньшее количество дисков означает, что на них может храниться меньше данных. Производительность также может уменьшиться из-за меньшего количества дисков, доступных для чередования.
- Количество доменов обновления и доменов сбоя на группу доступности. При создании виртуальных машин с помощью ARM на каждую группу доступности можно выделить до 3 доменов сбоя и 20 доменов обновления. Это ограничение может повлиять на устойчивость больших кластеров, которые часто подвергаются последовательным обновлениям.

Учитывая эти ограничения, следует всегда распределять виртуальные диски для виртуальных машин в кластере по учетным записям хранения, чтобы снизить вероятность регулирования количества операций ввода-вывода. В очень больших кластерах может потребоваться переработать логическую инфраструктуру, разбив ее на отдельные функциональные разделы. Например, может потребоваться разбить кластер по подпискам, хотя этот процесс может привести к дальнейшим сложностям из-за необходимости подключать виртуальные сети.

>	[AZURE.NOTE] Примите к сведению, что учетным записям хранения Azure присваивается определенная метка хранилища. Это внутренний механизм, который используется для поддержания согласованности и доступности. Дополнительные сведения о том, как это работает, приведены в статье [A Highly Available Cloud Storage Service with Strong Consistency][] (Высокодоступная служба облачного хранилища с высоким уровнем согласованности). Если в каком-то хранилище, которому назначена определенная метка, возникнет нехватка метка, на всех дисках, созданных с помощью этой учетной записи, появится ошибка. В этом случае все виртуальные машины, использующие эти диски, могут выйти из строя. Таким образом, размещение разных дисков одной виртуальной машины в нескольких учетных записях хранения увеличивает риск возникновения ошибки на этой виртуальной машине. В связи с этим для узла рекомендуется использовать только одну учетную запись хранения и хранить в ней системный диск и все диски с данными.

**Горизонтальное масштабирование кластера Elasticsearch**

В пределах Elasticsearch ограничения горизонтального масштабирования зависят от количества сегментов, определенных для каждого индекса. Первоначально для одного узла в кластере могут быть выделены несколько сегментов, но по мере роста объема данных можно добавлять дополнительные узлы, распределяя сегменты по этим узлам. Теоретически система прекращает масштабирование по горизонтали только в том случае, если количество узлов становится равным количеству сегментов.

Как и при вертикальном масштабировании, существуют аспекты, которые следует учитывать при планировании реализации горизонтального масштабирования, в частности:

- Максимальное количество виртуальных машин, которые можно подключить в виртуальной сети Azure. Оно может ограничить горизонтальную масштабируемость в очень больших кластерах. Чтобы обойти это ограничение, можно создать кластер с узлами в нескольких виртуальных сетях, но такой подход может привести к снижению производительности из-за отсутствия локальности между узлами.
- Количество дисков на размер виртуальной машины. Виртуальные машины разных серий и с разными номерами SKU поддерживают разное количество подключаемых дисков. Кроме того, также можно использовать временное хранилище, входящее в состав виртуальной машины, как хранилище данных ограниченного объема, в котором чтение и запись данных выполняется быстрее. Однако в таком случае следует учитывать возможные последствия для устойчивости и восстановления (дополнительные сведения см. в документе Configuring, Testing, and Analyzing Elasticsearch Resilience and Recovery (Настройка, тестирование и анализ устойчивости и восстановления Elasticsearch)). На виртуальных машинах серий D, DS, Dv2 и GS для временного хранилища используются твердотельные накопители.
- Максимальное количество операций ввода-вывода на виртуальный диск. Для обычных подключенных дисков, созданных с помощью стандартного хранилища, максимальное количество операций ввода-вывода в секунду равно 500 (на один диск). Подключенные SSD могут поддерживать до 5000 операций ввода-вывода на один диск. Чтобы использовать SSD для подключенных дисков (не для временного хранилища), необходимо создать виртуальную машину, которая поддерживает хранилище класса Premium (машину серии DS или GS).

Попробуйте использовать масштабируемые наборы Azure для запуска и остановки виртуальных машин по запросу (подробные сведения см. в статье [Автоматическое масштабирование машин в масштабируемом наборе виртуальных машин][]). Однако этот способ может не подходить для кластера Elasticsearch по следующим причинам:

- Этот способ лучше всего подходит для виртуальных машин без сохранения состояния. Каждый раз при добавлении узла в кластер Elasticsearch или при его удалении сегменты выделяются повторно для распределения нагрузки. Этот процесс может создавать значительные объемы сетевого трафика и дисковых операций ввода-вывода, существенно снижая скорость приема данных. Необходимо оценить, компенсируются ли эти издержки преимуществами дополнительной обработки и ресурсов памяти, которые становятся доступными при динамическом запуске дополнительных виртуальных машин.
- Виртуальная машина запускается не мгновенно — прежде чем дополнительные виртуальные машины станут доступны или завершат работу, может пройти несколько минут. Масштабирование такого типа следует использовать только при постоянных изменениях спроса.
- Необходимо ли после масштабирования снова уменьшать масштаб? Удаление виртуальной машины из кластера Elasticsearch может быть ресурсоемким процессом, в ходе которого служба Elasticsearch должна восстановить сегменты и реплики, которые находятся на этой виртуальной машине, и заново создать их на одном или нескольких из оставшихся узлов. Одновременное удаление нескольких виртуальных машин может нарушить целостность кластера, что усложняет восстановление. Кроме того, многие реализации Elasticsearch увеличиваются с течением времени, но данным не свойственно уменьшение в объеме. Можно вручную удалять документы или настраивать для них срок жизни, по истечении которого они будут удаляться, но в большинстве случаев выделенное для них ранее пространство заполняют новые или измененные документы. При удалении или изменении документов в индексе может выполняться фрагментация. В этом случае для дефрагментации можно использовать API [оптимизации][] Elasticsearch HTTP (Elasticsearch 2.0.0 и более ранних версий) или API [принудительного слияния][] (Elasticsearch 2.1.0 и более поздних версий).

> [AZURE.NOTE] Оптимизация — это очень затратная операция, которую не следует выполнять во время активного использования индекса. Индекс рекомендуется оптимизировать, когда он бездействует, так как это позволяет высвободить ресурсы, необходимые для выполнения поиска.

### Определение количества сегментов для индекса

Хотя количество узлов в кластере со временем может меняться, количество сегментов в индексе после его создания остается неизменным. Для добавления или удаления сегментов данные необходимо повторно индексировать — создавать новый индекс с требуемым количеством сегментов, а затем копировать данные из старого индекса. При этом вы можете использовать псевдонимы, чтобы скрыть от пользователей факт повторной индексации данных. Дополнительную информацию см. в документе Maximizing Data Aggregation and Query Performance with Elasticsearch on Azure (Максимальное увеличение производительности запросов и агрегирования данных с помощью Elasticsearch в Azure). Таким образом, требуемое количество сегментов лучше определить перед созданием первого индекса в кластере. Это можно сделать следующим образом.

1. Создайте кластер с одним узлом, используя конфигурацию оборудования, которую вы планируете развернуть в рабочей среде.
2. Создайте индекс, соответствующий структуре, которую вы планируете использовать в рабочей среде. Назначьте этому индексу один сегмент (без реплик).
3. Добавьте в индекс определенное количество реальных рабочих данных.
4. Выполните типовые запросы, агрегирование и другие рабочие нагрузки с использованием индекса, а затем оцените пропускную способность и время ответа.
5. Если пропускная способность и время ответа не превышают допустимые значения, повторите процедуру начиная с шага 3 (добавьте больше данных).
6. Когда емкость сегмента достигнет предельных значений (время ответа и пропускная способность станут неприемлемыми), отметьте количество документов.
7. Рассчитайте требуемое количество сегментов, экстраполируя емкость одного сегмента на планируемое количество документов в рабочей среде (учитывайте погрешность, так как экстраполяция не является точным методом расчета).

> [AZURE.NOTE] Помните, что каждый сегмент реализуется как индекс Lucene, который использует память, ресурсы ЦП и дескрипторы файлов. Чем больше у вас сегментов, тем больше ресурсов вам понадобится.

Кроме того, создавая большее количество сегментов, вы можете увеличить масштабируемость (в зависимости от рабочих нагрузок и сценария) и пропускную способность при приеме данных, хотя это может привести к снижению производительности многих запросов. По умолчанию запрос будет обращаться ко всем сегментам, которые используются индексом. (Такое поведение можно изменить с помощью [настраиваемой маршрутизации][], если известно, в каких сегментах расположены необходимые данные).

В рамках этого процесса можно рассчитать только количество сегментов. Предполагаемый объем документов, используемых в рабочей среде, может быть неизвестен. Следовательно, вам нужно определить исходный объем (как описано выше) и прогнозируемый темп роста. Создайте соответствующее количество сегментов, которые смогут обрабатывать увеличивающийся объем данных до момента выполнения повторной индексации базы данных. Для таких задач, как управление событиями и ведение журналов, используются другие стратегии, в том числе скользящие индексы. При этом для данных, принимаемых каждый день, создается новый индекс, доступ к которому осуществляется с помощью псевдонима. Псевдонимы ежедневно меняются, обеспечивая доступ к последней версии индекса. Такой подход позволяет не только легко исключать устаревшие данные (вы можете удалять индексы с информацией, которая больше не нужна), но и управлять объемом данных.

Помните, что количество узлов не соответствует количеству сегментов. Например, когда вы создаете 50 сегментов, вы можете сначала распределить их между 10 узлами и уже потом добавлять дополнительные узлы, масштабируя систему согласно увеличению рабочей нагрузки. При этом лучше избегать ситуаций, когда огромное количество сегментов распределено между несколькими узлами — например, когда 1000 сегментов разнесены на 2 узла. Теоретически систему с такой конфигурацией можно масштабировать до 1000 узлов. Но запуск 500 сегментов на одном узле может привести к снижению его производительности.

> [AZURE.NOTE] Для систем с высокой интенсивностью приема данных можно попробовать использовать количество сегментов, равное простому числу. Стандартный алгоритм Elasticsearch, используемый для маршрутизации документов в сегменты, предполагает более равномерное распределение.

### Вопросы безопасности

По умолчанию в Elasticsearch реализован минимальный уровень защиты. Встроенные средства для проверки подлинности и авторизации не предоставляются. Для выполнения этих процедур требуется настройка базовой операционной системы и сети, а также использование подключаемых модулей и служебных программ сторонних разработчиков. Примеры включают такие решения, как [Shield][] и [Search Guard][].

> [AZURE.NOTE] Shield от Elastic — это подключаемый модуль для проверки подлинности пользователей, шифрования данных, управления доступом на основе ролей, IP-фильтрации и аудита. Для реализации дополнительных мер безопасности, включая шифрование диска, может потребоваться настройка базовой операционной системы.

В рабочей системе следует продумать следующие моменты.

- Как предотвращать несанкционированный доступ к кластеру.
- Как выполнять идентификацию и проверку подлинности пользователей.
- Как выполнять авторизацию операций, которые могут запускать прошедшие проверку подлинности пользователи.
- Как защитить кластер от выполнения несанкционированных или опасных операций.
- Как защитить данные от несанкционированного доступа.
- Как обеспечить соответствие системы защиты коммерческих данных нормативным требованиям к информационной безопасности (если применимо).

### Защита доступа к кластеру

Elasticsearch — это сетевая служба. Узлы в кластере Elasticsearch ожидают передачи данных (входящих клиентских запросов) по протоколу HTTP, а обмениваются данными — через канал TCP. Следует принять меры для предотвращения отправки запросов через HTTP и TCP от неавторизованных клиентов или служб. Рекомендуются следующие действия.

- Настройте группы безопасности сети, которые будут ограничивать входящий и исходящий сетевой трафик для виртуальной сети или виртуальной машины только определенными портами.
- Измените порты по умолчанию, используемые для веб-доступа клиента (9200) и программного сетевого доступа (9300). Используйте брандмауэр, чтобы защитить все узлы от вредоносного интернет-трафика.
- В зависимости от расположения и подключения клиентов разместите кластер в частной подсети без прямого доступа к Интернету. Если кластер должен быть доступным за пределами этой подсети, чтобы защитить его, настройте жесткую маршрутизацию всех запросов через сервер-бастион или прокси-сервер.
- Если предусматривается предоставление прямого доступа к узлам, используйте подключаемый модуль Elasticsearch Jetty для SSL-подключения, проверки подлинности и ведения журнала подключения. Или вы можете настроить прокси-сервер nginx и проверку подлинности HTTPS.

> [AZURE.NOTE] С помощью прокси-сервера nginx вы также можете ограничить доступ к разным функциям. Например, если вам нужно запретить клиентам выполнять другие операции, вы можете настроить nginx так, чтобы он разрешал запросы только к конечной точке \_search.

- Если требуется расширенная защита доступа к сети, используйте подключаемые модули Shield или Search Guard.

### Идентификация и проверка подлинности пользователей

Все запросы, отправляемые клиентами в кластер, должны проходить проверку подлинности. Также следует запретить подключение неавторизованных узлов к кластеру, предотвращая возможность обхода проверки подлинности в системе.

С помощью подключаемых модулей Elasticsearch можно выполнять следующие виды проверки подлинности. 1) Обычная проверка подлинности HTTP: включает проверку имен пользователей и паролей. Все запросы должны быть зашифрованы с помощью SSL/TLS или иметь эквивалентный уровень защиты. 2) Интеграция с LDAP и Active Directory: при использовании этого подхода клиентам назначаются роли в группах LDAP или AD. 3) Собственная проверка подлинности с использованием удостоверений, определенных в самом кластере Elasticsearch: проверка подлинности TLS в кластере выполняется для проверки подлинности всех узлов. 4) IP-фильтрация: защита от подключения к кластеру клиентов и узлов из неавторизованных подсетей.

### Авторизация клиентских запросов

Способ авторизации зависит от подключаемого модуля Elasticsearch, используемого для этой операции. Например, подключаемый модуль, который обеспечивает обычную проверку подлинности, как правило, предоставляет функции, которые определяют уровень проверки подлинности. Подключаемый модуль, который использует LDAP или AD, обычно связывает клиенты с ролями, а затем назначает этим ролям права доступа. При использовании любого модуля следует учитывать следующие моменты. 1) Нужно ли ограничить операции, которые может выполнять клиент? Например, может ли клиент отслеживать состояние кластера, а также создавать и удалять индексы? 2) Нужно ли, чтобы клиент имел доступ только к определенным индексам? Это полезно, когда используется мультитенантная конфигурация, позволяющая назначать клиентам собственный набор индексов, которые должны быть недоступными для других клиентов. 3) Может ли клиент выполнять в индексе операции чтения и записи данных? Например, клиент может выполнять поиск данных с помощью индекса, но не может ни добавлять данные в этот индекс, ни удалять их оттуда.

Сейчас область действия большинства подключаемых модулей безопасности ограничивается операциями на уровне кластера или индекса, исключая воздействие на подмножества документов в индексах. Хотя это позволяет повысить производительность поиска, ограничить выполнение запросов конкретными документами в рамках одного индекса довольно сложно. Если вам требуется более точное управление, сохраняйте документы в отдельных индексах и используйте псевдонимы, которые группируют индексы. Для примера представим, что у вас есть система по учету персонала. Пользователю A требуется доступ ко всем документам, которые содержат сведения о сотрудниках отдела X, пользователю В — к документам, которые содержат сведения о сотрудниках отдела Y, а пользователю C — к документам, содержащих сведения о сотрудниках в обоих отделах. В этом случае вам нужно создать два индекса (отделы X и Y) и псевдоним, который ссылается на оба индекса. Предоставьте пользователю A право на чтение первого индекса, пользователю Б — право на чтение второго индекса, а пользователю С — право на чтение двух индексов с помощью этого псевдонима. Дополнительные сведения см. в разделе [Faking Index per User with Aliases][] (Создание фиктивных индексов для пользователей с помощью псевдонимов).

### Защита кластера

Незащищенный кластер может стать уязвимым при неправильном использовании. Ниже описаны примеры таких операций и действия по их предотвращению.

- Отключите в Elasticsearch возможность создания динамических запросов с помощью скриптов, так как их использование может привести к уязвимости системы безопасности. Старайтесь использовать собственные скрипты при создании запросов. Собственный скрипт — это подключаемый модуль Elasticsearch, написанный на языке Java и скомпилированный в JAR-файл.

> [AZURE.NOTE] Возможность создания динамических запросов с помощью скриптов теперь отключена по умолчанию. Включайте ее, только если у вас есть веская причина для этого.

- Не предоставляйте пользователям возможность поиска с помощью строки запроса. Такой метод поиска позволяет пользователям беспрепятственно выполнять ресурсоемкие запросы, что может серьезно повлиять на производительность кластера и сделать систему уязвимой для DOS-атак. Кроме того, при поиске с использованием строки запроса пользователи могут получать доступ к потенциально конфиденциальной информации.
- Запретите избыточное потребление памяти операциями, так как это вызывает исключения, связанные с нехваткой памяти, которые, в свою очередь, приводят к сбою Elasticsearch на узле. Длительные ресурсоемкие операции также могут использоваться для DOS-атак. Примеры приведены ниже.
- Запросы на поиск, которые пытаются загрузить в память очень большие поля (если запрос предусматривает сортировку этих полей, а также использование скриптов или аспектов).
  
> [AZURE.NOTE] Формат [doc\_values][] позволяет сократить объем используемой индексами памяти, сохраняя данные поля (элементы fielddata) на диск, вместо того чтобы загружать их в память. Это позволяет предотвратить нехватку памяти на узле, но за счет потери скорости.

- Операции поиска, которые одновременно отправляют запросы в несколько индексов.
- Операции поиска, которые получают большое количество полей. Такие операции могут вызвать переполнение памяти с последующим кэшированием огромного количества данных поля. По умолчанию кэш данных поля не ограничен в размере, но вы можете ограничить доступные ресурсы, задав свойство [indices.fielddata.cache.*] (https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-fielddata.html) в файле конфигурации elasticsearch.yml. Можно также настроить [размыкатель цепи данных поля][], чтобы предотвратить переполнение памяти при кэшировании данных из одного поля, и [размыкатель цепи запросов][], чтобы предотвратить эксклюзивное использование памяти отдельными запросами. Учтите, что при использовании этих параметров растет вероятность истечения времени ожидания или неудачного завершения некоторых запросов.
  
> [AZURE.NOTE] Предполагается, что в среде Elasticsearch всегда достаточно памяти для выполнения текущей рабочей нагрузки. В противном случае может произойти сбой службы Elasticsearch. В Elasticsearch доступны конечные точки, которые возвращают сведения об использовании ресурсов ([API-интерфейсы cat][] для HTTP). Рекомендуется внимательно отслеживать эту информацию.

- Слишком долгое ожидание записи данных из обрабатываемого сегмента памяти на диск, которое может привести к переполнению буфера памяти. В этом случае [настройте журнал транзакций][], чтобы снизить пороговые значения, при достижении которых данные будут записаны на диск.

- Создание индексов с большим количеством метаданных. Индекс, содержащий документы с разнообразными вариантами имен полей может потреблять большой объем памяти. Дополнительные сведения см. в разделе [Mapping Explosion][] (Избыточное сопоставление).
  
> [AZURE.NOTE] Определение длительных или ресурсоемких операций запроса в значительной мере зависит от сценария. Как правило, ожидаемые показатели рабочей нагрузки на двух разных кластерах существенно отличаются. Чтобы определить недопустимые операции, следует выполнить тщательный анализ и тестирование приложений.

Действуйте на упреждение: определяйте и останавливайте вредоносные действия, прежде чем они приведут к значительным повреждениям или потере данных. Рекомендуется использовать систему мониторинга безопасности и оповещения, которая может быстро выявлять необычные попытки доступа к данным и выдавать предупреждения. Подозрительное поведение может включать, например, неудачные запросы на вход, неожиданное подключение к узлам или отключение от них, а также превышение ожидаемого времени выполнения операций. Эти задачи может выполнять такое средство Elasticearch, как [Watcher][].

### Защита данных

Вы можете защитить данные, используя SSL/TLS, но среда Elasticsearch не предоставляет встроенные средства для шифрования информации, хранящейся на диске. Помните, что такая информация хранится в обычных файлах на диске, следовательно, любой пользователь с доступом к этим файлам может раскрыть их содержимое, например, скопировав их на свой кластер. Рекомендуется выполнить следующие действия. Защитите файлы, используемые в Elasticsearch для хранения данных.
- Не предоставляйте свободный доступ к операциям чтения или записи удостоверениям, не входящим в службу Elasticsearch.

- Шифруйте данные, хранящиеся в таких файлах, с помощью шифрованной файловой системы.

> [AZURE.NOTE] Сейчас Azure поддерживает шифрование данных на диске для виртуальных машин Windows и Linux. Дополнительные сведения см. в статье [Шифрование дисков Azure для виртуальных машин IaaS под управлением Windows и Linux (предварительная версия)][].

### Соответствие нормативным требованиям

Нормативные требования в основном касаются операций аудита для ведения журнала событий и обеспечения конфиденциальности этих операций, т. е. защиты от отслеживания и воспроизведения таких операций сторонними пользователями. В частности, вам нужно решить следующие задачи.

- Как отслеживать все запросы (успешные или нет), а также все попытки доступа к системе.
- Как шифровать данные, отправляемые клиентами в кластер, а также данные, передаваемые между узлами в кластере. Для всех взаимодействий на уровне кластера следует реализовать SSL/TLS. Среда Elasticsearch также поддерживает подключаемые шифры, если требования вашей организации предполагают использование средств, отличных от SSL/TLS.
- Как надежно хранить все данные аудита. Объем данных аудита может очень быстро увеличиваться, при этом сами данные должны быть надежно защищены во избежание их подмены.
- Как надежно архивировать данные аудита.

### Рекомендации по мониторингу

Мониторинг следует выполнять как на уровне операционной системы, так и на уровне Elasticsearch.

На уровне операционной системы мониторинг можно реализовать с помощью специальных средств операционной системы. В разных ОС доступны разные средства: в Windows это монитор производительности с соответствующими счетчиками производительности, а в Linux — средства *vmstat*, *iostat* и *top*. Главные объекты мониторинга на уровне операционной системы представлены показателями использования ЦП, объема дисковых операций ввода-вывода, времени ожидания дисковых операций ввода-вывода и сетевого трафика. В хорошо настроенном кластере Elasticsearch показатели использования ЦП кластером должны быть высокими, а время ожидания дисковых операций ввода-вывода диска должно быть минимальным.

На уровне программного обеспечения необходимо отслеживать показатели пропускной способности и время ответа запросов, а также сведения о неудачных запросах. Elasticsearch предоставляет ряд интерфейсов API, которые можно использовать для оценки разных показателей производительности кластера. Самые важные API — это *\_cluster/health* и *\_nodes/stats*. API *\_cluster/health* может использоваться для получения моментального снимка общей работоспособности кластера и предоставления подробных сведений по каждому индексу:

`GET _cluster/health?level=indices`

Приведенные ниже выходные данные созданы с помощью этого API:

```json
{
    "cluster_name": "elasticsearch",
    "status": "green",
    "timed_out": false,
    "number_of_nodes": 6,
    "number_of_data_nodes": 3,
    "active_primary_shards": 10,
    "active_shards": 20,
    "relocating_shards": 0,
    "initializing_shards": 0,
    "unassigned_shards": 0,
    "delayed_unassigned_shards": 0,
    "number_of_pending_tasks": 0,
    "number_of_in_flight_fetch": 0,
    "indices": {
        "systwo": {
            "status": "green",
            "number_of_shards": 5,
            "number_of_replicas": 1,
            "active_primary_shards": 5,
            "active_shards": 10,
            "relocating_shards": 0,
            "initializing_shards": 0,
            "unassigned_shards": 0
        },
        "sysfour": {
            "status": "green",
            "number_of_shards": 5,
            "number_of_replicas": 1,
            "active_primary_shards": 5,
            "active_shards": 10,
            "relocating_shards": 0,
            "initializing_shards": 0,
            "unassigned_shards": 0
        }
    }
}
```

Этот кластер содержит два индекса с именем *systwo* и *sysfour*. Главные отслеживаемые статистические данные по каждому индексу представлены параметрами status, active\_shards и unassigned\_shards. Параметр status должен иметь значение green, значение параметра active\_shards (количество) должно отражать значения number\_of\_shards и number\_of\_replicas, а значение unassigned\_shards должно быть равно нулю. Если параметр status имеет значение red, это значит, что часть индекса отсутствует или такой индекс поврежден. Это истинно, если значение параметра *active\_shards* меньше результата уравнения *number\_of\_shards* - (*number\_of\_replicas* + 1), а значение unassigned\_shards не равно нулю. Обратите внимание, что значение yellow параметра status указывает на переходное состояние индекса (например, в результате добавления дополнительных реплик или перемещения сегментов). После завершения перехода параметр status должен отображать значение green. Если значение yellow не меняется продолжительное время или меняется на red, следует проверить, не произошли ли на уровне операционной системы какие-то значимые события ввода-вывода (например, сбой диска или сети).
API \_nodes/stats предоставляет подробные сведения о каждом узле в кластере:

`GET _nodes/stats`

Выходные данные включают сведения о том, как индексы хранятся на каждом узле (включая размер и количество документов), а также о времени, затраченном на выполнение индексирования, обработку запросов, поиск, объединение и кэширование. Кроме того, отображается информация о работе операционной системы, процессах и пулах потоков, а также статистические данные о работе JVM (включая производительность сбора мусора). Дополнительные сведения см. в разделе [Monitoring Individual Nodes][] (Мониторинг отдельных узлов).

Если значительная часть запросов Elasticsearch возвращает ошибку с сообщением *EsRejectedExecutionException*, это значит, что Elasticsearch не может справиться с их отправкой. В таком случае необходимо определить причину нестабильной работы Elasticsearch. Рекомендуются следующие действия.

- Проблема может быть вызвана ограниченными ресурсами, включая недостаточный объем памяти, выделенной для JVM, что приводит к избыточному количеству операций сбора мусора. В таком случае рекомендуется выделить дополнительные ресурсы. В нашем примере это означает настройку JVM для использования большего объема памяти — до 50 % доступного объема хранилища на узле. Дополнительную информацию см. в разделе [Memory Requirements][] (Требования к памяти).
- Если кластер демонстрирует значительное время ожидания операций ввода-вывода, а статистические данные по слиянию, собранные для индекса с помощью API \_node/stats, содержат большие значения, это указывает на высокую интенсивность операций записи, выполняемых индексом. Чтобы настроить производительность индексирования, выполните действия, описанные в разделе [Optimizing Resources for Indexing Operations] (Оптимизация ресурсов для операций индексирования).
- Запустите регулирование клиентских приложений, которые выполняют операции приема данных, и определите, как это влияет на производительность. Если этот подход позволяет добиться значительных улучшений, рекомендуется продолжать регулирование. Также можно выполнить развертывание, распределив нагрузку, вызванную использованием индексов с высокой интенсивностью записи, между несколькими узлами. Дополнительные сведения см. в документе Maximizing Data Ingestion Performance with Elasticsearch on Azure (Максимальное увеличение производительности приема данных с помощью Elasticsearch в Azure).
- Если статистика поиска в индексе указывает на продолжительное выполнение запросов, рекомендуется оптимизировать запросы. Дополнительные сведения см. в разделе [Настройка запросов][]. Обратите внимание: вы можете использовать включенные в статистику поиска значения *query\_time\_in\_millis* и *query\_total*, чтобы рассчитать приблизительные показатели производительности запросов. Результат решения уравнения *query\_time\_in\_millis*/*query\_total* и будет средним временем для каждого запроса.

### Средства для мониторинга Elasticsearch

Для выполнения ежедневного мониторинга показателей Elasticsearch в рабочей среде доступны разные средства. Чтобы собирать данные и представлять подробные сведения в более удобном формате, чем необработанные данные, эти средства обычно используют базовые интерфейсы API Elasticsearch. Наиболее распространенные примеры — [Elasticsearch-Head][], [Bigdesk][], [Kopf][] и [Marvel][].

Elasticsearch Head, Bigdesk и Kopf выполняются как подключаемые модули для программного обеспечения Elasticsearch. Более поздние версии Marvel могут работать независимо, но обеспечение сбора данных и среды размещения предполагает использование платформы [Kibana][]. Преимущество применения Marvel с Kibana заключается в том, что вы можете выполнять мониторинг и анализ проблем в среде Elasticsearch, оставаясь за ее пределами (такие операции могут быть недоступными, когда средства мониторинга работают как часть программного обеспечения Elasticsearch). Например, если операции кластера Elasticsearch несколько раз завершаются сбоем или выполняются очень медленно, средства, работающие как подключаемые модули Elasticsearch, еще больше усложнят процесс мониторинга и диагностики.

На уровне операционной системы для сбора данных о производительности виртуальных машин, на которых размещены узлы Elasticsearch, можно использовать такие средства, как компонент аналитики журналов [Azure Operations Management Suite][] или [система диагностики Azure на портале Azure][]. Другой подход заключается в использовании конвейера данных [Logstash][] для регистрации показателей производительности и данных журналов, сохранении этих сведений на отдельном кластере Elasticsearch (не используйте кластер, на котором выполняется приложение) и визуализации данных с помощью Kibana. Дополнительные сведения см. в статье [Microsoft Azure Diagnostics with ELK][] (Система диагностики Microsoft с использованием ELK).

### Средства для тестирования производительности Elasticsearch

Для тестирования производительности Elasticsearch (или подготовки кластера к тестированию) доступны другие средства, которые предназначены для использования скорее в среде разработки или тестирования, чем в рабочей среде. Распространенный пример — [Apache JMeter][].

JMeter используется для выполнения оценки производительности и других нагрузочных тестов, описанных в связанной документации. Подробные инструкции по настройке и использованию средства JMeter см. в документе Running Performance Tests on Elasticsearch Using JMeter (Запуск тестов производительности Elasticsearch с помощью JMeter).

## Дальнейшие действия
- [Elasticsearch: The Definitive Guide](https://www.elastic.co/guide/en/elasticsearch/guide/master/index.html) (Elasticsearch: полное руководство)

[Apache JMeter]: http://jmeter.apache.org/
[Apache Lucene]: https://lucene.apache.org/
[Автоматическое масштабирование машин в масштабируемом наборе виртуальных машин]: virtual-machines-vmss-walkthrough/
[Шифрование дисков Azure для виртуальных машин IaaS под управлением Windows и Linux (предварительная версия)]: azure-security-disk-encryption/
[балансировщик нагрузки Azure]: load-balancer-overview/
[ExpressRoute]: expressroute-introduction/
[внутреннюю подсистему балансировки нагрузки]: load-balancer-internal-overview/
[статье о размерах виртуальных машин]: virtual-machines-size-specs/

[Memory Requirements]: #memory-requirements
[Требования к памяти]: #memory-requirements
[Требования к сети]: #network-requirements
[Обнаружение узлов]: #node-discovery
[Настройка запросов]: #query-tuning

[A Highly Available Cloud Storage Service with Strong Consistency]: http://blogs.msdn.com/b/windowsazurestorage/archive/2011/11/20/windows-azure-storage-a-highly-available-cloud-storage-service-with-strong-consistency.aspx
[облачный подключаемый модуль Azure]: https://www.elastic.co/blog/azure-cloud-plugin-for-elasticsearch
[подключаемый модуль облачных служб Azure]: https://www.elastic.co/blog/azure-cloud-plugin-for-elasticsearch
[система диагностики Azure на портале Azure]: https://azure.microsoft.com/blog/windows-azure-virtual-machine-monitoring-with-wad-extension/
[Azure Operations Management Suite]: https://www.microsoft.com/server-cloud/operations-management-suite/overview.aspx
[шаблонов быстрого запуска Azure]: https://azure.microsoft.com/documentation/templates/
[Bigdesk]: http://bigdesk.org/
[API-интерфейсы cat]: https://www.elastic.co/guide/en/elasticsearch/reference/1.7/cat.html
[настройте журнал транзакций]: https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-translog.html
[настраиваемой маршрутизации]: https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-routing-field.html
[doc\_values]: https://www.elastic.co/guide/en/elasticsearch/guide/current/doc-values.html
[Elasticsearch]: https://www.elastic.co/products/elasticsearch
[Elasticsearch-Head]: https://mobz.github.io/elasticsearch-head/
[Elasticsearch.Net и NEST]: http://nest.azurewebsites.net/
[модуля моментальных снимков и восстановления в Elasticsearch]: https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html
[Faking Index per User with Aliases]: https://www.elastic.co/guide/en/elasticsearch/guide/current/faking-it.html
[размыкатель цепи данных поля]: https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-fielddata.html#fielddata-circuit-breaker
[принудительного слияния]: https://www.elastic.co/guide/en/elasticsearch/reference/2.1/indices-forcemerge.html
[обмена сведениями]: https://en.wikipedia.org/wiki/Gossip_protocol
[Kibana]: https://www.elastic.co/downloads/kibana
[Kopf]: https://github.com/lmenezes/elasticsearch-kopf
[Logstash]: https://www.elastic.co/products/logstash
[Mapping Explosion]: https://www.elastic.co/blog/found-crash-elasticsearch#mapping-explosion
[Marvel]: https://www.elastic.co/products/marvel
[Microsoft Azure Diagnostics with ELK]: https://github.com/mspnp/semantic-logging/tree/elk
[Monitoring Individual Nodes]: https://www.elastic.co/guide/en/elasticsearch/guide/current/_monitoring_individual_nodes.html#_monitoring_individual_nodes
[nginx]: http://nginx.org/en/
[API Node Client]: https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/node-client.html
[оптимизации]: https://www.elastic.co/guide/en/elasticsearch/reference/1.7/indices-optimize.html
[подключаемый модуль изменений PubNub]: http://www.pubnub.com/blog/quick-start-realtime-geo-replication-for-elasticsearch/
[размыкатель цепи запросов]: https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-fielddata.html#request-circuit-breaker
[Search Guard]: https://github.com/floragunncom/search-guard
[Shield]: https://www.elastic.co/products/shield
[API Transport Client]: https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/transport-client.html
[узлов Tribe]: https://www.elastic.co/blog/tribe-node
[Watcher]: https://www.elastic.co/products/watcher
[Zen]: https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-zen.html

<!---HONumber=AcomDC_0211_2016-->
