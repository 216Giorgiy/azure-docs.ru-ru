---
title: "Запустите кластер Cassandra в Linux в Azure из Node.js"
description: "Как запустить кластер Cassandra под управлением Linux в Виртуальных машинах Azure из приложения Node.js."
services: virtual-machines-linux
documentationcenter: nodejs
author: craigshoemaker
manager: routlaw
editor: 
tags: azure-service-management
ms.assetid: 30de1f29-e97d-492f-ae34-41ec83488de0
ms.service: virtual-machines-linux
ms.workload: infrastructure-services
ms.tgt_pltfrm: vm-linux
ms.devlang: na
ms.topic: article
ms.date: 08/17/2017
ms.author: cshoe
ms.openlocfilehash: 9782df5a5c94169b42d476b0c478fedd3465e3d0
ms.sourcegitcommit: 68aec76e471d677fd9a6333dc60ed098d1072cfc
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 12/18/2017
---
# <a name="run-a-cassandra-cluster-on-linux-in-azure-with-nodejs"></a>Запустите кластер Cassandra в Linux в Azure с помощью Node.js

> [!IMPORTANT] 
> В Azure предлагаются две модели развертывания для создания ресурсов и работы с ними: [модель Resource Manager и классическая модель](../../../resource-manager-deployment-model.md). В этой статье рассматривается использование классической модели развертывания. Для большинства новых развертываний Майкрософт рекомендует использовать модель диспетчера ресурсов. Ознакомьтесь с шаблонами Resource Manager для [Datastax Enterprise](https://azure.microsoft.com/documentation/templates/datastax) и [кластера Spark и Cassandra с дистрибутивом CentOS](https://azure.microsoft.com/documentation/templates/spark-and-cassandra-on-centos/).

## <a name="overview"></a>Обзор
Microsoft Azure — открыть облачная платформа, запущенная Майкрософт и сторонних производителей, включая операционных систем, серверов приложений, обмена сообщениями по промежуточного слоя, а также базы данных SQL и NoSQL из обеих моделей коммерческих с открытым исходным кодом. При создании гибких служб в общедоступных облаках (в том числе Azure) требуется тщательное планирование и разработка архитектуры обоих серверов приложений, а также уровней хранилищ. Архитектура распределенного хранилища Cassandra помогает создать системы высокой доступности, которые отказоустойчивы к сбоям кластеров. Cassandra является шкалу облачных баз данных NoSQL температуру cassandra.apache.org Фондом Apache программного обеспечения. Cassandra написан на языке Java. Поэтому он выполняется как на платформах Windows и Linux.

Темой этой статьи — Показать Cassandra развертывания на Ubuntu как кластер с одной и нескольких центрах данных, использующий виртуальные машины Azure и виртуальных сетей. Развертывание кластера для оптимизации рабочих нагрузок выходит за рамки этой статьи, поскольку для этого требуются конфигурация узла с несколькими дисками, соответствующая кольцевая топология и моделирование данных для выполнения требований к репликации, согласованности данных, пропускной способности и высокой доступности.

В этой статье рассматривается фундаментальный подход, который позволяет показать все компоненты, участвующие в создании сравнимого с кластером Cassandra объекта Docker, Chef или Puppet, который значительно упрощает развертывание инфраструктуры.  

## <a name="the-deployment-models"></a>Модели развертывания
Сеть Microsoft Azure обеспечивает развертывание изолированных частных кластеров, доступ к которым может ограничиваться из соображений сетевой безопасности.  Поскольку в этой статье об отображении развертывания Cassandra на базовом уровне, он не сосредоточиться на уровень согласованности и конструкция оптимальной хранилища для пропускной способности. Ниже приведен список сетевые требования для гипотетического кластера:

* Внешние системы не могут получить доступ к базе данных Cassandra из службы Azure или за пределами.
* Кластер Cassandra должен размещаться за подсистемой балансировки нагрузки для экономии трафика.
* Узлы Cassandra должны развертываться в двух группах в каждом центре обработки данных для улучшенной доступности кластера.
* Кластер следует заблокировать, чтобы прямой доступ к базе данных получила только ферма серверов приложений.
* Не допускаются открытые сетевые конечные точки, отличные от SSH.
* Каждый узел Cassandra должен иметь фиксированный внутренний IP-адрес.

Решение Cassandra можно развернуть в одном или нескольких регионах Azure в зависимости от характера распределения рабочей нагрузки. Развертывание в нескольких регионах модели можно использовать для предоставления пользователям ближе конкретного географического объекта через одну и ту же Cassandra инфраструктуру. При репликации встроенных узлов Cassandra отслеживается синхронизация операций записи в несколько источников, изначально выполненных из нескольких центров обработки данных, и предоставляется согласованное представление данных для приложений. Развертывание в нескольких регионах также позволяет устранить риски, связанные с простоями ряда служб Azure. В Cassandra пройтись согласованности и топологию репликации помогает удовлетворения разнообразными потребностями RPO приложения.

### <a name="single-region-deployment"></a>Развертывание в одной области
Давайте Возьмите развертывание в одном регионе и сбора данных при создании модели регионов. Виртуальные сети Azure используется для создания изолированных подсетей, так что можно выполнить требования безопасности сети, упомянутых выше.  Процесс, описанный в создании развертывание в одном регионе использует Ubuntu 14.04 LTS и Cassandra 2.08. Тем не менее процесс, легко может быть использована для других вариантов Linux. Ниже приведены некоторые системные характеристики развертывания в одном регионе.  

**Высокий уровень доступности.** Узлы Cassandra, изображенные на рис. 1, развертываются в двух группах доступности. Таким образом узлы распределяются между несколькими доменами сбоя для обеспечения высокой доступности. Виртуальные машины с заметками о каждой группе доступности сопоставляются с 2 доменами сбоев. Azure использует концепцию домен сбоя для управления незапланированных простоев (например сбоев оборудования или программного обеспечения). Понятие обновление домена (например, узла или гостевой ОС установка исправлений и обновлений, обновлений приложений) используется для управления запланированных простоев. Дополнительную информацию о роли доменов сбоя и обновления в обеспечении высокого уровня доступности см. в статье [Аварийное восстановление и высокая доступность для приложений на платформе Azure](http://msdn.microsoft.com/library/dn251004.aspx).

![Развертывание в одной области](./media/cassandra-nodejs/cassandra-linux1.png)

Рисунок 1. Развертывание в одном регионе

Обратите внимание, что на момент написания этой статьи в службе Azure запрещено явное сопоставление группы виртуальных машин с определенным доменом сбоя. Следовательно, даже в модели развертывания, показанной на рисунке 1, статистически вероятно, что все виртуальные машины можно сопоставить с двумя доменами сбоя вместо четырех.

**Трафик Thrift с балансировкой нагрузки.** Клиентские библиотеки Thrift на веб-сервере подключаются к кластеру через внутреннюю подсистему балансировки нагрузки. Для этого требуется добавить внутреннюю подсистему балансировки нагрузки в подсеть "данных" (см. рисунок 1) в контексте облачной службы, в которой размещен кластер Cassandra. После определения внутренней подсистемы балансировки нагрузки на каждый узел необходимо добавить конечную точку со сбалансированной нагрузкой с заметками о наборе со сбалансированной нагрузкой и ранее определенным именем подсистемы балансировки нагрузки. Дополнительные сведения см. в статье [Обзор внутренней подсистемы балансировки нагрузки](../../../load-balancer/load-balancer-internal-overview.md).

**Начальные значения кластера:** важно выбрать большинства узлов высокой доступности для начальные значения, как новые узлы взаимодействовать с узлами начальное значение для обнаружения топологии кластера. Один узел из каждой группы доступности обозначается в качестве узлов начальных значений во избежание возникновения единой точки отказа.

**Коэффициент репликации и уровень согласованности.** Функции обеспечения высокой доступности и надежности данных, встроенные в решение Cassandra, имеют коэффициент репликации (количество копий каждой строки, хранящихся в кластере) и уровень согласованности (число реплик для чтения или записи перед возвратом результата вызывающему объекту). Коэффициент репликации указывается во время создания ПРОСТРАНСТВА КЛЮЧЕЙ (аналогично реляционной базе данных), в то время как уровень согласованности задается при подаче запроса CRUD. Дополнительные сведения о согласованности и формуле для вычисления кворума см. в документации к Cassandra в разделе [Configuring data consistency](http://www.datastax.com/documentation/cassandra/2.0/cassandra/dml/dml_config_consistency_c.html) (Настройка согласованности данных).

Cassandra поддерживает два типа моделей целостности данных — и согласованности окончательной; Коэффициент репликации и уровень согласованности вместе определите данные согласованное сразу операции записи после завершения или согласованным. Например указав КВОРУМА, как уровень согласованности всегда гарантирует данных согласованности при любом уровне согласованности, меньше числа реплик для записи, необходимыми для достижения КВОРУМА (например, один) приводит данных согласованным.

Показанный выше кластер из 8 узлов с коэффициентом репликации 3 и КВОРУМОМ (2 узла считываются или записываются для согласованности) на уровне согласованности чтения и записи теоретически может перенести потерю не более 1 узла на каждую группу репликации, прежде чем приложение заметит ошибку. Предполагается, что все пространства ключей содержат сбалансированные запросы на чтение и запись.  Ниже приведены параметры, используемые для развернутых кластеров.

Конфигурация кластера Cassandra, развернутого в одном регионе

| Параметр кластера | Значение | Примечания |
| --- | --- | --- |
| Число узлов (N) |8 |Общее количество узлов в кластере |
| Коэффициент репликации (RF) |3 |Число реплик данной строки |
| Уровень согласованности (запись) |QUORUM [(RF/2) +1) = 2] Результат формулы округляется |Записывает максимум 2 реплики перед отправкой ответа вызывающему объекту; 3-я реплика записывается для обеспечения окончательной согласованности. |
| Уровень согласованности (чтение) |QUORUM [(RF/2) +1= 2] Результат формулы округляется |Считывается 2 реплики перед отправкой ответа вызывающему объекту. |
| Стратегия репликации |Дополнительную информацию о NetworkTopologyStrategy см. в разделе [Data Replication](http://www.datastax.com/documentation/cassandra/2.0/cassandra/architecture/architectureDataDistributeReplication_c.html) (Репликация данных) в документации по Cassandra |Понимает топологию развертывания и помещает реплики на узлы таким образом, чтобы все реплики не попадут в одну стойку. |
| Информатор |См. в разделе GossipingPropertyFileSnitch [коммутаторы](http://www.datastax.com/documentation/cassandra/2.0/cassandra/architecture/architectureSnitchesAbout_c.html) в документации Cassandra Дополнительные сведения |NetworkTopologyStrategy использует концепцию информатора для получения данных о топологии. GossipingPropertyFileSnitch обеспечивает лучший контроль при сопоставлении каждого узла с центром обработки данных и стойкой. Затем кластер использует полученные данные для распространения этих сведений. Процесс выполняется гораздо проще в параметре динамического IP-адреса относительно PropertyFileSnitch |

**Azure рекомендации для кластера Cassandra:** возможности виртуальных машин Microsoft Azure использует хранилища больших двоичных объектов Azure для сохранения состояния диска; Хранилища Azure сохраняет три реплики каждого диска для обеспечения высокой надежности. Это означает, что каждая строка данных, вставляемых в таблицу Cassandra уже хранится в трех репликах. Поэтому согласованность данных уже работают даже если коэффициент репликации (RF)-1. Основная проблема с коэффициентом репликации, 1 — что приложение работает время простоя, даже в случае сбоя одного узла Cassandra. Тем не менее если узел не работает для проблемы (например оборудования, программного обеспечения сбои системы), распознаваемые Azure Fabric Controller, инициализированных на новый узел в его месте, используя же дисков хранилища. Подготовка нового узла для замены старого может занять несколько минут.  Аналогичным образом, для действий по плановому обслуживанию, например внесения изменений в гостевую ОС или приложения либо обновления Cassandra, контроллер структуры Azure развертывает обновления узлов в кластере.  Кроме того, обновления могут развертываться одновременно на нескольких узлах, поэтому в работе кластера могут возникать небольшие простои в нескольких разделах. Однако данные не потеряны из-за встроенной избыточности хранилища Azure.  

Для развертывания в Azure, которой не требуется высокий уровень доступности систем (например около 99,9 это эквивалентно 8.76 часов/год; см. раздел [высокий уровень доступности](http://en.wikipedia.org/wiki/High_availability) подробности) можно запускать с RF = 1 и уровень согласованности = один.  Для приложений с требованиями высокой доступности, RF = 3 и уровень согласованности = КВОРУМА допускает время простоя одного из узлов реплик. RF = 1 в традиционной развертываниях (например локальной) не может использоваться из-за возможной потери данных из подобных сбоев диска.   

## <a name="multi-region-deployment"></a>Развертывание в нескольких регионах
Cassandra элемента данных center поддерживающей репликации и согласованности приведенной выше модели помогает развертывание в нескольких регионах без необходимости для любого внешнего средства. Это отличается от традиционных реляционных баз данных, где Настройка зеркального отображения базы данных для операций с несколькими хозяевами записи могут быть сложными. Cassandra в нескольких регионах установке могут помочь от использования сценариев, включая сценарии:

**Развертывание с учетом расположения.** На работу мультитенантных приложений с явным сопоставлением пользователей клиентов с регионами могут положительно повлиять низкие задержки в работе кластера, развернутого в нескольких регионах. Например, обучения систем управления для образовательных учреждений можно развернуть кластер распределенного в регионах Восток США и Запад США для предоставления соответствующих необязательно для транзакций, а также аналитики. Данные могут отличаться локальной согласованностью во время операций чтения и записи, а также окончательной согласованностью в обоих регионах. Имеются другие примеры, такие как распределение носителей и электронная торговля. Так, любые сферы, пользовательская база в которых сосредоточена в одном регионе, — хороший пример использования данной модели развертывания.

**Высокая доступность.** Избыточность — ключевой фактор обеспечения высокой доступности программного и аппаратного обеспечения. Дополнительные сведения см. в статье Building Reliable Cloud Systems on Microsoft Azure (Создание надежных облачных систем в Microsoft Azure). В Microsoft Azure единственным надежным способом обеспечения истинной избыточности является развертывание кластера в нескольких регионах. Приложения можно развернуть в режиме "активный-активный" или"активный-пассивный", и если один из регионов не работает, диспетчер трафика Azure может перенаправить трафик в активный регион.  Если кластер развернут в одном регионе, а уровень доступности составляет 99,9, то в случае развертывания в двух регионах можно обеспечить доступность на уровне 99,9999, которая вычисляется по следующей формуле: (1 – (1 – 0,999)*(1–0,999))*100). Дополнительные сведения см. в упомянутой выше статье.

**Аварийное восстановление.** Если кластер Cassandra, развернутый в нескольких регионах, разработан должным образом, он может выдерживать катастрофические сбои центра обработки данных. Если работа в одном регионе невозможна, приложение, развернутое в других регионах, может начать обслуживать пользователей. Как и в любых других примерах реализации непрерывности бизнес-процессов, приложение должно быть отказоустойчивым к потере некоторых данных в асинхронном конвейере. Тем не менее решение Cassandra выполняет восстановление намного быстрее по сравнению с традиционными процессами восстановления баз данных. На рисунке 2 показана типичная модель развертывания в нескольких регионах с восемью узлами в каждом из них. Обе области являются зеркальным отображением друг от друга для одной из симметрии; схемы в реальном мире зависят от типа рабочей нагрузки (например транзакций или аналитическая), RPO, RTO, согласованность данных и требования к доступности.

![Развертывание в нескольких регионах](./media/cassandra-nodejs/cassandra-linux2.png)

Рисунок 2. Развертывание Cassandra в нескольких регионах

### <a name="network-integration"></a>Сетевая интеграция
Наборы виртуальных машин, развернутых в частных сетях, которые расположены в двух регионах, обменивается данными друг с другом через VPN-туннель. VPN-туннель соединяет два шлюза программного обеспечения, подготовленные в процессе развертывания сети. Оба региона имеют сходную архитектуру сети в терминах "веб-подсетей" и "подсетей данных"; сети Azure позволяют создавать необходимое количество подсетей и применять списки ACL, если это требуется из соображений сетевой безопасности. При проектировании топологии кластера, как задержка связи центра данных и экономический эффект от сетевого трафика необходимо рассматривать.

### <a name="data-consistency-for-multi-data-center-deployment"></a>Согласованность данных при развертывании в нескольких центрах обработки данных
При распределенных развертываниях необходимо учитывать влияние топологии кластера на пропускную способность и высокий уровень доступности. Коэффициент репликации и уровень согласованности необходимо выбрать таким образом, чтобы кворум не зависел от доступности во всех центрах обработки данных.
Для системы, которому требуется высокий уровень согласованности LOCAL_QUORUM для уровня согласованности (для операций чтения и записи) гарантирует, локальных операций чтения и записи выполняются из локальных узлов хотя данные реплицируются асинхронно центров обработки данных на удаленном.  В таблице 2 суммируются сведения о конфигурации кластера, развернутого в нескольких регионах (он описан ниже).

**Конфигурация кластера Cassandra, развернутого в двух регионах**

| Параметр кластера | Значение | Примечания |
| --- | --- | --- |
| Число узлов (N) |8 + 8 |Общее количество узлов в кластере |
| Коэффициент репликации (RF) |3 |Число реплик данной строки |
| Уровень согласованности (запись) |LOCAL_QUORUM [(sum(RF)/2) +1) = 4] Результат формулы округляется |2 узлов записывается в центре обработки данных первого синхронно. Дополнительные 2 узлов, необходимое для кворума асинхронно записывается во второй центр обработки данных. |
| Уровень согласованности (чтение) |LOCAL_QUORUM ((RF/2) +1) = 2 Результат формулы округляется |Запросы на чтение удовлетворяются только из одного региона. 2 узла считываются перед обратной отправкой ответа клиенту. |
| Стратегия репликации |Дополнительную информацию о NetworkTopologyStrategy см. в разделе [Data Replication](http://www.datastax.com/documentation/cassandra/2.0/cassandra/architecture/architectureDataDistributeReplication_c.html) (Репликация данных) в документации по Cassandra |Понимает топологию развертывания и помещает реплики на узлы таким образом, чтобы все реплики не попадут в одну стойку. |
| Информатор |Дополнительную информацию о GossipingPropertyFileSnitch см. в разделе [Snitches](http://www.datastax.com/documentation/cassandra/2.0/cassandra/architecture/architectureSnitchesAbout_c.html) (Информаторы) в документации по Cassandra |NetworkTopologyStrategy использует концепцию информатора для получения данных о топологии. GossipingPropertyFileSnitch обеспечивает лучший контроль при сопоставлении каждого узла с центром обработки данных и стойкой. Затем кластер использует полученные данные для распространения этих сведений. Процесс выполняется гораздо проще в параметре динамического IP-адреса относительно PropertyFileSnitch |

## <a name="the-software-configuration"></a>КОНФИГУРАЦИЯ ПРОГРАММНОГО ОБЕСПЕЧЕНИЯ
Во время развертывания используются следующие версии программного обеспечения.

<table>
<tr><th>Программное обеспечение</th><th>Источник</th><th>Version (версия)</th></tr>
<tr><td>JRE    </td><td>[JRE 8](http://www.oracle.com/technetwork/java/javase/downloads/server-jre8-downloads-2133154.html) </td><td>8U5</td></tr>
<tr><td>JNA    </td><td>[JNA](https://github.com/twall/jna) </td><td> 3.2.7</td></tr>
<tr><td>Cassandra</td><td>[Apache Cassandra 2.0.8](http://www.apache.org/dist/cassandra/2.0.8/apache-cassandra-2.0.8-bin.tar.gz)</td><td> 2.0.8</td></tr>
<tr><td>Ubuntu    </td><td>[Microsoft Azure](https://azure.microsoft.com/) </td><td>14.04 LTS</td></tr>
</table>

Необходимо вручную принять лицензии Oracle, при загрузке JRE. Таким образом чтобы упростить развертывание, загрузите все необходимое программное обеспечение на рабочий стол. Затем отправьте его Ubuntu образ шаблона для создания в качестве основы для развертывания кластера.

Загрузите выше программное обеспечение в каталог хорошо известных загрузки (например %TEMP%/downloads в Windows или ~/Downloads на большинстве ОС Linux или Mac) на локальном компьютере.

### <a name="create-ubuntu-vm"></a>СОЗДАНИЕ ВИРТУАЛЬНОЙ МАШИНЫ UBUNTU
На этом шаге процесса создании Ubuntu изображения необходимое программное обеспечение, чтобы изображение может быть повторно использован для инициализации несколько узлов Cassandra.  

#### <a name="step-1-generate-ssh-key-pair"></a>Шаг 1. Создание пары ключей SSH
Azure требует открытый ключ X509, который во время подготовки был закодирован в формат PEM или DER. Создайте пару,состоящую из открытого и закрытого ключей, следуя инструкциям, приведенным в разделе "Использование SSH с Linux в Azure". Если вы планируете использовать putty.exe как клиент SSH на Windows или Linux, то необходимо преобразовать кодировке PEM закрытый ключ RSA PPK формат, используя puttygen.exe. Соответствующие инструкции можно найти в веб-странице выше.

#### <a name="step-2-create-ubuntu-template-vm"></a>Шаг 2. Создание виртуальной машины шаблонов Ubuntu
Чтобы создать шаблон виртуальной Машины, войдите на портал Azure и использовать следующую последовательность: щелкните NEW, ВЫЧИСЛЕНИЙ, ВИРТУАЛЬНОЙ МАШИНЫ, из КОЛЛЕКЦИИ, UBUNTU, Ubuntu Server 14.04 LTS, а затем щелкните стрелку вправо. Если вам нужен учебник по созданию виртуальной машины в Linux, см. разделе "Создание виртуальной машины под управлением Linux".

На экране № 1 ("Конфигурация виртуальной машины") введите следующие сведения.

<table>
<tr><th>ИМЯ ПОЛЯ              </td><td>       ЗНАЧЕНИЕ ПОЛЯ               </td><td>         ПРИМЕЧАНИЯ                </td><tr>
<tr><td>ДАТА ВЫПУСКА ВЕРСИИ    </td><td> Выберите дату в раскрывающемся списке.</td><td></td><tr>
<tr><td>ИМЯ ВИРТУАЛЬНОЙ МАШИНЫ    </td><td> cass-template                   </td><td> Это имя узла виртуальной машины </td><tr>
<tr><td>УРОВЕНЬ                     </td><td> STANDARD                           </td><td> Оставьте значение по умолчанию.              </td><tr>
<tr><td>РАЗМЕР                     </td><td> A1                              </td><td>Выберите виртуальную машину с учетом потребностей к вводу-выводу; для этого оставьте значение по умолчанию. </td><tr>
<tr><td> НОВОЕ ИМЯ ПОЛЬЗОВАТЕЛЯ             </td><td> localadmin                       </td><td> "admin" — это зарезервированное имя пользователя в Ubuntu 12.xx и более поздних версий.</td><tr>
<tr><td> ПРОВЕРКА ПОДЛИННОСТИ         </td><td> Щелкните флажок                 </td><td>Установите флажок, если хотите безопасно пользоваться ключом SSH </td><tr>
<tr><td> СЕРТИФИКАТ             </td><td> имя файла сертификата открытого ключа </td><td> Используйте ранее созданный открытый ключ.</td><tr>
<tr><td> Новый пароль    </td><td> надежный пароль </td><td> </td><tr>
<tr><td> Подтверждение пароля    </td><td> надежный пароль </td><td></td><tr>
</table>

На экране № 2 ("Конфигурация виртуальной машины") введите следующие сведения.

<table>
<tr><th>ИМЯ ПОЛЯ             </th><th> ЗНАЧЕНИЕ ПОЛЯ                       </th><th> ПРИМЕЧАНИЯ                                 </th></tr>
<tr><td> ОБЛАЧНАЯ СЛУЖБА    </td><td> Создать новую облачную службу.    </td><td>Облачная служба — это контейнер вычислительных ресурсов, таки как виртуальные машины.</td></tr>
<tr><td> DNS-ИМЯ ОБЛАЧНОЙ СЛУЖБЫ    </td><td>ubuntu-template.cloudapp.net    </td><td>Назначьте имя независимой подсистеме балансировки нагрузки</td></tr>
<tr><td> РЕГИОН/ТЕРРИТОРИАЛЬНАЯ ГРУППА/ВИРТУАЛЬНАЯ СЕТЬ </td><td>    Запад США    </td><td> Выберите регион, из которого веб-приложения осуществляют доступ к кластеру Cassandra</td></tr>
<tr><td>УЧЕТНАЯ ЗАПИСЬ ХРАНЕНИЯ </td><td>    Используйте значение по умолчанию.    </td><td>Используйте учетную запись хранения по умолчанию или учетную запись хранения, предварительно созданную в определенном регионе.</td></tr>
<tr><td>группа доступности; </td><td>    Нет </td><td>    Оставьте пустым.</td></tr>
<tr><td>КОНЕЧНЫЕ ТОЧКИ    </td><td>Используйте значение по умолчанию. </td><td>    Используйте конфигурацию SSH по умолчанию. </td></tr>
</table>

Нажмите кнопку со стрелкой вправо, оставьте настройки по умолчанию на экране #3. Нажмите кнопку «проверить», чтобы завершить процесс подготовки виртуальной Машины. Через несколько минут виртуальная машина с именем "ubuntu-template" получит состояние "выполняется".

### <a name="install-the-necessary-software"></a>УСТАНОВКА НЕОБХОДИМОГО ПРОГРАММНОГО ОБЕСПЕЧЕНИЯ
#### <a name="step-1-upload-tarballs"></a>Шаг 1. Передача архивов в формате TAR
Используя scp или pscp, скопируйте ранее скачанное программное обеспечение в каталог ~/downloads, используя команду в следующем формате:

##### <a name="pscp-server-jre-8u5-linux-x64targz-localadminhk-cas-templatecloudappnethomelocaladmindownloadsserver-jre-8u5-linux-x64targz"></a>pscp server-jre-8u5-linux-x64.tar.gz localadmin@hk-cas-template.cloudapp.net:/home/localadmin/downloads/server-jre-8u5-linux-x64.tar.gz
Снова выполните приведенную выше команду для JRE и битов Cassandra.

#### <a name="step-2-prepare-the-directory-structure-and-extract-the-archives"></a>Шаг 2. Подготовка структуры каталогов и извлечение архивов
Войдите на виртуальную машину, создайте структуру каталогов и извлеките программное обеспечение в качестве суперпользователя с помощью указанного ниже сценария bash:

    #!/bin/bash
    CASS_INSTALL_DIR="/opt/cassandra"
    JRE_INSTALL_DIR="/opt/java"
    CASS_DATA_DIR="/var/lib/cassandra"
    CASS_LOG_DIR="/var/log/cassandra"
    DOWNLOADS_DIR="~/downloads"
    JRE_TARBALL="server-jre-8u5-linux-x64.tar.gz"
    CASS_TARBALL="apache-cassandra-2.0.8-bin.tar.gz"
    SVC_USER="localadmin"

    RESET_ERROR=1
    MKDIR_ERROR=2

    reset_installation ()
    {
       rm -rf $CASS_INSTALL_DIR 2> /dev/null
       rm -rf $JRE_INSTALL_DIR 2> /dev/null
       rm -rf $CASS_DATA_DIR 2> /dev/null
       rm -rf $CASS_LOG_DIR 2> /dev/null
    }
    make_dir ()
    {
       if [ -z "$1" ]
       then
          echo "make_dir: invalid directory name"
          exit $MKDIR_ERROR
       fi

       if [ -d "$1" ]
       then
          echo "make_dir: directory already exists"
          exit $MKDIR_ERROR
       fi

       mkdir $1 2>/dev/null
       if [ $? != 0 ]
       then
          echo "directory creation failed"
          exit $MKDIR_ERROR
       fi
    }

    unzip()
    {
       if [ $# == 2 ]
       then
          tar xzf $1 -C $2
       else
          echo "archive error"
       fi

    }

    if [ -n "$1" ]
    then
       SVC_USER=$1
    fi

    reset_installation
    make_dir $CASS_INSTALL_DIR
    make_dir $JRE_INSTALL_DIR
    make_dir $CASS_DATA_DIR
    make_dir $CASS_LOG_DIR

    #unzip JRE and Cassandra
    unzip $HOME/downloads/$JRE_TARBALL $JRE_INSTALL_DIR
    unzip $HOME/downloads/$CASS_TARBALL $CASS_INSTALL_DIR

    #Change the ownership to the service credentials

    chown -R $SVC_USER:$GROUP $CASS_DATA_DIR
    chown -R $SVC_USER:$GROUP $CASS_LOG_DIR
    echo "edit /etc/profile to add JRE to the PATH"
    echo "installation is complete"


Если вставить этот сценарий в окно vim, не забудьте удалить символ возврата каретки (‘\r”) с помощью следующей команды:

    tr -d '\r' <infile.sh >outfile.sh

#### <a name="step-3-edit-etcprofile"></a>Шаг 3. Изменение etc/profile
Добавьте следующие данные в конце.

    JAVA_HOME=/opt/java/jdk1.8.0_05
    CASS_HOME= /opt/cassandra/apache-cassandra-2.0.8
    PATH=$PATH:$HOME/bin:$JAVA_HOME/bin:$CASS_HOME/bin
    export JAVA_HOME
    export CASS_HOME
    export PATH

#### <a name="step-4-install-jna-for-production-systems"></a>Шаг 4. Установка JNA для производственных систем
Используйте следующую последовательность команд: следующая команда устанавливает jna 3.2.7.jar и jna платформы 3.2.7.jar для /usr/share.java каталог sudo apt-get установите libjna java

Создайте символические ссылки в каталоге $CASS_HOME/lib, чтобы сценарий запуска Cassandra смог найти эти JAR-файлы.

    ln -s /usr/share/java/jna-3.2.7.jar $CASS_HOME/lib/jna.jar

    ln -s /usr/share/java/jna-platform-3.2.7.jar $CASS_HOME/lib/jna-platform.jar

#### <a name="step-5-configure-cassandrayaml"></a>Шаг 5. Настройка cassandra.yaml
Измените cassandra.yaml на каждой виртуальной Машины в соответствии с конфигурацией, необходимые для всех виртуальных машин [настроить конфигурацию во время подготовки фактическое]:

<table>
<tr><th>Имя поля   </th><th> Значение  </th><th>    Примечания </th></tr>
<tr><td>cluster_name </td><td>    "CustomerService"    </td><td> Используйте имя, которое отражает развертывание.</td></tr>
<tr><td>listen_address    </td><td>[Оставьте пустым.]    </td><td> Удалите "localhost". </td></tr>
<tr><td>rpc_addres   </td><td>[Оставьте пустым.]    </td><td> Удалите "localhost". </td></tr>
<tr><td>начальные значения    </td><td>"10.1.2.4, 10.1.2.6, 10.1.2.8"    </td><td>Список всех IP-адресов, которые обозначены как начальные значения.</td></tr>
<tr><td>endpoint_snitch </td><td> org.apache.cassandra.locator.GossipingPropertyFileSnitch </td><td> Этот элемент используется NetworkTopologyStrateg для обозначения центра обработки данных и стойки виртуальной машины.</td></tr>
</table>

#### <a name="step-6-capture-the-vm-image"></a>Шаг 6. Запись образа виртуальной машины
Войдите на виртуальную машину, используя ранее созданные имя узла (hk-cas-template.cloudapp.net) и закрытый ключ SSH. Дополнительные сведения о том, как войти в систему с помощью команды ssh или средства putty.exe, см. в разделе "Использование SSH с Linux в Azure".

Выполните действия в следующей последовательности, чтобы записать образ.

##### <a name="1-deprovision"></a>1. Отзыв
Выполните команду "sudo waagent -deprovision + user", чтобы удалить сведения о конкретных экземплярах виртуальной машины. Дополнительную информацию о записи образа и использовании его в качестве шаблона см. в статье [Запись классической виртуальной машины Linux в виде образа](capture-image.md).

##### <a name="2-shut-down-the-vm"></a>2: завершение работы виртуальной Машины
Убедитесь, что виртуальная машина выделена, и щелкните ссылку "ЗАВЕРШЕНИЕ РАБОТЫ" в нижней панели команд.

##### <a name="3-capture-the-image"></a>3. Запись образа
Убедитесь, что виртуальная машина выделена, и щелкните ссылку "ЗАПИСЬ" в нижней панели команд. Далее экрана, предоставьте имя ОБРАЗА (например hk-cas-2-08-ub-14-04-2014071), соответствующее описание ИЗОБРАЖЕНИЯ, и нажмите кнопку «Проверить» пометить для завершения процесса ОТСЛЕЖИВАНИЯ.

Этот процесс занимает несколько секунд и изображения должен быть доступен в разделе "Мои ИЗОБРАЖЕНИЯ" коллекции образов. После успешного образа исходной виртуальной Машины автоматически удаляется. 

## <a name="single-region-deployment-process"></a>Процесс развертывания в одном регионе
**Шаг 1. Создание виртуальной сети.** Войдите на портал Azure и создайте классическую виртуальную сеть с атрибутами, приведенными в следующей таблице. Подробное описание процесса см. в статье [Создание (классической) виртуальной сети с помощью портала предварительной версии Azure](../../../virtual-network/virtual-networks-create-vnet-classic-pportal.md).      

<table>
<tr><th>Имя атрибута виртуальной машины</th><th>Значение</th><th>Примечания</th></tr>
<tr><td>ИМЯ</td><td>vnet-cass-west-us</td><td></td></tr>
<tr><td>Регион</td><td>Запад США</td><td></td></tr>
<tr><td>DNS-серверы</td><td>Нет</td><td>Пропустите этот атрибут, поскольку мы не используем DNS-сервер</td></tr>
<tr><td>Пространство адресов</td><td>10.1.0.0/16</td><td></td></tr>    
<tr><td>Начальный IP-адрес</td><td>10.1.0.0</td><td></td></tr>    
<tr><td>CIDR </td><td>/16 (65531)</td><td></td></tr>
</table>

Добавьте следующие подсети.

<table>
<tr><th>ИМЯ</th><th>Начальный IP-адрес</th><th>CIDR</th><th>Примечания</th></tr>
<tr><td>web</td><td>10.1.1.0</td><td>/24 (251)</td><td>Подсеть для веб-фермы</td></tr>
<tr><td>data</td><td>10.1.2.0</td><td>/24 (251)</td><td>Подсеть для узлов базы данных</td></tr>
</table>

Подсети данных и веб-подсети можно защитить с помощью сетевых групп безопасности, которые не рассматриваются в этой статье.  

**Шаг 2: Подготовка виртуальных машин** используя образ, созданный ранее, можно создать следующие виртуальные машины на сервере облака «hk-c-svc запад» и привязывать их к подсетей, как показано ниже:

<table>
<tr><th>Имя компьютера    </th><th>Подсеть    </th><th>IP-адрес    </th><th>Группа доступности</th><th>Контроллер домена/стойка</th><th>Начальное значение?</th></tr>
<tr><td>hk-c1-west-us    </td><td>data    </td><td>10.1.2.4    </td><td>hk-c-aset-1    </td><td>dc =WESTUS rack =rack1 </td><td>Yes</td></tr>
<tr><td>hk-c2-west-us    </td><td>data    </td><td>10.1.2.5    </td><td>hk-c-aset-1    </td><td>dc =WESTUS rack =rack1    </td><td>Нет  </td></tr>
<tr><td>hk-c3-west-us    </td><td>data    </td><td>10.1.2.6    </td><td>hk-c-aset-1    </td><td>dc =WESTUS rack =rack2    </td><td>Yes</td></tr>
<tr><td>hk-c4-west-us    </td><td>data    </td><td>10.1.2.7    </td><td>hk-c-aset-1    </td><td>dc =WESTUS rack =rack2    </td><td>Нет  </td></tr>
<tr><td>hk-c5-west-us    </td><td>data    </td><td>10.1.2.8    </td><td>hk-c-aset-2    </td><td>dc =WESTUS rack =rack3    </td><td>Yes</td></tr>
<tr><td>hk-c6-west-us    </td><td>data    </td><td>10.1.2.9    </td><td>hk-c-aset-2    </td><td>dc =WESTUS rack =rack3    </td><td>Нет  </td></tr>
<tr><td>hk-c7-west-us    </td><td>data    </td><td>10.1.2.10    </td><td>hk-c-aset-2    </td><td>dc =WESTUS rack =rack4    </td><td>Yes</td></tr>
<tr><td>hk-c8-west-us    </td><td>data    </td><td>10.1.2.11    </td><td>hk-c-aset-2    </td><td>dc =WESTUS rack =rack4    </td><td>Нет  </td></tr>
<tr><td>hk-w1-west-us    </td><td>web    </td><td>10.1.1.4    </td><td>hk-w-aset-1    </td><td>                       </td><td>Н/Д</td></tr>
<tr><td>hk-w2-west-us    </td><td>web    </td><td>10.1.1.5    </td><td>hk-w-aset-1    </td><td>                       </td><td>Н/Д</td></tr>
</table>

Для создания приведенного выше списка виртуальных машин необходимо выполнить следующие действия.

1. Создайте пустую облачную службу в определенном регионе.
2. Создайте виртуальную машину из ранее записанного образа и подключите ее к ранее созданной виртуальной сети; повторите это действие для всех виртуальных машин.
3. Добавьте внутреннюю подсистему балансировки нагрузки в облачную службу и подключите ее к подсети "данных".
4. Для каждой ранее созданной виртуальной машины добавьте конечную точку с балансировкой нагрузки для экономии трафика через набор балансировки нагрузки, подключенный к ранее созданной внутренней подсистеме балансировки нагрузки.

Описанный выше процесс может выполняться с помощью портала Azure; используется компьютером Windows (используйте ВМ в Azure, если нет доступа к машине), используйте следующий сценарий PowerShell автоматически подготовить все 8 виртуальные машины.

**Список 1. Сценарий PowerShell для подготовки виртуальных машин**

        #Tested with Azure Powershell - November 2014
        #This powershell script deployes a number of VMs from an existing image inside an Azure region
        #Import your Azure subscription into the current Powershell session before proceeding
        #The process: 1. create Azure Storage account, 2. create virtual network, 3.create the VM template, 2. crate a list of VMs from the template

        #fundamental variables - change these to reflect your subscription
        $country="us"; $region="west"; $vnetName = "your_vnet_name";$storageAccount="your_storage_account"
        $numVMs=8;$prefix = "hk-cass";$ilbIP="your_ilb_ip"
        $subscriptionName = "Azure_subscription_name";
        $vmSize="ExtraSmall"; $imageName="your_linux_image_name"
        $ilbName="ThriftInternalLB"; $thriftEndPoint="ThriftEndPoint"

        #generated variables
        $serviceName = "$prefix-svc-$region-$country"; $azureRegion = "$region $country"

        $vmNames = @()
        for ($i=0; $i -lt $numVMs; $i++)
        {
           $vmNames+=("$prefix-vm"+($i+1) + "-$region-$country" );
        }

        #select an Azure subscription already imported into Powershell session
        Select-AzureSubscription -SubscriptionName $subscriptionName -Current
        Set-AzureSubscription -SubscriptionName $subscriptionName -CurrentStorageAccountName $storageAccount

        #create an empty cloud service
        New-AzureService -ServiceName $serviceName -Label "hkcass$region" -Location $azureRegion
        Write-Host "Created $serviceName"

        $VMList= @()   # stores the list of azure vm configuration objects
        #create the list of VMs
        foreach($vmName in $vmNames)
        {
           $VMList += New-AzureVMConfig -Name $vmName -InstanceSize ExtraSmall -ImageName $imageName |
           Add-AzureProvisioningConfig -Linux -LinuxUser "localadmin" -Password "Local123" |
           Set-AzureSubnet "data"
        }

        New-AzureVM -ServiceName $serviceName -VNetName $vnetName -VMs $VMList

        #Create internal load balancer
        Add-AzureInternalLoadBalancer -ServiceName $serviceName -InternalLoadBalancerName $ilbName -SubnetName "data" -StaticVNetIPAddress "$ilbIP"
        Write-Host "Created $ilbName"
        #Add add the thrift endpoint to the internal load balancer for all the VMs
        foreach($vmName in $vmNames)
        {
            Get-AzureVM -ServiceName $serviceName -Name $vmName |
                Add-AzureEndpoint -Name $thriftEndPoint -LBSetName "ThriftLBSet" -Protocol tcp -LocalPort 9160 -PublicPort 9160 -ProbePort 9160 -ProbeProtocol tcp -ProbeIntervalInSeconds 10 -InternalLoadBalancerName $ilbName |
                Update-AzureVM

            Write-Host "created $vmName"     
        }

**Шаг 3. Настройка Cassandra на каждой виртуальной машине**

Войдите на виртуальную машину и выполните следующие действия:

* Внесите изменения в $CASS_HOME/conf/cassandra-rackdc.properties, чтобы указать свойства центра обработки данных и стойки.
  
       dc =EASTUS, rack =rack1
* Внесите изменения в cassandra.yaml, чтобы настроить узлы начальных значений, как показано ниже.
  
       Seeds: "10.1.2.4,10.1.2.6,10.1.2.8,10.1.2.10"

**Шаг 4. Запуск виртуальных машин и тестирование кластера**

Войдите на одном из узлов (например hk-c1-Запад us) и выполните следующую команду, чтобы увидеть состояние кластера:

       nodetool –h 10.1.2.4 –p 7199 status

На экране кластера из 8 узлов отобразятся данных, похожие на приведенные ниже.

<table>
<tr><th>Status</th><th>Адрес    </th><th>загрузить    </th><th>Маркеры    </th><th>Владение </th><th>Идентификатор узла    </th><th>Стойка</th></tr>
<tr><th>UN    </td><td>10.1.2.4     </td><td>87,81 КБ    </td><td>256    </td><td>38,0 %    </td><td>GUID (удален)</td><td>rack1</td></tr>
<tr><th>UN    </td><td>10.1.2.5     </td><td>41,08 КБ    </td><td>256    </td><td>68,9 %    </td><td>GUID (удален)</td><td>rack1</td></tr>
<tr><th>UN    </td><td>10.1.2.6     </td><td>55,29 КБ    </td><td>256    </td><td>68,8 %    </td><td>GUID (удален)</td><td>rack2</td></tr>
<tr><th>UN    </td><td>10.1.2.7     </td><td>55,29 КБ    </td><td>256    </td><td>68,8 %    </td><td>GUID (удален)</td><td>rack2</td></tr>
<tr><th>UN    </td><td>10.1.2.8     </td><td>55,29 КБ    </td><td>256    </td><td>68,8 %    </td><td>GUID (удален)</td><td>rack3</td></tr>
<tr><th>UN    </td><td>10.1.2.9     </td><td>55,29 КБ    </td><td>256    </td><td>68,8 %    </td><td>GUID (удален)</td><td>rack3</td></tr>
<tr><th>UN    </td><td>10.1.2.10     </td><td>55,29 КБ    </td><td>256    </td><td>68,8 %    </td><td>GUID (удален)</td><td>rack4</td></tr>
<tr><th>UN    </td><td>10.1.2.11     </td><td>55,29 КБ    </td><td>256    </td><td>68,8 %    </td><td>GUID (удален)</td><td>rack4</td></tr>
</table>

## <a name="test-the-single-region-cluster"></a>Тестирование отдельного кластера региона
Чтобы протестировать кластер, выполните следующие действия.

1. С помощью командлет Get-AzureInternalLoadbalancer команду Powershell, получите IP-адрес внутренней подсистемы балансировки нагрузки (например 10.1.2.101). Синтаксис команды приведен ниже: Get-AzureLoadbalancer –ServiceName "hk-c-svc-west-us" (выводит на экран данные о внутренней подсистеме балансировки нагрузки и ее IP-адрес).
2. Войдите на веб-ферме виртуальной Машины (например hk-w1-Запад us) с помощью Putty или ssh
3. Выполните команду $CASS_HOME/bin/cqlsh 10.1.2.101 9160
4. Проверьте работу кластера с помощью следующих команд CQL:
   
     CREATE KEYSPACE customers_ks WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 3 };   USE customers_ks;   CREATE TABLE Customers(customer_id int PRIMARY KEY, firstname text, lastname text);   INSERT INTO Customers(customer_id, firstname, lastname) VALUES(1, 'John', 'Doe');   INSERT INTO Customers(customer_id, firstname, lastname) VALUES (2, 'Jane', 'Doe');
   
     SELECT * FROM Customers;

Вы увидите примерно следующие результаты:

<table>
  <tr><th> customer_id </th><th> firstname </th><th> lastname </th></tr>
  <tr><td> 1 </td><td> Артем </td><td> Кузнецов </td></tr>
  <tr><td> 2 </td><td> Ольга </td><td> Кузнецов </td></tr>
</table>

Пространство ключей, созданного на шаге 4 используется SimpleStrategy с replication_factor 3. SimpleStrategy рекомендуется для развертываний с одним центром обработки данных, а NetworkTopologyStrategy — для развертываний с несколькими центрами. Replication_factor 3 дает устойчивости к сбоям узла.

## <a id="tworegion"> </a>Процесс развертывания в нескольких регионах
Использовать развертывание в одном регионе завершена и повторите этот процесс для установки второй области. Основное различие между развертывание одного или нескольких регионах является установка туннеля VPN для связи между области; Начните с установки сети, подготовить виртуальные машины и настроить Cassandra.

### <a name="step-1-create-the-virtual-network-at-the-2nd-region"></a>Шаг 1. Создание виртуальной сети во втором регионе
Войдите на портал Azure и создайте виртуальную сеть с Показать атрибуты в таблице. В разделе [Настройка виртуальной сети только на портале Azure](../../../virtual-network/virtual-networks-create-vnet-classic-pportal.md) более подробное пошаговое описание процесса.      

<table>
<tr><th>Имя атрибута    </th><th>Значение    </th><th>Примечания</th></tr>
<tr><td>ИМЯ    </td><td>vnet-cass-east-us</td><td></td></tr>
<tr><td>Регион    </td><td>Восток США</td><td></td></tr>
<tr><td>DNS-серверы        </td><td></td><td>Пропустите этот атрибут, поскольку мы не используем DNS-сервер</td></tr>
<tr><td>Настройка VPN-подключения типа "точка-сеть"</td><td></td><td>        Пропустите этот атрибут</td></tr>
<tr><td>"Настроить VPN типа "сеть-сеть"</td><td></td><td>        Пропустите этот атрибут</td></tr>
<tr><td>Пространство адресов    </td><td>10.2.0.0/16</td><td></td></tr>
<tr><td>Начальный IP-адрес    </td><td>10.2.0.0    </td><td></td></tr>
<tr><td>CIDR    </td><td>/16 (65531)</td><td></td></tr>
</table>

Добавьте следующие подсети.

<table>
<tr><th>ИМЯ    </th><th>Начальный IP-адрес    </th><th>CIDR    </th><th>Примечания</th></tr>
<tr><td>web    </td><td>10.2.1.0    </td><td>/24 (251)    </td><td>Подсеть для веб-фермы</td></tr>
<tr><td>data    </td><td>10.2.2.0    </td><td>/24 (251)    </td><td>Подсеть для узлов базы данных</td></tr>
</table>


### <a name="step-2-create-local-networks"></a>Шаг 2. Создание локальных сетей
Локальная сеть в виртуальной сети Azure — это пространство прокси-адресов, которое сопоставляется с удаленным сайтом, включая личное облако или другой регион Azure. Это пространство прокси-адресов привязывается к удаленному шлюзу для маршрутизации сети к нужным точкам назначения. Указания по установке подключения между виртуальными сетями см. в статье [Настройка подключения между виртуальными сетями для классической модели развертывания](../../../vpn-gateway/virtual-networks-configure-vnet-to-vnet-connection.md).

Создайте две локальных сети со следующими параметрами:

| Имя сети | Адрес VPN-шлюза | Пространство адресов | Примечания |
| --- | --- | --- | --- |
| hk-lnet-map-to-east-us |23.1.1.1 |10.2.0.0/16 |При создании локальной сети укажите замещающий адрес шлюза. Настоящий адрес указывается после создания шлюза. Убедитесь, что пространство адресов точно совпадает с соответствующей удаленной виртуальной сетью. В данном случае виртуальная сеть создается для восточного региона США. |
| hk-lnet-map-to-west-us |23.2.2.2 |10.1.0.0/16 |При создании локальной сети укажите замещающий адрес шлюза. Настоящий адрес указывается после создания шлюза. Убедитесь, что пространство адресов точно совпадает с соответствующей удаленной виртуальной сетью. В данном случае виртуальная сеть создается для западного региона США. |

### <a name="step-3-map-local-network-to-the-respective-vnets"></a>Шаг 3. Сопоставление "локальной" сети с соответствующими виртуальными сетями
На портале Azure выбор каждой виртуальной сети, щелкните «Настроить», проверьте «Подключение к локальной сети» и локальных сетей на следующие сведения:

| Виртуальная сеть | Локальная сеть |
| --- | --- |
| hk-vnet-west-us |hk-lnet-map-to-east-us |
| hk-vnet-east-us |hk-lnet-map-to-west-us |

### <a name="step-4-create-gateways-on-vnet1-and-vnet2"></a>Шаг 4. Создание шлюзов для сетей VNET1 и VNET2
С помощью панели мониторинга виртуальной сети нажмите кнопку Создать ШЛЮЗ для запуска процесса инициализации шлюза VPN. Спустя несколько минут на панели мониторинга каждой виртуальной сети должен появиться настоящий адрес шлюза.

### <a name="step-5-update-local-networks-with-the-respective-gateway-addresses"></a>Шаг 5. Обновление "локальных" сетей соответствующими адресами шлюзов
Измените обе локальные сети, заменив подстановочные IP-адреса шлюза настоящим IP-адресами подготовленных шлюзов. Используйте следующие сопоставления:

<table>
<tr><th>Локальная сеть    </th><th>Шлюз виртуальной сети</th></tr>
<tr><td>hk-lnet-map-to-east-us </td><td>Шлюз сети hk-vnet-west-us</td></tr>
<tr><td>hk-lnet-map-to-west-us </td><td>Шлюз сети hk-vnet-east-us</td></tr>
</table>

### <a name="step-6-update-the-shared-key"></a>Шаг 6. Обновление общего ключа
Используйте следующий сценарий Powershell, чтобы обновить ключ IPSec каждого из VPN-шлюзов. Используйте один и тот же ключ для обоих шлюзов: Set-AzureVNetGatewayKey -VNetName hk-vnet-east-us -LocalNetworkSiteName hk-lnet-map-to-west-us -SharedKey D9E76BKK Set-AzureVNetGatewayKey -VNetName hk-vnet-west-us -LocalNetworkSiteName hk-lnet-map-to-east-us -SharedKey D9E76BKK.

### <a name="step-7-establish-the-vnet-to-vnet-connection"></a>Шаг 7. Установка подключения между виртуальными сетями
На портале Azure используйте меню «Панель МОНИТОРИНГА» виртуальных сетей для подключения шлюза к шлюзу. Используйте пункты меню "ПОДКЛЮЧЕНИЕ" в нижней панели инструментов. Спустя несколько минут на панели мониторинга должно появиться графическое представление сведений о подключении.

### <a name="step-8-create-the-virtual-machines-in-region-2"></a>Шаг 8. Создание виртуальных машин во втором регионе
Создайте образ Ubuntu, как при развертывании в первом регионе, выполнив те же действия, или скопируйте VHD-файл образа в учетную запись хранения Azure, расположенную во втором регионе, и создайте образ. Используя этот образ, создайте следующий список виртуальных машин в новой облачной службе hk-c-svc-east-us:

| Имя компьютера | Подсеть | IP-адрес | Группа доступности | Контроллер домена/стойка | Начальное значение? |
| --- | --- | --- | --- | --- | --- |
| hk-c1-east-us |data |10.2.2.4 |hk-c-aset-1 |dc =EASTUS rack =rack1 |Yes |
| hk-c2-east-us |data |10.2.2.5 |hk-c-aset-1 |dc =EASTUS rack =rack1 |Нет  |
| hk-c3-east-us |data |10.2.2.6 |hk-c-aset-1 |dc =EASTUS rack =rack2 |Yes |
| hk-c5-east-us |data |10.2.2.8 |hk-c-aset-2 |dc =EASTUS rack =rack3 |Yes |
| hk-c6-east-us |data |10.2.2.9 |hk-c-aset-2 |dc =EASTUS rack =rack3 |Нет  |
| hk-c7-east-us |data |10.2.2.10 |hk-c-aset-2 |dc =EASTUS rack =rack4 |Yes |
| hk-c8-east-us |data |10.2.2.11 |hk-c-aset-2 |dc =EASTUS rack =rack4 |Нет  |
| hk-w1-east-us |web |10.2.1.4 |hk-w-aset-1 |Н/Д |Н/Д |
| hk-w2-east-us |web |10.2.1.5 |hk-w-aset-1 |Н/Д |Н/Д |

Выполните те же действия, что и в первом регионе, но используйте пространство адресов 10.2.xxx.xxx.

### <a name="step-9-configure-cassandra-on-each-vm"></a>Шаг 9. Настройка Cassandra на каждой виртуальной машине
Войдите на виртуальную машину и выполните следующие действия:

1. Отредактируйте файл $CASS_HOME/conf/cassandra-rackdc.properties, указав свойства центра обработки данных и стойки в следующем формате: dc =EASTUS rack =rack1.
2. Отредактируйте файл cassandra.yaml, чтобы настроить начальные узлы. Начальные значения: "10.1.2.4,10.1.2.6,10.1.2.8,10.1.2.10,10.2.2.4,10.2.2.6,10.2.2.8,10.2.2.10"

### <a name="step-10-start-cassandra"></a>Шаг 10. Запуск Cassandra
Войдите на каждую виртуальную машину и запустите Cassandra в фоновом режиме с помощью следующей команды: $CASS_HOME/bin/cassandra.

## <a name="test-the-multi-region-cluster"></a>Тестирование кластера с несколькими регионами
В данный момент выполнено развертывание Cassandra на 16 узлах: по 8 узлов в каждом регионе Azure. Эти узлы объединены в кластер с помощью общего имени кластера и конфигурации начальных узлов. Чтобы протестировать кластер, выполните следующие действия:

### <a name="step-1-get-the-internal-load-balancer-ip-for-both-the-regions-using-powershell"></a>Шаг 1. Получение IP-адреса внутренней подсистемы балансировки нагрузки для обоих регионов с помощью PowerShell
* Get-AzureInternalLoadbalancer -ServiceName "hk-c-svc-west-us"
* Get-AzureInternalLoadbalancer -ServiceName "hk-c-svc-east-us"  
  
    Обратите внимание, IP-адреса (для примера Западная - 10.1.2.101, восток - 10.2.2.101) отображается.

### <a name="step-2-execute-the-following-in-the-west-region-after-logging-into-hk-w1-west-us"></a>Шаг 2. Выполнение команд в западном регионе после входа в hk-w1-west-us
1. Выполните команду $CASS_HOME/bin/cqlsh 10.1.2.101 9160
2. Выполните следующие команды CQL:
   
     CREATE KEYSPACE customers_ks   WITH REPLICATION = { 'class' : 'NetworkToplogyStrategy', 'WESTUS' : 3, 'EASTUS' : 3};   USE customers_ks;   CREATE TABLE Customers(customer_id int PRIMARY KEY, firstname text, lastname text);   INSERT INTO Customers(customer_id, firstname, lastname) VALUES(1, 'John', 'Doe');   INSERT INTO Customers(customer_id, firstname, lastname) VALUES (2, 'Jane', 'Doe');   SELECT * FROM Customers;

На экране должно появиться примерно следующее:

| customer_id | firstname | Lastname |
| --- | --- | --- |
| 1 |Артем |Кузнецов |
| 2 |Ольга |Кузнецов |

### <a name="step-3-execute-the-following-in-the-east-region-after-logging-into-hk-w1-east-us"></a>Шаг 3. Выполнение команд в восточном регионе после входа в hk-w1-east-us
1. Выполните команду $CASS_HOME/bin/cqlsh 10.2.2.101 9160.
2. Выполните следующие команды CQL:
   
     USE customers_ks;   CREATE TABLE Customers(customer_id int PRIMARY KEY, firstname text, lastname text);   INSERT INTO Customers(customer_id, firstname, lastname) VALUES(1, 'John', 'Doe');   INSERT INTO Customers(customer_id, firstname, lastname) VALUES (2, 'Jane', 'Doe');   SELECT * FROM Customers;

На экране должны появиться такие же данные, как и для западного региона:

| customer_id | firstname | Lastname |
| --- | --- | --- |
| 1 |Артем |Кузнецов |
| 2 |Ольга |Кузнецов |

Выполните еще несколько вставок и убедитесь, что они реплицируются в часть кластера west-us.

## <a name="test-cassandra-cluster-from-nodejs"></a>Тестирование кластера Cassandra из файла Node.js
С помощью одного из виртуальных машин Linux, ранее создан на уровне «web», выполняется простой сценарий Node.js для чтения ранее вставленных данных

**Шаг 1. Установка Node.js и клиента Cassandra**

1. Установка Node.js и NPM
2. Установите пакет узлов "cassandra-client" с помощью NPM
3. Выполните следующий сценарий в окне командной строки, в которой отображается строка JSON полученных данных:
   
        var pooledCon = require('cassandra-client').PooledConnection;
        var ksName = "custsupport_ks";
        var cfName = "customers_cf";
        var hostList = ['internal_loadbalancer_ip:9160'];
        var ksConOptions = { hosts: hostList,
                             keyspace: ksName, use_bigints: false };
   
        function createKeyspace(callback){
           var cql = 'CREATE KEYSPACE ' + ksName + ' WITH strategy_class=SimpleStrategy AND strategy_options:replication_factor=1';
           var sysConOptions = { hosts: hostList,  
                                 keyspace: 'system', use_bigints: false };
           var con = new pooledCon(sysConOptions);
           con.execute(cql,[],function(err) {
           if (err) {
             console.log("Failed to create Keyspace: " + ksName);
             console.log(err);
           }
           else {
             console.log("Created Keyspace: " + ksName);
             callback(ksConOptions, populateCustomerData);
           }
           });
           con.shutdown();
        }
   
        function createColumnFamily(ksConOptions, callback){
          var params = ['customers_cf','custid','varint','custname',
                        'text','custaddress','text'];
          var cql = 'CREATE COLUMNFAMILY ? (? ? PRIMARY KEY,? ?, ? ?)';
        var con =  new pooledCon(ksConOptions);
          con.execute(cql,params,function(err) {
              if (err) {
                 console.log("Failed to create column family: " + params[0]);
                 console.log(err);
              }
              else {
                 console.log("Created column family: " + params[0]);
                 callback();
              }
          });
          con.shutdown();
        }
   
        //populate Data
        function populateCustomerData() {
           var params = ['John','Infinity Dr, TX', 1];
           updateCustomer(ksConOptions,params);
   
           params = ['Tom','Fermat Ln, WA', 2];
           updateCustomer(ksConOptions,params);
        }
   
        //update also inserts the record if none exists
        function updateCustomer(ksConOptions,params)
        {
          var cql = 'UPDATE customers_cf SET custname=?,custaddress=? where custid=?';
          var con = new pooledCon(ksConOptions);
          con.execute(cql,params,function(err) {
              if (err) console.log(err);
              else console.log("Inserted customer : " + params[0]);
          });
          con.shutdown();
        }
   
        //read the two rows inserted above
        function readCustomer(ksConOptions)
        {
          var cql = 'SELECT * FROM customers_cf WHERE custid IN (1,2)';
          var con = new pooledCon(ksConOptions);
          con.execute(cql,[],function(err,rows) {
              if (err)
                 console.log(err);
              else
                 for (var i=0; i<rows.length; i++)
                    console.log(JSON.stringify(rows[i]));
            });
           con.shutdown();
        }
   
        //exectue the code
        createKeyspace(createColumnFamily);
        readCustomer(ksConOptions)

## <a name="conclusion"></a>Заключение
Microsoft Azure — это гибкая платформа, которая позволяет запускать как программы Майкрософт, так и ПО с открытым кодом, как показано в этом упражнении. Кластеры Cassandra с высокой доступностью можно развернуть в одном центре обработки данных, распределяя узлы кластера между несколькими доменами сбоя. Кластеры Cassandra также можно развертывать в нескольких географически разделенных регионах Azure для отказоустойчивых систем. Совместное использование Azure и Cassandra позволяет создавать масштабируемые облачные службы с высокой доступностью и возможностью аварийного восстановления, необходимые для современных служб Интернета.  

## <a name="references"></a>Ссылки
* [http://cassandra.apache.org](http://cassandra.apache.org)
* [http://www.datastax.com](http://www.datastax.com)
* [http://www.nodejs.org](http://www.nodejs.org)

