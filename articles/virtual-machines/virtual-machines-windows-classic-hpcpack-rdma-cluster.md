<properties
 pageTitle="Настройка кластера Windows RDMA для запуска приложений MPI | Microsoft Azure"
 description="Создание кластера Windows HPC Pack с виртуальными машинами размеров H16r, H16mr, A8 или A9, чтобы использовать сеть Azure RDMA для запуска приложений MPI."
 services="virtual-machines-windows"
 documentationCenter=""
 authors="dlepow"
 manager="timlt"
 editor=""
 tags="azure-service-management,hpc-pack"/>
<tags
ms.service="virtual-machines-windows"
 ms.devlang="na"
 ms.topic="article"
 ms.tgt_pltfrm="vm-windows"
 ms.workload="big-compute"
 ms.date="09/20/2016"
 ms.author="danlep"/>

# Настройка кластера RDMA в Windows с помощью пакета HPC для запуска приложений MPI

Настройте кластер Windows RDMA в Azure с помощью [пакета Microsoft HPC](https://technet.microsoft.com/library/cc514029) и [экземпляров серии A для ресурсоемких вычислений или серии H](virtual-machines-windows-a8-a9-a10-a11-specs.md) для запуска параллельных приложений MPI. Если настроить узлы с поддержкой RDMA под управлением Windows Server в кластере пакета HPC, приложения MPI будут эффективно взаимодействовать по сети с низкой задержкой и высокой пропускной способностью в Azure, основанной на технологии удаленного доступа к памяти (RDMA).

Если требуется применить рабочие нагрузки MPI на виртуальных машинах под управлением Linux, получающих доступ к сети RDMA в Azure, см. раздел [Настройка кластера Linux RDMA для запуска приложений MPI](virtual-machines-linux-classic-rdma-cluster.md).


## Варианты развертывания кластера пакета HPC
Пакет Microsoft HPC — это бесплатное средство для создания кластеров HPC в локальной сети или в Azure, в которых могут выполняться приложения HPC для Windows или Linux. В пакет HPC входит среда выполнения для реализации интерфейса передачи сообщений для Windows (MS-MPI). При использовании пакета HPC с экземплярами с поддержкой RDMA под управлением поддерживаемой операционной системы Windows Server этот пакет позволяет эффективно выполнять приложения MPI для Windows с доступом к сети Azure RDMA.

В этой статье представлены два сценария и ссылки на подробные руководства по настройке кластера Windows RDMA с помощью пакета Microsoft HPC.

* Сценарий 1. Развертывание экземпляров рабочих ролей для ресурсоемких вычислений (PaaS)

* Сценарий 2. Развертывание вычислительных узлов на виртуальных машинах с ресурсоемкими вычислениями (IaaS)

Общие предварительные требования для использования ресурсоемких экземпляров с Windows описаны в статье [Виртуальные машины серии A (для ресурсоемких вычислений) и серии H](virtual-machines-windows-a8-a9-a10-a11-specs.md).



## Сценарий 1. Развертывание экземпляров рабочих ролей для ресурсоемких вычислений (PaaS)

Из существующего кластера HPC Pack добавьте дополнительные вычислительные ресурсы в экземплярах рабочих ролей Azure (узлах Azure), запускаемых в облачной службе (PaaS). Эта функция, называемая также «ускорением в Azure» из пакета HPC, поддерживает определенный диапазон размеров для экземпляров рабочих ролей. При добавлении узлов Azure просто укажите один из размеров с поддержкой RDMA.

Ниже описаны рекомендации и действия для расширения существующего (как правило, локального) кластера с помощью экземпляров Azure с поддержкой RDMA. Аналогичные процедуры позволяют добавлять экземпляры рабочих ролей в головной узел HPC Pack, который развертывается на виртуальной машине Azure.

>[AZURE.NOTE] Учебник по ускорению в Azure с помощью пакета HPC см. в разделе [Настройка гибридного кластера с пакетом HPC](../cloud-services/cloud-services-setup-hybrid-hpcpack-cluster.md). Обратите внимание на рекомендации в описанных ниже шагах, которые применяются к узлам Azure с поддержкой RDMA.

![Ускорение в Azure][burst]

### Действия

4. **Развертывание и настройка головного узла HPC Pack 2012 R2.**

    Загрузите новейшую версию пакета установки HPC Pack из [Центра загрузки Майкрософт](https://www.microsoft.com/download/details.aspx?id=49922). Требования и инструкции для подготовки к развертыванию Azure, которое поддерживает расширение, см. в [руководстве по началу работы с пакетами Microsoft HPC 2012 R2 и HPC 2012](https://technet.microsoft.com/library/jj884144.aspx) и статье, посвященной [расширению развертывания Azure с помощью пакета Microsoft HPC](https://technet.microsoft.com/library/gg481749.aspx).

5. **Настройка сертификата управления в подписке Azure.**

    Настройте сертификат для обеспечения безопасного соединения между головным узлом и Azure. Параметры и процедуры см. в разделе [Сценарии настройки сертификата управления Azure для пакета HPC](http://technet.microsoft.com/library/gg481759.aspx). Для тестового развертывания пакет HPC устанавливает сертификат управления Microsoft HPC Azure по умолчанию, который вы можете быстро отправить в подписку Azure.

6. **Создание новой облачной службы и учетной записи хранения.**

    С помощью классического портала Azure создайте облачную службу и учетную запись хранения для развертывания в регионе, в котором доступны экземпляры с поддержкой RDMA.

7. **Создание шаблона узла Azure.**

    Используйте мастер создания шаблона узла в диспетчере кластеров HPC. Пошаговые инструкции см. в разделе [Создание шаблона узла Azure](http://technet.microsoft.com/library/gg481758.aspx#BKMK_Templ) документа «Шаги по развертыванию узлов Azure с помощью пакета Microsoft HPC».

    Для первоначальных тестов рекомендуется настроить ручную политику доступности в шаблоне.

8. **Добавление узлов в кластер.**

    Используйте мастер добавления узла в диспетчере кластеров HPC. Дополнительные сведения см. в разделе [Добавление узлов Azure в кластер HPC Windows](http://technet.microsoft.com/library/gg481758.aspx#BKMK_Add).

    При указании размера узлов выберите один из размеров экземпляров с поддержкой RDMA.
    
    >[AZURE.NOTE]При каждом расширении развертывания Azure с помощью вычислений пакет HPC автоматически развертывает не менее 2 экземпляров с поддержкой RDMA (например A8) в качестве прокси-узлов (в дополнение к указанным вами экземплярам рабочих ролей Azure). Прокси-узлы используют ядра, привязанные к подписке, за которые взимается плата на тех же условиях, что и за экземпляры рабочих ролей Azure.

9. **Запуск (подготовка) узлов и их подключение к сети для выполнения заданий.**

    Выберите узлы и используйте действие **Запуск** в диспетчере кластеров HPC. По завершении подготовки выберите узлы и используйте действие **Перевести в оперативный режим** в диспетчере кластеров HPC. После этого узлы будут готовы к выполнению заданий.

10. **Отправка заданий в кластер.**

    Используйте средства отправки заданий HPC Pack для выполнения заданий кластера. См. раздел [Пакет Microsoft HPC Pack: управление заданиями](http://technet.microsoft.com/library/jj899585.aspx).

11. **Остановка (отзыв) узлов**

    После завершения выполнения заданий отключите узлы и используйте действие **Остановить** в диспетчере кластеров HPC.





## Сценарий 2. Развертывание вычислительных узлов на виртуальных машинах с ресурсоемкими вычислениями (IaaS)

В этом сценарии развертываются головной узел HPC Pack и вычислительные узлы кластера на виртуальных машинах, присоединенных к домену Active Directory в виртуальной сети Azure. Пакет HPC предоставляет несколько [вариантов развертывания на виртуальных машинах Azure](virtual-machines-linux-hpcpack-cluster-options.md), в том числе сценарии автоматизированного развертывания и шаблоны быстрого запуска Azure. Приведенные далее рекомендации и действия могут служить примером использования [сценария развертывания IaaS пакета HPC](virtual-machines-windows-classic-hpcpack-cluster-powershell-script.md) для автоматизации большей части этого процесса.

![Кластер на виртуальных машинах Azure][iaas]



### Действия

1. **Создание головного узла кластера и виртуальных машин вычислительных узлов путем выполнения скрипта развертывания IaaS с пакетом HPC на клиентском компьютере.**

    Загрузите пакет скрипта развертывания IaaS для пакета HPC Pack из [Центра загрузки Майкрософт](https://www.microsoft.com/download/details.aspx?id=49922).

    Чтобы подготовить клиентский компьютер, создайте файл конфигурации скрипта и выполните скрипт. См. раздел [Создание кластера HPC с помощью скрипта развертывания IaaS с пакетом HPC Pack](virtual-machines-windows-classic-hpcpack-cluster-powershell-script.md).
    
    При развертывании вычислительных узлов с поддержкой RDMA обратите внимание на следующие дополнительные рекомендации.
    
    * **Виртуальная сеть** — укажите новую виртуальную сеть в регионе, в котором доступны нужные размеры экземпляров с поддержкой RDMA.

    * **Операционная система Windows Server** — для поддержки соединения RDMA укажите операционную систему Windows Server 2012 R2 или Windows Server 2012 для виртуальных машин вычислительных узлов.

    * **Облачные службы** — мы рекомендуем развертывать головной узел в одной облачной службе, а вычислительные узлы — в другой облачной службе.

    * **Размер головного узла** — для нашего сценария головной узел должен иметь по меньшей мере размер A4 (очень крупный).

    * **Расширение HpcVmDrivers** — скрипт развертывания автоматически устанавливает агент ВМ Azure и расширение HpcVmDrivers, когда вы развертываете вычислительные узлы размера A8 или A9, в которых используется ОС Windows Server. HpcVmDrivers устанавливает драйверы на виртуальные машины вычислительного узла, чтобы они могли подключаться к сети RDMA.

    * **Конфигурация сети кластера** — сценарий развертывания автоматически настраивает кластер пакета HPC с топологией 5 (все узлы в корпоративной сети). Данная топология является обязательной для всех развертываний кластера пакета HPC на виртуальных машинах. В дальнейшем не следует изменять топологию сети кластера.

2. **Перевод вычислительных узлов в оперативный режим для запуска заданий.**

    Выберите узлы и используйте действие **Перевести в оперативный режим** в диспетчере кластеров HPC. После этого узлы будут готовы к выполнению заданий.

3. **Отправка заданий в кластер.**

    Подключитесь к головному узлу для отправки заданий или настройте для этого локальный компьютер. Сведения см. в разделе [Отправка заданий в кластер HPC в Azure](virtual-machines-windows-hpcpack-cluster-submit-jobs.md).

4. **Отключение узлов и их остановка (отзыв).**

    После выполнения заданий отключите узлы в диспетчере кластеров HPC. Затем с помощью средств управления Azure завершите их работу.



## Запуск приложений MPI в кластере

### Пример. Выполнение команды mpipingpong в кластере HPC Pack

Чтобы проверить развертывание экземпляров с поддержкой RDMA, которое выполнялось с помощью пакета HPC, запустите в кластере команду **mpipingpong** из этого пакета. Команда **mpipingpong** многократно отправляет пакеты данных из одного спаренного узла в другой, чтобы вычислить задержки, измерить пропускную способность и собрать статистику для сети приложений с поддержкой RDMA. В этом примере показана типичная схема запуска задания MPI (в данном случае **mpipingpong**) с помощью команды кластера **mpiexec**.

В примере предполагается, что узлы Azure добавлены в конфигурации "ускорение в Azure" ([Сценарий 1](#scenario-1.-deploy-compute-intensive-worker-role-instances-(PaaS)) в данной статье). Если пакет HPC Pack развернут в кластере виртуальных машин Azure, необходимо изменить синтаксис команды, чтобы указать другую группу узлов и задать дополнительные переменные среды для направления сетевого трафика по сети RDMA.


Чтобы выполнить команду mpipingpong в кластере:


1. На головном узле или на соответствующим образом настроенном клиентском компьютере откройте командную строку.

2. Чтобы оценить задержку между парами узлов в развертывании ускорения Azure, состоящем из четырех узлов, запустите задание выполнения команды mpipingpong с небольшим размером пакетов и большим количеством итераций. Для этого введите такую команду:

    ```
    job submit /nodegroup:azurenodes /numnodes:4 mpiexec -c 1 -affinity mpipingpong -p 1:100000 -op -s nul
    ```

    Команда вернет идентификатор запущенного задания.

    Если кластер HPC Pack развернут на виртуальных машинах Azure, укажите группу узлов, которая содержит виртуальные машины вычислительных узлов, развернутые в одной облачной службе, и измените команду **mpiexec** следующим образом:

    ```
    job submit /nodegroup:vmcomputenodes /numnodes:4 mpiexec -c 1 -affinity -env MSMPI_DISABLE_SOCK 1 -env MSMPI_PRECONNECT all -env MPICH_NETMASK 172.16.0.0/255.255.0.0 mpipingpong -p 1:100000 -op -s nul
    ```

3. Чтобы после завершения задания просмотреть выходные данные (в данном случае — выходные данные задачи 1 задания), введите следующую команду

    ```
    task view <JobID>.1
    ```

    где &lt;*JobID*&gt; — это идентификатор запущенного задания.

    Выходные данные будут содержать сведения о задержке, подобные следующим.

    ![Задержка проверки связи][pingpong1]

4. Чтобы оценить пропускную способность между парами узлов ускорения Azure, запустите задание выполнения команды **mpipingpong** с большим размером пакетов и небольшим количеством итераций. Для этого введите такую команду:

    ```
    job submit /nodegroup:azurenodes /numnodes:4 mpiexec -c 1 -affinity mpipingpong -p 4000000:1000 -op -s nul
    ```

    Команда вернет идентификатор запущенного задания.

    В кластере HPC Pack, развернутом на виртуальных машинах Azure, измените команду, как указано в шаге 2.

5. Чтобы после завершения задания просмотреть выходные данные (в данном случае — выходные данные задачи 1 задания), введите следующую команду:

    ```
    task view <JobID>.1
    ```

  Выходные данные будут содержать сведения о пропускной способности, подобные следующим.

  ![Пропускная способность при «пинг-понговой» передаче данных][pingpong2]


### Рекомендации по приложениям MPI


Ниже приведены рекомендации по запуску в Azure приложений MPI с пакетом HPC. Некоторые из них применимы только к развертыванию узлов Azure (экземпляров рабочих ролей, добавленных в конфигурации «ускорение в Azure»).

* Экземпляры рабочих ролей в облачной службе периодически повторно подготавливаются без уведомления системой Azure (например, для обслуживания системы или в случае сбоя экземпляра). Если экземпляр повторно подготавливается в то время, когда он выполняет задание MPI, экземпляр теряет все свои данные и возвращается в исходное состояние, что может привести к сбою задания MPI. Чем больше узлов используется для одного задания MPI и чем дольше задание выполняется, тем вероятнее, что один из экземпляров будет подготовлен повторно во время выполнения задания. Это следует учитывать также в случае назначения одного узла в развертывании в качестве файлового сервера.


* Для запуска заданий MPI в Azure не обязательно использовать экземпляры с поддержкой RDMA. Вы можете использовать экземпляр любого размера, поддерживаемый пакетом HPC. Тем не менее экземпляры с поддержкой RDMA рекомендуются для относительно крупных заданий MPI, чувствительных к задержке и пропускной способности сети, соединяющей узлы. Если для запуска заданий MPI, чувствительных к задержке и пропускной способности, вы будете использовать другие размеры экземпляров, рекомендуем запускать мелкие задания, в которых каждая отдельная задача выполняется только на нескольких узлах.

* Приложения, развернутые в экземплярах Azure, подлежат условиям лицензирования, связанным с приложением. Проконсультируйтесь с поставщиками всех коммерческих приложений насчет лицензирования или иных ограничений на запуск приложений в облаке. Не все поставщики предлагают лицензирование с оплатой по мере использования.


* Экземплярам Azure требуется дополнительная настройка для доступа к локальным узлам, общим ресурсам и серверам лицензий. Например, чтобы включить для узлов Azure доступ к локальному серверу лицензий, вы можете настроить виртуальную сеть Azure «сеть — сеть».


* Чтобы запустить приложение MPI в экземплярах Azure, следует зарегистрировать приложение MPI в брандмауэре Windows в экземплярах, запустив команду **hpcfwutil**. Это позволит осуществлять обмен данными MPI через порт, который динамически назначается брандмауэром.

    >[AZURE.NOTE] Для развертываний ускорения в Azure можно также настроить команду исключения брандмауэра с целью автоматического запуска на всех новых узлах Azure, которые добавляются к кластеру. Запустив команду **hpcfwutil** и убедившись в том, что ваше приложение работает, добавьте команду в скрипт запуска для узлов Azure. Дополнительные сведения см. в статье об [использовании сценария запуска для узлов Azure](https://technet.microsoft.com/library/jj899632.aspx).



* Пакет HPC использует переменную среды кластера CCP\_MPI\_NETMASK для указания диапазона допустимых адресов для связи MPI. Начиная с пакета HPC Pack 2012 R2, переменная среды кластера CCP\_MPI\_NETMASK влияет только на связь MPI между вычислительными узлами кластера, присоединенными к домену (локально или на виртуальных машинах Azure). Переменная игнорируется узлами, добавленными в рамках ускорения в конфигурацию Azure.


* Задания MPI не могут выполняться между экземплярами Azure, развернутыми в разных облачных службах (например, в развертываниях ускорения в Azure с разными шаблонами узлов или на вычислительных узлах виртуальных машин Azure, развернутых в нескольких облачных службах). При наличии нескольких развертываний узлов Azure, запущенных с разными шаблонами узлов, задание MPI должно выполняться только на одном наборе узлов Azure.


* При добавлении узлов Azure в кластер и переводе их в оперативный режим служба планировщика заданий HPC немедленно пытается запустить задания на узлах. Если в Azure можно запустить только часть имеющейся рабочей нагрузки, не забудьте обновить или создать шаблоны заданий, чтобы таким образом определить типы заданий, которые можно выполнять в Azure. Например, чтобы на узлах Azure выполнялись только те задания, которые были отправлены с использованием шаблона задания, добавьте свойство «Группы узлов» в шаблон задания и выберите AzureNodes в качестве обязательного значения. Для создания настраиваемых групп узлов Azure можно использовать командлет PowerShell Add-HpcGroup HPC.


## Дальнейшие действия

* В качестве альтернативы пакету HPC для выполнения приложений MPI в управляемых пулах вычислительных узлов в Azure можно использовать пакетную службу Azure. Ознакомьтесь со статьей [Использование задач с несколькими экземплярами для запуска приложений с интерфейсом передачи сообщений в пакетной службе Azure](../batch/batch-mpi.md).

* Если требуется запускать приложения MPI для Linux, получающие доступ к сети RDMA в Azure, см. раздел [Настройка кластера Linux RDMA для запуска приложений MPI](virtual-machines-linux-classic-rdma-cluster.md).

<!--Image references-->
[burst]: ./media/virtual-machines-windows-classic-hpcpack-rdma-cluster/burst.png
[iaas]: ./media/virtual-machines-windows-classic-hpcpack-rdma-cluster/iaas.png
[pingpong1]: ./media/virtual-machines-windows-classic-hpcpack-rdma-cluster/pingpong1.png
[pingpong2]: ./media/virtual-machines-windows-classic-hpcpack-rdma-cluster/pingpong2.png

<!---HONumber=AcomDC_0928_2016-->