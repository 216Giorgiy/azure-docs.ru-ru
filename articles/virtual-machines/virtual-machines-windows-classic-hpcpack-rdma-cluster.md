<properties
 pageTitle="Настройка кластера Windows RDMA для запуска приложений MPI | Microsoft Azure"
 description="Сведения о том, как создать кластер Windows HPC Pack с виртуальными машинами размера A8 или A9, чтобы использовать сеть Azure RDMA для запуска приложений MPI."
 services="virtual-machines-windows"
 documentationCenter=""
 authors="dlepow"
 manager="timlt"
 editor=""
 tags="azure-service-management,hpc-pack"/>
<tags
ms.service="virtual-machines-windows"
 ms.devlang="na"
 ms.topic="article"
 ms.tgt_pltfrm="vm-windows"
 ms.workload="big-compute"
 ms.date="01/13/2016"
 ms.author="danlep"/>

# Настройка кластера Windows RDMA с пакетом HPC Pack и экземплярами A8 и A9 для запуска приложений MPI

[AZURE.INCLUDE [learn-about-deployment-models](../../includes/learn-about-deployment-models-classic-include.md)]Модель диспетчера ресурсов.


В этой статье рассказывается о том, как настроить кластер Windows RDMA в Azure с помощью [пакета Microsoft HPC](https://technet.microsoft.com/library/cc514029) и [экземпляров для ресурсоемких вычислений размеров A8 и A9](virtual-machines-windows-a8-a9-a10-a11-specs.md) для запуска параллельных приложений, использующих интерфейс передачи сообщений (MPI). Если настроить экземпляры размеров A8 и A9 под управлением Windows Server для выполнения в кластере пакета HPC, приложения MPI будут эффективно взаимодействовать по сети с низкой задержкой и высокой пропускной способностью в Azure на основе технологии удаленного доступа к памяти (RDMA).

Если требуется применить рабочие нагрузки MPI на виртуальных машинах под управлением Linux, получающих доступ к сети RDMA в Azure, см. раздел [Настройка кластера Linux RDMA для запуска приложений MPI](virtual-machines-linux-classic-rdma-cluster.md).

## Варианты развертывания кластера пакета HPC
Пакет Microsoft HPC является рекомендованным инструментом для создания кластеров HPC на базе Windows Server в Azure. При использовании с экземплярами A8 и A9 пакет HPC обеспечивает эффективный способ запуска приложений MPI на базе Windows, которые получают доступ к сети RDMA в Azure. В пакет HPC входит среда выполнения для реализации корпорацией Майкрософт интерфейса передачи сообщений для Windows (MSMPI).

В этой статье рассматриваются два сценария развертывания кластеризованных экземпляров A8 и A9 с помощью пакета Microsoft HPC.

* Сценарий 1. Развертывание экземпляров рабочих ролей с ресурсоемкими вычислениями (PaaS)

* Сценарий 2. Развертывание вычислительных узлов на виртуальных машинах с ресурсоемкими вычислениями (IaaS)

## Предварительные требования

* **Ознакомьтесь[ с основной информацией и рекомендациями](virtual-machines-windows-a8-a9-a10-a11-specs.md)** по экземплярам для ресурсоемких вычислений.


* **Подписка Azure** — если ее нет, можно создать бесплатную пробную учетную запись всего за несколько минут. Дополнительные сведения см. в разделе [Бесплатная пробная версия Azure](https://azure.microsoft.com/pricing/free-trial/).


* **Квота ядер** — вам может потребоваться увеличить квоту ядер для развертывания кластера виртуальных машин A8 или A9. Например, для развертывания восьми экземпляров A9 с пакетом HPC необходимо не меньше 128 ядер. Чтобы увеличить квоту, [отправьте запрос в службу поддержки](https://azure.microsoft.com/blog/2014/06/04/azure-limits-quotas-increase-requests/). Это можно сделать бесплатно.

## Сценарий 1. Развертывание экземпляров рабочих ролей с ресурсоемкими вычислениями (PaaS)


Из существующего кластера HPC Pack добавьте дополнительные вычислительные ресурсы в экземплярах рабочих ролей Azure (узлах Azure), запускаемых в облачной службе (PaaS). Эта функция, называемая также «ускорением в Azure» из пакета HPC, поддерживает определенный диапазон размеров для экземпляров рабочих ролей. Чтобы использовать экземпляры с ресурсоемкими вычислениями, просто укажите размер A8 или A9 при добавлении узлов Azure.

Ниже описаны рекомендации и действия для ускорения до экземпляров Azure A8 или A9 из существующего (как правило, локального) кластера. Аналогичные процедуры позволяют добавлять экземпляры рабочих ролей в головной узел HPC Pack, который развертывается на виртуальной машине Azure.

>[AZURE.NOTE] Учебник по ускорению в Azure с помощью пакета HPC см. в разделе [Настройка гибридного кластера с пакетом HPC](../cloud-services/cloud-services-setup-hybrid-hpcpack-cluster.md). Обратите внимание на рекомендации в описанных ниже шагах, которые применяются к узлам Azure размера A8 и A9.

![Ускорение в Azure][burst]

### Рекомендации по использованию экземпляров A8 и A9

* **Прокси-узлы**. При каждом развертывании ускорения в Azure с помощью экземпляров с интенсивным использованием вычислительных ресурсов пакет HPC автоматически развертывает не менее 2 дополнительных экземпляров размера A8 в качестве прокси-узлов (в дополнение к указанным вами экземплярам рабочих ролей). Прокси-узлы используют ядра, выделенные для подписки, и за них взимается плата наряду с экземплярами рабочих ролей Azure.

### Действия

4. **Развертывание и настройка головного узла HPC Pack 2012 R2.**

    Загрузите новейшую версию пакета установки HPC Pack из [Центра загрузки Майкрософт](https://www.microsoft.com/download/details.aspx?id=49922). Требования и инструкции для подготовки к развертыванию ускорения в Azure см. в разделах [Руководство по началу работы с HPC Pack](https://technet.microsoft.com/library/jj884144.aspx) и [Ускорение в Azure с помощью пакета Microsoft HPC](https://technet.microsoft.com/library/gg481749.aspx).

5. **Настройка сертификата управления в подписке Azure.**

    Настройте сертификат для обеспечения безопасного соединения между головным узлом и Azure. Параметры и процедуры см. в разделе [Сценарии настройки сертификата управления Azure для пакета HPC](http://technet.microsoft.com/library/gg481759.aspx). Для тестового развертывания пакет HPC устанавливает сертификат управления Microsoft HPC Azure по умолчанию, который вы можете быстро отправить в подписку Azure.

6. **Создание новой облачной службы и учетной записи хранения.**

    С помощью классического портала Azure создайте облачную службу и учетную запись хранения для развертывания в регионе, в котором доступны экземпляры с ресурсоемкими вычислениями.

7. **Создание шаблона узла Azure.**

    Используйте мастер создания шаблона узла в диспетчере кластеров HPC. Пошаговые инструкции см. в разделе [Создание шаблона узла Azure](http://technet.microsoft.com/library/gg481758.aspx#BKMK_Templ) документа «Шаги по развертыванию узлов Azure с помощью пакета Microsoft HPC».

    Для первоначальных тестов рекомендуется настроить ручную политику доступности в шаблоне.

8. **Добавление узлов в кластер.**

    Используйте мастер добавления узла в диспетчере кластеров HPC. Дополнительные сведения см. в разделе [Добавление узлов Azure в кластер HPC Windows](http://technet.microsoft.com/library/gg481758.aspx#BKMK_Add).

    При указании размера узлов выберите A8 или A9.

9. **Запуск (подготовка) узлов и их подключение к сети для выполнения заданий.**

    Выберите узлы и используйте действие **Запуск** в диспетчере кластеров HPC. По завершении подготовки выберите узлы и используйте действие **Перевести в оперативный режим** в диспетчере кластеров HPC. После этого узлы будут готовы к выполнению заданий.

10. **Отправка заданий в кластер.**

    Используйте средства отправки заданий HPC Pack для выполнения заданий кластера. См. раздел [Пакет Microsoft HPC Pack: управление заданиями](http://technet.microsoft.com/library/jj899585.aspx).

11. **Остановка (отзыв) узлов**

    После завершения выполнения заданий отключите узлы и используйте действие **Остановить** в диспетчере кластеров HPC.





## Сценарий 2. Развертывание вычислительных узлов на виртуальных машинах с ресурсоемкими вычислениями (IaaS)

В этом сценарии развертываются головной узел HPC Pack и вычислительные узлы кластера на виртуальных машинах, присоединенных к домену Active Directory в виртуальной сети Azure. Пакет HPC предоставляет несколько [вариантов развертывания на виртуальных машинах Azure](virtual-machines-linux-hpcpack-cluster-options.md), в том числе сценарии автоматизированного развертывания и шаблоны быстрого запуска Azure. Например, приведенные далее рекомендации и действия помогут вам использовать [сценарий развертывания IaaS пакета HPC](virtual-machines-windows-classic-hpcpack-cluster-powershell-script.md) для автоматизации большей части этого процесса.

![Кластер на виртуальных машинах Azure][iaas]

### Рекомендации по использованию экземпляров A8 и A9

* **Виртуальная сеть** — укажите новую виртуальную сеть в регионе, в котором доступны экземпляры A8 и A9.


* **Операционная система Windows Server** — для поддержки соединения RDMA укажите операционную систему Windows Server 2012 R2 или Windows Server 2012 для виртуальных машин вычислительных узлов размера A8 или A9.


* **Облачные службы** — мы рекомендуем развертывать головной узел в одной облачной службе, а вычислительные узлы A8 и A9 — в другой облачной службе.


* **Размер головного узла** — добавляя виртуальные машины вычислительного узла в размере A8 или A9, для головного узла выберите размер не менее A4 (очень крупный).

* **Расширение HpcVmDrivers** — скрипт развертывания автоматически устанавливает агент ВМ Azure и расширение HpcVmDrivers, когда вы развертываете вычислительные узлы размера A8 или A9, в которых используется ОС Windows Server. HpcVmDrivers устанавливает драйверы на виртуальные машины вычислительного узла, чтобы они могли подключаться к сети RDMA.

* **Конфигурация сети кластера** — скрипт развертывания автоматически настраивает кластер HPC Pack с топологией 5 (все узлы в корпоративной сети). Эта топология обязательна для всех развертываний кластера HPC Pack на виртуальных машинах, включая оснащенные вычислительными узлами размера A8 или A9. В дальнейшем не следует изменять топологию сети кластера.

### Действия

1. **Создание головного узла кластера и виртуальных машин вычислительных узлов путем выполнения скрипта развертывания IaaS с пакетом HPC на клиентском компьютере.**

    Загрузите пакет скрипта развертывания IaaS для пакета HPC Pack из [Центра загрузки Майкрософт](https://www.microsoft.com/download/details.aspx?id=49922).

    Чтобы подготовить клиентский компьютер, создайте файл конфигурации скрипта и выполните скрипт. См. раздел [Создание кластера HPC с помощью скрипта развертывания IaaS с пакетом HPC Pack](virtual-machines-windows-classic-hpcpack-cluster-powershell-script.md). Для развертывания вычислительных узлов размера A8 и A9 см. дополнительные рекомендации далее в этой статье.

2. **Перевод вычислительных узлов в оперативный режим для запуска заданий.**

    Выберите узлы и используйте действие **Перевести в оперативный режим** в диспетчере кластеров HPC. После этого узлы будут готовы к выполнению заданий.

3. **Отправка заданий в кластер.**

    Подключитесь к головному узлу для отправки заданий или настройте для этого локальный компьютер. Сведения см. в разделе [Отправка заданий в кластер HPC в Azure](virtual-machines-windows-hpcpack-cluster-submit-jobs.md).

4. **Отключение узлов и их остановка (отзыв).**

    После выполнения заданий отключите узлы в диспетчере кластеров HPC. Затем с помощью средств управления Azure завершите их работу.




## Запуск приложений MPI на экземплярах A8 и A9

### Пример. Выполнение команды mpipingpong в кластере HPC Pack

Чтобы проверить развертывание экземпляров с ресурсоемкими вычислениями (которое выполнялось с помощью пакета HPC Pack), запустите команду **mpipingpong** этого пакета в кластере. Команда **mpipingpong** многократно отправляет пакеты данных из одного спаренного узла в другой, чтобы вычислить задержки и изменить пропускную способность и статистику для сети приложений с поддержкой RDMA. В этом примере показана типичная схема запуска задания MPI (в данном случае **mpipingpong**) с помощью команды кластера **mpiexec**.

В примере предполагается, что узлы Azure добавлены в конфигурации "ускорение в Azure" ([Сценарий 1](#scenario-1.-deploy-compute-intensive-worker-role-instances-(PaaS)) в данной статье). Если пакет HPC Pack развернут в кластере виртуальных машин Azure, необходимо изменить синтаксис команды, чтобы указать другую группу узлов и задать дополнительные переменные среды для направления сетевого трафика по сети RDMA.


Чтобы выполнить команду mpipingpong в кластере:


1. На головном узле или на соответствующим образом настроенном клиентском компьютере откройте командную строку.

2. Чтобы оценить задержку между парами узлов в развертывании ускорения Azure, состоящем из четырех узлов, запустите задание выполнения команды mpipingpong с небольшим размером пакетов и большим количеством итераций. Для этого введите такую команду:

    ```
    job submit /nodegroup:azurenodes /numnodes:4 mpiexec -c 1 -affinity mpipingpong -p 1:100000 -op -s nul
    ```

    Команда вернет идентификатор запущенного задания.

    Если кластер HPC Pack развернут на виртуальных машинах Azure, укажите группу узлов, которая содержит виртуальные машины вычислительных узлов, развернутые в одной облачной службе, и измените команду **mpiexec** следующим образом:

    ```
    job submit /nodegroup:vmcomputenodes /numnodes:4 mpiexec -c 1 -affinity -env MSMPI\_DISABLE\_SOCK 1 -env MSMPI\_PRECONNECT all -env MPICH\_NETMASK 172.16.0.0/255.255.0.0 mpipingpong -p 1:100000 -op -s nul
    ```

3. Чтобы после завершения задания просмотреть выходные данные (в данном случае — выходные данные задачи 1 задания), введите следующую команду

    ```
    task view <JobID>.1
    ```

    где & lt;*JobID*& gt; — это идентификатор запущенного задания.

    Выходные данные будут содержать сведения о задержке, подобные следующим.

    ![Задержка проверки связи][pingpong1]

4. Чтобы оценить пропускную способность между парами узлов ускорения Azure, запустите задание выполнения команды **mpipingpong** с большим размером пакетов и небольшим количеством итераций. Для этого введите такую команду:

    ```
    job submit /nodegroup:azurenodes /numnodes:4 mpiexec -c 1 -affinity mpipingpong -p 4000000:1000 -op -s nul
    ```

    Команда вернет идентификатор запущенного задания.

    В кластере HPC Pack, развернутом на виртуальных машинах Azure, измените команду, как указано в шаге 2.

5. Чтобы после завершения задания просмотреть выходные данные (в данном случае — выходные данные задачи 1 задания), введите следующую команду:

    ```
    task view <JobID>.1
    ```

  Выходные данные будут содержать сведения о пропускной способности, подобные следующим.

  ![Пропускная способность при «пинг-понговой» передаче данных][pingpong2]


### Рекомендации по приложениям MPI


Ниже приведены рекомендации по запуску приложений MPI на экземплярах Azure. Некоторые из них применимы только к развертыванию узлов Azure (экземпляров рабочих ролей, добавленных в конфигурации «ускорение в Azure»).

* Экземпляры рабочих ролей в облачной службе периодически повторно подготавливаются без уведомления системой Azure (например, для обслуживания системы или в случае сбоя экземпляра). Если экземпляр повторно подготавливается в то время, когда он выполняет задание MPI, экземпляр теряет все свои данные и возвращается в состояние, когда он был первоначально развернут, что может привести к сбою задания MPI. Чем больше узлов используется для одного задания MPI и чем дольше задание выполняется, тем вероятнее, что один из экземпляров будет подготовлен повторно во время выполнения задания. Это следует учитывать также в случае назначения одного узла в развертывании в качестве файлового сервера.


* Для запуска заданий MPI в Azure нет необходимости использовать экземпляры A8 и A9. Вы можете использовать экземпляр любого размера, поддерживаемый пакетом HPC. Тем не менее экземпляры A8 и A9 рекомендуются для выполнения относительно крупномасштабных заданий MPI, чувствительных к задержке и пропускной способности сети, соединяющей узлы. При использовании экземпляров, отличных от A8 и A9, для запуска заданий MPI, чувствительных к задержке и пропускной способности, мы рекомендуем выполнять мелкие задания, в которых отдельная задача работает только на нескольких узлах.

* Приложения, развернутые в экземплярах Azure, подлежат условиям лицензирования, связанным с приложением. Проконсультируйтесь с поставщиками всех коммерческих приложений насчет лицензирования или иных ограничений на запуск приложений в облаке. Не все поставщики предлагают лицензирование с оплатой по мере использования.


* Экземплярам Azure требуется дополнительная настройка для доступа к локальным узлам, общим ресурсам и серверам лицензий. Например, чтобы включить для узлов Azure доступ к локальному серверу лицензий, вы можете настроить виртуальную сеть Azure «сеть — сеть».


* Чтобы запустить приложение MPI в экземплярах Azure, следует зарегистрировать приложение MPI в брандмауэре Windows в экземплярах, запустив команду **hpcfwutil**. Это позволит осуществлять обмен данными MPI через порт, который динамически назначается брандмауэром.

    >[AZURE.NOTE] Для развертываний ускорения в Azure можно также настроить команду исключения брандмауэра с целью автоматического запуска на всех новых узлах Azure, которые добавляются к кластеру. Запустив команду **hpcfwutil** и убедившись в том, что ваше приложение работает, добавьте команду в скрипт запуска для узлов Azure. Дополнительные сведения см. в статье [Use a Startup Script for Azure Nodes](https://technet.microsoft.com/library/jj899632.aspx) (Использование скрипта запуска для узлов Azure).



* Пакет HPC использует переменную среды кластера CCP\_MPI\_NETMASK для указания диапазона допустимых адресов для связи MPI. Начиная с пакета HPC Pack 2012 R2, переменная среды кластера CCP\_MPI\_NETMASK влияет только на связь MPI между вычислительными узлами кластера, присоединенными к домену (локально или на виртуальных машинах Azure). Переменная игнорируется узлами, добавленными в рамках ускорения в конфигурацию Azure.


* Задания MPI не могут выполняться между экземплярами Azure, развернутыми в разных облачных службах (например, в развертываниях ускорения в Azure с разными шаблонами узлов или на вычислительных узлах виртуальных машин Azure, развернутых в нескольких облачных службах). При наличии нескольких развертываний узлов Azure, запущенных с разными шаблонами узлов, задание MPI должно выполняться только на одном наборе узлов Azure.


* При добавлении узлов Azure в кластер и переводе их в оперативный режим служба планировщика заданий HPC немедленно пытается запустить задания на узлах. Если в Azure можно запустить только часть имеющейся рабочей нагрузки, не забудьте обновить или создать шаблоны заданий, чтобы таким образом определить типы заданий, которые можно выполнять в Azure. Например, чтобы на узлах Azure выполнялись только те задания, которые были отправлены с использованием шаблона задания, добавьте свойство «Группы узлов» в шаблон задания и выберите AzureNodes в качестве обязательного значения. Для создания настраиваемых групп узлов Azure можно использовать командлет PowerShell Add-HpcGroup HPC.


## Дальнейшие действия

* Если требуется запускать приложения MPI для Linux, получающие доступ к сети RDMA в Azure, см. раздел [Настройка кластера Linux RDMA для запуска приложений MPI](virtual-machines-linux-classic-rdma-cluster.md).

<!--Image references-->
[burst]: ./media/virtual-machines-windows-classic-hpcpack-rdma-cluster/burst.png
[iaas]: ./media/virtual-machines-windows-classic-hpcpack-rdma-cluster/iaas.png
[pingpong1]: ./media/virtual-machines-windows-classic-hpcpack-rdma-cluster/pingpong1.png
[pingpong2]: ./media/virtual-machines-windows-classic-hpcpack-rdma-cluster/pingpong2.png

<!---HONumber=AcomDC_0323_2016-->