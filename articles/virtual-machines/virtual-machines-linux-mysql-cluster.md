<properties
	pageTitle="Использование наборов балансировки нагрузки для кластеризации MySQL в Linux"
	description="В этой статье описываются шаблоны настройки кластера Linux высокой доступности с балансировкой нагрузки в Azure на примере MySQL."
	services="virtual-machines"
	documentationCenter=""
	authors="bureado"
	manager="timlt"
	editor=""/>

<tags
	ms.service="virtual-machines"
	ms.workload="infrastructure-services"
	ms.tgt_pltfrm="vm-linux"
	ms.devlang="na"
	ms.topic="article"
	ms.date="04/14/2015"
	ms.author="jparrel"/>

# Использование наборов балансировки нагрузки для кластеризации MySQL в Linux

* [Подготовка](#getting-ready)
* [Установка кластера](#setting-up-the-cluster)
* [Установка MySQL](#setting-up-mysql)
* [Установка Corosync](#setting-up-corosync)
* [Установка Pacemaker](#setting-up-pacemaker)
* [Тестирование](#testing)
* [STONITH](#stonith)
* [Ограничения](#limitations)

## Введение

Целью этой статьи является исследование и демонстрация разных возможных подходов к развертыванию высокодоступных служб на основе Linux в Microsoft Azure на примере MySQL Server. Видеоролик, демонстрирующий этот подход, см. на [канале 9](http://channel9.msdn.com/Blogs/Open/Load-balancing-highly-available-Linux-services-on-Windows-Azure-OpenLDAP-and-MySQL).

Мы рассматриваем не использующее общие ресурсы высокодоступное решение MySQL с двумя узлами и одним хозяином на основе DRBD, Corosync и Pacemaker. В каждый момент времени MySQL работает только в одном узле. Операции чтения и записи из ресурса DRBD в каждый момент времени также возможны только в одном узле.

В данном случае не требуется решение с применением виртуального IP-адреса, такое как LVS, поскольку мы используем наборы балансировки нагрузки Microsoft Azure, чтобы обеспечить функциональность виртуального IP-адреса для циклического перебора, обнаружения конечной точки, удаления и восстановления после ошибки. Виртуальный IP-адрес — это IPv4-адрес с поддержкой глобальной маршрутизации, назначаемый Microsoft Azure при первом создании облачной службы.

Существуют и другие доступные архитектуры для MySQL, в том числе NBD Cluster, Percona и Galera, а также несколько решений ПО промежуточного слоя, включая по крайней мере одно, доступное в качестве виртуальной машины на сайте [VM Depot](http://vmdepot.msopentech.com). Поскольку эти решения могут реплицироваться в одноадресной (в отличие от многоадресной и широковещательной) передаче и не зависят от общего хранилища и множества сетевых интерфейсов, такие сценарии должны легко развертываться в Microsoft Azure.

Конечно, эти архитектуры кластеризации можно аналогичным образом распространить и на другие продукты, такие как PostgreSQL и OpenLDAP. Например, данная процедура балансировки нагрузки без использования общих ресурсов была успешно протестирована с OpenLDAP с несколькими хозяевами, и это можно увидеть в нашем блоге "Канал 9".

## Подготовка

Вам потребуется учетная запись Microsoft Azure с действующей подпиской и возможностью создания по крайней мере двух виртуальных машин (в данном примере использовался тип XS), сеть и подсеть, территориальная группа и группа доступности, а также возможность создания новых виртуальных жестких дисков в одном регионе с облачной службой и подключения их к виртуальным машинам Linux.

### Тестовая среда

- Ubuntu 13.10
  - DRBD
  - MySQL Server
  - Corosync и Pacemaker

### Территориальная группа

Территориальная группа для решения создается путем входа на портал Azure, перехода к разделу настроек и создания в нем новой территориальной группы. Этой территориальной группе будут назначаться выделенные ресурсы, созданные позднее.

### Сети

Создается новая сеть, и в ней создается подсеть. Мы выбрали сеть 10.10.10.0/24 только с одной подсетью /24.

### Виртуальные машины

Первая виртуальная машина Ubuntu 13.10 создается с помощью коллекции рекомендованных образов Ubuntu и получает имя `hadb01`. В процессе создается новая облачная служба с именем hadb. Это имя выбрано, чтобы подчеркнуть, что после добавления дополнительных ресурсов эта служба будет общедоступной с балансировкой нагрузки. Виртуальная машина `hadb01` создается обычным образом с помощью портала. Автоматически создается конечная точка для SSH, и выбирается наша созданная сеть. Мы также решили создать для ВМ новую группу доступности.

После создания первой виртуальной машины (технически при создании облачной службы) создается вторая виртуальная машина с именем `hadb02`. Вторая виртуальная машина также создается на портале с помощью образа Ubuntu 13.10 из коллекции, но выбирается не создание новой облачной службы, а использование уже существующей облачной службы, `hadb.cloudapp.net`. Сеть и группа доступности должны выбираться автоматически. Также будет создана конечная точка SSH.

После создания обеих виртуальных машин мы обратим внимание на порт SSH для `hadb01` (TCP 22) и `hadb02` (автоматически назначенный Azure).

### Подключаемое хранилище

Мы подключаем новый диск к обеим ВМ и в процессе создаем новые диски объемом в 5 ГБ. Эти диски будут размещаться в контейнере виртуальных жестких дисков, используемом для наших главных дисков операционной системы. После создания и подключения дисков не требуется перезапускать Linux, поскольку ядро увидит новое устройство (обычно это `/dev/sdc`, результат можно проверить в `dmesg`).

Затем мы создаем новые разделы с помощью `cfdisk` (в первую очередь раздел Linux) в каждой виртуальной машине и записываем новую таблицу разделов. **Не создавайте в этом разделе файловую систему**.

## Установка кластера

На обеих ВМ Ubuntu необходимо использовать APT для установки Corosync, Pacemaker и DRBD. С помощью `apt-get`:

    sudo apt-get install corosync pacemaker drbd8-utils.

**Не устанавливайте MySQL в это время**. Сценарии установки Debian и Ubuntu будут инициализировать каталог данных MySQL в `/var/lib/mysql`, а так как этот каталог будет заменен файловой системой DRBD, это необходимо делать позднее.

На этот момент также следует убедиться (с помощью `/sbin/ifconfig`), что обе виртуальные машины используют адреса в подсети 10.10.10.0/24 и могут проверять связь друг с другом по имени. При желании можно также убедиться (с помощью `ssh-keygen` и `ssh-copy-id`), что обе виртуальные машины могут взаимодействовать через SSH, не требуя пароля.

### Установка DRBD

Мы создадим ресурс DRBD, использующий базовый раздел `/dev/sdc1` для создания ресурса `/dev/drbd1`, который можно форматировать с помощью ext3 и использовать как в основном, так и в дополнительном узле. Для этого откроем `/etc/drbd.d/r0.res` и скопируем туда следующее определение ресурса. Это необходимо сделать на обеих ВМ.

    resource r0 {
      on `hadb01` {
        device  /dev/drbd1;
        disk   /dev/sdc1;
        address  10.10.10.4:7789;
        meta-disk internal;
      }
      on `hadb02` {
        device  /dev/drbd1;
        disk   /dev/sdc1;
        address  10.10.10.5:7789;
        meta-disk internal;
      }
    }

Затем инициализируем ресурс с помощью `drbdadm` на обеих виртуальных машинах:

    sudo drbdadm -c /etc/drbd.conf role r0
    sudo drbdadm up r0

Наконец, в основной виртуальной машине (`hadb01`) установим владение этим ресурсом DRBD:

    sudo drbdadm primary --force r0

Если проверить содержимое /proc/drbd (`sudo cat /proc/drbd`) на обеих виртуальных машинах, то там должно быть указано `Primary/Secondary` на `hadb01` и `Secondary/Primary` на `hadb02`, в соответствии с решением на этот момент. Диск в 5 ГБ будет синхронизирован в сети 10.10.10.0/24 бесплатно.

После синхронизации диска можно создать файловую систему в `hadb01`. При тестировании мы использовали файловую систему ext2, но следующая инструкция создает файловую систему ext3.

    mkfs.ext3 /dev/drbd1

### Подключение ресурса DRBD

Теперь в `hadb01` все готово для подключения ресурсов DRBD. В Debian и производных дистрибутивах в качестве каталога данных MySQL используется `/var/lib/mysql`. Поскольку MySQL еще не установлен, создадим этот каталог и подключим ресурс DRBD. На `hadb01`:

    sudo mkdir /var/lib/mysql
    sudo mount /dev/drbd1 /var/lib/mysql

## Установка MySQL

Теперь можно приступить к установке MySQL в `hadb01`:

    sudo apt-get install mysql-server

Для `hadb02` существует два варианта. Можно сейчас установить MySQL Server, который создаст /var/lib/mysql и включит туда новый каталог данных, а затем перейти к удалению содержимого. На `hadb02`:

    sudo apt-get install mysql-server
    sudo service mysql stop
    sudo rm –rf /var/lib/mysql/*

Второй вариант заключается в том, чтобы перейти на другой ресурс (`hadb02`), а затем установить там MySQL Server (сценарии установки заметят существующую установку и не будут ее затрагивать).

На `hadb01`:

    sudo drbdadm secondary –force r0

На `hadb02`:

    sudo drbdadm primary –force r0
    sudo apt-get install mysql-server

Если в текущий момент переход на другой ресурс не планируется, то первый вариант более простой, хотя, возможно, менее красивый. После выполнения этой установки можно начинать работу с базой данных MySQL. На `hadb02` (или на виртуальной машине, где действует один из серверов в соответствии с DRBD):

    mysql –u root –p
    CREATE DATABASE azureha;
    CREATE TABLE things ( id SERIAL, name VARCHAR(255) );
    INSERT INTO things VALUES (1, "Yet another entity");
    GRANT ALL ON things.* TO root;

**Внимание**! Последняя инструкция фактически отключает проверку подлинности привилегированного пользователя в этой таблице. Она включена только для иллюстрации, и в инструкциях GRANT для рабочей среды это необходимо изменить.

Кроме того, необходимо включить сеть для MySQL, если планируется выполнять запросы извне ВМ, что является целью данного руководства. На обеих виртуальных машинах откройте `/etc/mysql/my.cnf`, перейдите к параметру `bind-address` и измените его значение со 127.0.0.1 на 0.0.0.0. Сохранив этот файл, вызовите `sudo service mysql restart` в текущей основной виртуальной машине.

### Создание набора балансировки нагрузки MySQL

Вернемся на портал Azure и перейдем на виртуальную машину `hadb01`, а затем в раздел конечных точек. Создадим новую конечную точку, выберем в раскрывающемся меню пункт MySQL (TCP 3306) и установим флажок *Create new load balanced set* (Создать новый набор балансировки нагрузки). Назовем нашу конечную точку с балансировкой нагрузки `lb-mysql`. Не будем трогать большинство параметров, уменьшим только время до минимального значения 5 (секунд).

После создания конечной точки перейдем в раздел конечных точек на `hadb02` и создадим конечную точку, но выберем `lb-mysql`, а затем в раскрывающемся меню выберем пункт MySQL. Для этого действия можно также использовать Azure CLI.

Теперь у нас есть все, что требуется для работы с кластером вручную.

### Тестирование набора балансировки нагрузки

Тестирование можно выполнять с внешнего компьютера, с помощью клиента MySQL, а также приложений (например, приложения phpMyAdmin, работающего в качестве веб-сайта Azure). В данном случае использовалась программа командной строки MySQL на другом компьютере Linux:

    mysql azureha –u root –h hadb.cloudapp.net –e "select * from things;"

### Переход на другой ресурс вручную

Теперь можно смоделировать отработку отказа, завершив работу MySQL, переключившись на владельца DRBD и запустив MySQL снова.

На hadb01:

    service mysql stop && umount /var/lib/mysql ; drbdadm secondary r0

Затем на hadb02:

    drbdadm primary r0 ; mount /dev/drbd1 /var/lib/mysql && service mysql start

После выполнения перехода на другой ресурс вручную можно повторить удаленный запрос, и он должен прекрасно работать.

## Установка Corosync

Corosync — это базовая кластерная инфраструктура, необходимая для работы Pacemaker. Для пользователей Heartbeat версий 1 и 2 (а также других методологий, например Ultramonkey) Corosync — это ответвление функциональных возможностей CRM, в то время как функциональность Pacemaker остается более похожей на Heartbeat.

Основное ограничение для Corosync в Azure заключается в том, что в Corosync многоадресные взаимодействия предпочтительнее одноадресных и широковещательных однако в сети Microsoft Azure предусмотрена поддержка только одноадресных взаимодействий.

К счастью, в Corosync имеется рабочий одноадресный режим, и единственное фактическое ограничение заключается в том, что поскольку все узлы не взаимодействуют между собой *автоматически*, необходимо задавать узлы в файлах конфигурации вместе с их IP-адресами. Можно воспользоваться примерами файлов Corosync для Unicast и просто изменить адрес привязки, списки узлов и каталог для ведения журнала (в Ubuntu `/var/log/corosync` используется, а в примерах файлов — `/var/log/cluster`), а также инструменты включения кворума.

**Обратите внимание на директиву `transport: udpu` ниже и на заданные вручную IP-адреса для узлов**.

На `/etc/corosync/corosync.conf` для обоих узлов:

    totem {
      version: 2
      crypto_cipher: none
      crypto_hash: none
      interface {
        ringnumber: 0
        bindnetaddr: 10.10.10.0
        mcastport: 5405
        ttl: 1
      }
      transport: udpu
    }

    logging {
      fileline: off
      to_logfile: yes
      to_syslog: yes
      logfile: /var/log/corosync/corosync.log
      debug: off
      timestamp: on
      logger_subsys {
        subsys: QUORUM
        debug: off
        }
      }

    nodelist {
      node {
        ring0_addr: 10.10.10.4
        nodeid: 1
      }

      node {
        ring0_addr: 10.10.10.5
        nodeid: 2
      }
    }

    quorum {
      provider: corosync_votequorum
    }

Скопируем этот файл конфигурации на обе ВМ и запустим Corosync в обоих узлах:

    sudo service start corosync

Вскоре после запуска службы должен быть установлен кластер в текущем кольце и построен кворум. Эту функциональность можно проверить, просмотрев журналы или выполнив следующую команду:

    sudo corosync-quorumtool –l

Должен получиться результат, подобный следующему:

![corosync-quorumtool -l sample output](media/virtual-machines-linux-mysql-cluster/image001.png)

## Установка Pacemaker

Pacemaker использует кластер для мониторинга ресурсов, обнаружения выхода из строя основных ресурсов и переключения на дополнительные ресурсы. Ресурсы можно задавать с помощью ряда доступных скриптов или с помощью скриптов LSB (подобных init), а также другими способами.

Нам нужно, чтобы Pacemaker "владел" ресурсом DRBD, точкой подключения и службой MySQL. Если Pacemaker может включать и выключать, подключать и отключать DRBD, а также запускать и останавливать MySQL в правильном порядке при возникновении аварийной ситуации, наша настройка завершена.

При первой установке Pacemaker конфигурация должна быть довольно простой, как показано ниже:

    node $id="1" hadb01
      attributes standby="off"
    node $id="2" hadb02
      attributes standby="off"

Проверьте ее, выполнив `sudo crm configure show`. Теперь создайте файл (например, `/tmp/cluster.conf`) со следующими ресурсами:

    primitive drbd_mysql ocf:linbit:drbd \
          params drbd_resource="r0" \
          op monitor interval="29s" role="Master" \
          op monitor interval="31s" role="Slave"

    ms ms_drbd_mysql drbd_mysql \
          meta master-max="1" master-node-max="1" \
            clone-max="2" clone-node-max="1" \
            notify="true"

    primitive fs_mysql ocf:heartbeat:Filesystem \
          params device="/dev/drbd/by-res/r0" \
          directory="/var/lib/mysql" fstype="ext3"

    primitive mysqld lsb:mysql

    group mysql fs_mysql mysqld

    colocation mysql_on_drbd \
           inf: mysql ms_drbd_mysql:Master

    order mysql_after_drbd \
           inf: ms_drbd_mysql:promote mysql:start

    property stonith-enabled=false

    property no-quorum-policy=ignore

Загрузите его в конфигурацию (это требуется сделать только в одном узле):

    sudo crm configure
      load update /tmp/cluster.conf
      commit
      exit

Убедитесь, что Pacemaker запускается при загрузке в обоих узлах:

    sudo update-rc.d pacemaker defaults

Через несколько секунд с помощью `sudo crm_mon –L` убедитесь, что один из узлов стал главным в кластере и на нем запущены все ресурсы. Для проверки запущенных ресурсов можно использовать mount и ps.

На следующем снимке экрана показан `crm_mon` с одним остановленным узлом (выход с помощью клавиш CTRL+C).

![crm\_mon node stopped](media/virtual-machines-linux-mysql-cluster/image002.png)

На следующем снимке экрана показаны оба узла, один из которых главный, а второй подчиненный:

![crm\_mon operational master/slave](media/virtual-machines-linux-mysql-cluster/image003.png)

## Тестирование

Теперь все подготовлено для моделирования автоматической отработки отказа. Это можно осуществить двумя способами: мягким и жестким. Мягкий путь заключается в использовании функции завершения работы кластера: ``crm_standby -U `uname -n` -v on``. Если эта команда используется на главном узле, его функции принимает на себя подчиненный узел. Не забудьте восстановить его прежнее состояние с помощью параметра off (в противном случае crm\_mon будет сообщать, что один из узлов находится в резервном режиме).

Жесткий способ — это остановка работы основной ВМ (hadb01) на портале или изменение уровня запуска (runlevel) на ВМ (например, halt, shutdown), а затем сообщение Corosync и Pacemaker, что уровень главного узла понижен. Это можно протестировать (что имеет смысл для периодов обслуживания), но можно также принудительно вызвать этот сценарий, просто заморозив ВМ.

## STONITH

Должна присутствовать возможность остановки работы виртуальной машины с помощью Azure CLI вместо сценария STONITH, который управляет физическим устройством. Можно взять `/usr/lib/stonith/plugins/external/ssh` за основу и включить STONITH в конфигурации кластера. Требуется установка Azure CLI на глобальном уровне и загрузка параметров и профиля публикации для пользователя кластера.

Образец кода для ресурса можно найти на сайте [GitHub](https://github.com/bureado/aztonith). Вам необходимо изменить конфигурацию кластера, добавив в `sudo crm configure` следующее:

    primitive st-azure stonith:external/azure \
      params hostlist="hadb01 hadb02" \
      clone fencing st-azure \
      property stonith-enabled=true \
      commit

**Примечание.** Этот сценарий не выполняет проверки состояния работоспособности. Исходный ресурс SSH имел 15 проверок связи, но время восстановления для ВМ Azure может изменяться в более широких пределах.

## Ограничения

Действительны следующие ограничения.

- Сценарий linbit ресурса DRBD, управляющий DRBD как ресурсом в Pacemaker, использует `drbdadm down` при остановке работы узла, даже если узел просто переходит в режим ожидания. Это лучший подход, поскольку подчиненный узел не будет синхронизировать ресурс DRBD при выполнении операций записи в главном узле. Если вовремя не произойдет сбой главного узла, подчиненный может принять на себя более старое состояние файловой системы. Имеется два способа разрешения этой проблемы:
  - принудительное выполнение `drbdadm up r0` во всех узлах кластера с помощью локальной (не кластерной) программы наблюдения watchdog, или
  - изменение сценария linbit DRBD в `/usr/lib/ocf/resource.d/linbit/drbd` таким образом, чтобы не вызывалась команда `down`.
- Подсистеме балансировки нагрузки потребуется не менее 5 секунд на ответ, поэтому приложения должны быть кластерными и более устойчивыми к времени ожидания; также могут быть полезными другие архитектуры, например, очереди в приложениях, ПО промежуточного слоя запросов и т. п.
- Настройка MySQL необходима, чтобы гарантировать выполнение операций записи в разумном темпе и запись кэша на диск с максимально возможной частотой для сокращения потерь памяти.
- Производительность операций записи будет зависеть от межсоединения ВМ в виртуальном коммутаторе, поскольку это механизм, используемый DRBD для репликации устройства.
 

<!---HONumber=August15_HO6-->