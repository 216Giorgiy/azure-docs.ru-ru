<properties 
	title="" 
	pageTitle="Как выбрать алгоритм в Машинном обучении Azure | Azure" 
	description="В этом разделе объясняется, как выбрать алгоритм в Машинном обучении Azure." 
	services="machine-learning"
	documentationCenter="" 
	authors="bradsev" 
	manager="paulettm" 
	editor="cgronlun"/>

<tags 
	ms.service="machine-learning" 
	ms.workload="data-services" 
	ms.tgt_pltfrm="na" 
	ms.devlang="na" 
	ms.topic="article" 
	ms.date="02/9/2015" 
	ms.author="bradsev" />


# Как выбрать алгоритм в Машинном обучении Azure

В этом разделе объясняются некоторые основные аспекты подхода к машинному обучению и описывается, в частности, как выбрать соответствующий алгоритм, чтобы проанализировать данные заданного типа, ответить на поставленный вопрос, выполнить указанную задачу или определить критерии для принятия решения. При использовании машинного обучения для выполнения анализа нам, как правило, требуется ответить на два вопроса: 

* Какой вид анализа необходим для достижения поставленных целей при использовании доступных данных? 
* Какой наиболее подходящий алгоритм или какую модель следует использовать для этого анализа?

Далее рассматриваются три типа анализа и сравниваются причины их использования: 

* классификация; 
* регрессия; 
* кластеризация.

Кроме того, здесь рассматриваются различные алгоритмы для каждого из этих типов анализа и доступные в Машинном обучении Azure модули, которые их содержат. 


<a name="anchor-1"></a>
##**Машинное обучение**

Машинное обучение - это дисциплина, изучающая класс управляемых данными алгоритмов, в том смысле, что они обучаются по данным, а не используют конкретную, предварительно определенную модель для их проверки. Идея заключается в том, чтобы получать знания более индуктивно, изучая закономерности в наборе данных, а не с помощью так называемого гипотетически-дедуктивного метода, при применении которого сначала предпринимается попытка угадать подходящую модель для всего набора данных, а затем проверить ее эмпирически. Существует два типа такого обучения на основе данных: контролируемое и неконтролируемое. 

<a name="anchor-2"></a>
##**Контролируемое обучение**  

Для контролируемого обучения требуется, чтобы целевая переменная была определена должным образом и чтобы было предоставлено достаточное количество ее значений. 

Контролируемое обучение представляет собой такой тип обучения, которое выполняется, когда известны правильные выходные результаты (или целевые переменные) для вводимых обучающих экземпляров. Алгоритм машинного обучения обучается для того, чтобы найти модель (то есть, правило или функцию), которая сопоставляет входные данные с известными выходными значениями. Этот механизм работает, как диспетчер, который может сообщить агенту алгоритма о том, правильно ли он сопоставляет входные данные с выходными. После завершения процесса обучения работоспособную модель можно применить к новым входным данным для прогнозирования ожидаемого результата, в котором, в отличие от набора данных для обучения, целевое значение не известно заранее.

Тип модели зависит от характера целевой переменной.

![screenshot_of_experiment](./media/machine-learning-algorithm-choice/help9.png)

Существуют две общие категории анализа с использованием контролируемого обучения: классификация и регрессия. Контролируемое обучение довольно часто используется для решения *classification problems*, так как цель зачастую состоит в том, чтобы обучить компьютер созданной системе классификации. Ответы, как правило, представляют собой несколько известных значений (меток), таких как 'true' или 'false'. Алгоритмы классификации применяются к номинальным, а не порядковым значениям ответа. Типичный пример обучения классификации - распознавание цифр. В более широком понимании обучение классификации подходит для любой проблемы, при решении которой создается классификация, которую легко определить.

При контролируемом обучении исследуемые переменные можно разделить на две группы: независимые переменные (также называемые предикторами) и зависимые переменные (также называемые переменными ответа). Цель анализа - указать связь между независимыми и зависимыми переменными, как это делается в *regression analysis*. Значения зависимой переменной должны быть известны для достаточно большой части набора данных. В регрессии ответы или выходная переменная принимают непрерывные значения, такие как количество километров, которые конкретный автомобиль может проехать на одном литре топлива, возраст человека и т. д.

Контролируемое обучение также является наиболее распространенным способом обучения нейронных сетей и деревьев принятия решений. Оба этих метода сильно зависят от информации, предоставляемой предварительно определенными классификациями. 

* При обучении нейронных сетей классификация используется для определения ошибки сети и настройки сети таким образом, чтобы свести к минимуму воздействие такой ошибки. 
* При обучении деревьев принятия решений классификации используются для определения атрибутов, предоставляющих большую часть информации, которую можно использовать для решения задачи классификации.  

В обоих этих примерах требуется определенная степень "контроля" в том смысле, что в них нужно предоставлять предварительно определенные классификации. 

При распознавании речи с помощью скрытых моделей Маркова и байесовских сетей элементы контроля применяются также для настройки параметров, которые позволяют свести к минимуму воздействие ошибки на заданные входные данные. 

<a name="anchor-3"></a>
##**Неконтролируемое обучение**

В машинном обучении задача неконтролируемого обучения заключается в поиске шаблонов или скрытой структуры в неразмеченных данных. Модели не предоставляются "правильные результаты" для набора данных, по которому будет проводится обучение. Так как примеры, предоставляемые ученику, неразмечены, сигнал, оповещающий об ошибке или успехе для оценки возможного решения, отсутствует. Цель заключается в том, чтобы обучить компьютер тому, как выполнить что-то без явных указаний о том, как это сделать! При неконтролируемом обучении все переменные обрабатываются одинаково вне зависимости от типа (независимые или зависимые). Однако такое обучение все же преследует определенную цель. Это может быть цель общего характера, например сокращение объема данных, или более конкретная цель, например поиск кластеров. 

В Машинном обучении Azure можно выполнять как неконтролируемое, так и контролируемое обучение при **классификации**, **кластеризации** и **регрессии**.

   ![screenshot_of_experiment](./media/machine-learning-algorithm-choice/help2.png)

<a name="anchor-4"></a>
##**Кластеризация**  
Кластеризация - пример неконтролируемого обучения. В этом типе обучения цель заключается в том, чтобы определить сходства в данных для обучения и разделить набор данных на подмножества, разграниченные по этим сходным чертам. Расчет на то, что наиболее значимые кластеры, обнаруженные в ходе этих управляемых данными процедур, согласуются с интуитивной классификацией, зачастую вполне оправдан, хотя и не всегда. 

Несмотря на то, что алгоритм не назначает этим кластерам соответствующие имена, он может создавать их, а затем использовать для прогнозирования сходных черт, которые предположительно должны быть в новых примерах, классифицируя их как принадлежащие к наиболее подходящему кластеру. Это управляемый данными подход, который дает неплохие результаты при наличии достаточного количества данных. Например, алгоритмы фильтрации информации из социальных сетей, примером которых могут служить алгоритмы, используемые компанией Amazon.com для рекомендации книг, предполагают поиск схожих групп людей и назначение новых пользователей в эти группы для предоставления рекомендаций.

Алгоритм кластеризации, доступный в Машинном обучении Azure, - это кластеризация методом К-средних.

![screenshot_of_experiment](./media/machine-learning-algorithm-choice/help10.png)

Метод К-средних - один из простейших алгоритмов неконтролируемого обучения, который помогает решить известные проблемы кластеризации. Метод K-средних предполагает кластеризацию данных в попытке разделить примеры на N групп с равной дисперсией и свести к минимуму критерий, известный как инерция или сумма квадратов в пределах кластера. Для этого алгоритма необходимо указывать количество кластеров. Он хорошо обрабатывает большое количество примеров и использовался в разнообразных областях применения во множестве сфер деятельности.

Модуль **Кластеризация методом K-средних**, реализующий метод К-средних, возвращает необученную модель кластеризации методом К-средних, которую можно передать в модуль **Обучение модели кластеризации** для обучения.

![screenshot_of_experiment](./media/machine-learning-algorithm-choice/k4.png)

На этом рисунке показано, что при использовании кластеризации методом К-средних необходимо настроить определенные параметры. Метод К-средних находит указанное число кластеров для набора D-мерных точек данных. Начиная с *initial set of K centroids*, метод использует алгоритм Ллойда для итеративного уточнения расположения центроидов. Выполнение алгоритма прекращается, когда стабилизируются центроиды или когда выполняется *specified number of iterations*.
Модуль инициализирует массив "K на D" с использованием последних центроидов, определяющих K кластеров, которые были обнаружены в N точках данных. Алгоритм также использует вектор длиной N, при этом каждая точка данных назначена одному из K кластеров.
Если требуется найти определенное число кластеров (К), модуль назначает первые К точек данных К кластерам по порядку.
Этот модуль также принимает или создает начальные точки для определения исходной конфигурации кластера.
*Метрика* определяет метод, используемый для измерения расстояния между точкой данных и центроидом.
Каждая точка данных назначается кластеру, центроид которого расположен ближе всего к этой точке данных. По умолчанию в методе используется *Euclidean metric*. В качестве альтернативной метрики для метода также можно использовать *cosine metric*.
Обратите внимание, что метод К-средних может найти только локально оптимальную конфигурацию кластера для набора данных. При наличии другой исходной конфигурации метод может найти другую, возможно, улучшенную конфигурацию.

<a name="anchor-5"></a>
##**Классификация**  
При анализе классификации примеры разделяются на классы и используется обученный набор предварительно помеченных данных. Этот метод используется, чтобы спрогнозировать принадлежность экземпляров данных к определенным группам. 
В Машинном обучении Azure доступны следующие алгоритмы классификации.

![screenshot_of_experiment](./media/machine-learning-algorithm-choice/help3.png)

 *Two-Class algorithms* используются для двоичных переменных ответа ("да" или "нет", "0" или "1", True или False и т. д.), а *Multiclass algorithms* - для любой номинальной переменной ответа, позволяющей классифицировать экземпляры по более чем двум классам.

### Рекомендации по выбору алгоритма классификации
Такой длинный список алгоритмов вызывает ряд вопросов: 

* Как узнать, какой из этих классификаторов наиболее подходящий для конкретного набора данных? 
* Может ли один из них оказаться наиболее логичным вариантом? 
* Почему следует выбрать именно его?

Один из рекомендуемых подходов заключается в том, чтобы протестировать несколько разных классификаторов, а также разных наборов параметров в каждом из алгоритмов, а затем выбрать лучшие из них с помощью перекрестной проверки. Ниже приведены некоторые общие рекомендации, с которых можно начать выбор алгоритма. При выборе используемого алгоритма необходимо учитывать следующее:

**Каков размер данных для обучения?**
Если обучающий набор небольшого размера и вам необходимо обучить контролируемый классификатор, согласно теории машинного обучения следует выбрать классификаторы с высоким значением смещения или низкой дисперсией, такие как классификаторы упрощенного алгоритма Байеса. Эти классификаторы имеют преимущество перед классификаторами с низким значением смещения или высокой дисперсией, такими как классификаторы по методу k-ближайших соседних результатов, так как для последнего типа классификаторов характерна склонность к лжевзаимосвязям. Тем не менее, по мере роста обучающего набора классификаторы с низким значением смещения или высокой дисперсией начинают давать лучшие результаты (меньшую асимптотическую погрешность), ведь классификаторы с высоким значением смещения не достаточно мощны, чтобы предоставлять точные модели. Теоретические и эмпирические результаты указывают на то, что в такой ситуации прекрасно подходит упрощенный алгоритм Байеса. Однако учтите, что в большинстве случаев эффективнее улучшить набор данных, чем алгоритм. Кроме того, существенное преимущество дает разработка правильных признаков. При наличии большого набора данных используемый алгоритм классификации может не иметь столь важного значения для эффективности классификации. Поэтому, возможно, самый оптимальный вариант - это выбор алгоритма на основе  показателей масштабируемости, скорости или простоты использования.

**Каким образом лучше всего выполнять обучение: поэтапно или в пакетном режиме?** 
Если вам необходимо часто обновлять свой классификатор новыми данными (или использовать большой объем данных), возможно, лучше использовать байесовские алгоритмы, которые очень эффективно выполняют обновление. Как нейронные сети, так и SVM требуют использования пакетного режима при работе с данными для обучения.

**Какие данные вы используете: исключительно категориальные, исключительно числовые или их сочетание?** 
Байесовский классификатор показывает самые лучшие результаты при работе с категориальными и биномиальными данными. Деревья принятия решений не могут прогнозировать числовые значения.

**Существует ли необходимость в понимании того, как работает классификатор, для вас или вашей аудитории?**  Используйте байесовский метод или дерево принятия решений, так как принцип их работы с легкостью воспринимает большинство пользователей. Нейронные сети и SVM - это "черные ящики" в том смысле, что нельзя увидеть применяемый ими метод классификации данных.

**Насколько быстро вам необходимо создавать классификацию?** 
Метод SVM выполняет быструю классификацию, так ему необходимо только определить, на какой стороне "линии" находятся ваши данные. Деревья принятия решений могут выполнять классификацию медленно, особенно если для них характерна сложная структура (например, много ветвей).

**Насколько сложна проблема или какой сложности решение она требует?** Нейронные сети и SVM могут обрабатывать сложную нелинейную классификацию.

### Преимущества и недостатки алгоритмов классификации
Для каждого из этих алгоритмов классификации характерны некоторые преимущества и  недостатки.

<a name="anchor-5a"></a>
**Преимущества и недостатки логистической регрессии**   
Логистический регрессионный анализ основывается на вычислении вероятности результата в виде соотношения вероятности получения результатов, разделенной на вероятность их отсутствия. Логистическая модель -  это параметрическая модель, и поэтому ее преимуществом является то, что она предоставляет информацию о влиянии каждой предикторной переменной на переменную ответа. Благодаря наличию естественных вероятностных интерпретаций (в отличие от деревьев принятия решений или SVM) можно с легкостью обновлять модель для использования с новыми данными. Такую модель можно регуляризовать множеством способов. Кроме того, с ней не придется уделять много внимания корреляции признаков, как при использовании упрощенного алгоритма Байеса. Логистическую регрессию удобно использовать если: 

* необходима вероятностная схема, которая позволяет с легкостью настроить пороговые значения классификации, чтобы, скажем, убедиться в правильности или получить доверительные интервалы; 
* в будущем ожидается получение большего объема данных для обучения, которые необходимо быстро включить в модель. 

Логистическая регрессия классифицирует данные высокой размерности лучше, чем дерево принятия решений. Пусть, например, нужно классифицировать текст более чем 100 тысяч документов, в которых содержатся 500 тысяч разных слов (признаков). В этом случае более эффективным является такое простое правило, как обучение линейной гиперплоскости, так как в деревьях принятия решений слишком много степеней свободы и для них характерно образование лжевзаимосвязей. Вы можете выбрать признаки, чтобы использовать их в дереве принятия решений для классификации текстовых данных, но в случае выбора слишком сокращенного подмножества признаков будет утеряно много ценных данных. При применении моделей обучения к данным высокой размерности значение дисперсионных погрешностей может легко увеличиться, поэтому предпочтительнее использовать простые модели с высоким значением смещения. 

Недостаток логистической регрессии заключается в нестабильной работе, которая проявляется, когда одним предиктором можно практически полностью объяснить переменную ответа, потому что коэффициент этой переменной резко увеличится.

<a name="anchor-5b"></a>
**Преимущества и недостатки деревьев принятия решений**   
[Деревья принятия решений](http://research.microsoft.com/pubs/155552/decisionForests_MSR_TR_2011_114.pdf) просты в понимании и интерпретации. Они непараметричны и с легкостью обрабатывают взаимодействия признаков, поэтому не нужно беспокоиться о выбросах или о линейной разделимости данных (например, деревья принятия решений легко справляются со случаями, когда на нижнем пределе некоего признака x находится класс A, в середине диапазона - класс B, а на верхнем пределе - также класс A). Деревья принятия решений создадут выходные данные в виде правил и метрик, таких как "Поддержка", "Достоверность" и "Подъем". 

Один из недостатков заключается в том, что такие деревья принятия решений не поддерживают обучение по сети, поэтому при поступлении новых примеров дерево придется перестраивать. Другим недостатком является возникновение лжевзаимосвязей, но его легко исправить с помощью таких ансамблевых методов, как метод случайного леса (или увеличивающихся деревьев). Кроме того, метод случайного леса зачастую позволяет решить многие проблемы классификации. Как правило, он немного эффективнее, чем метод SVM. В отличие от SVM, случайные леса масштабируются, быстро работают, а также освобождают от необходимости настраивать целый ряд параметров.


<a name="anchor-5c"></a>
**Преимущества и недостатки SVM**   
Метод опорных векторов (SVM) эффективно работает в пространствах высокой размерности. Этот метод эффективен даже в тех случаях, когда количество измерений больше, чем количество примеров. Однако, если количество признаков значительно больше, чем количество примеров, применение этого метода, вероятно, будет не столь эффективным. Данный метод также эффективно использует память за счет применения подмножества точек обучения для функции принятия решений (известной как опорные векторы).  Это очень универсальный метод: для функции принятия решения можно указывать различные функции ядра. Обычно указываются стандартные ядра, но можно также указать и пользовательские. Функция ядра используется для преобразования примеров для обучения низкой размерности в примеры для обучения более высокой размерности, что удобно для решения проблем линейной разделимости. 

Однако SVM не предоставляет непосредственные оценки вероятности. Они вычисляются с помощью затратной пятикратной перекрестной проверки. Благодаря высокой точности, подтвержденному теорией отсутствию склонности к лжевзаимосвязям и использованию соответствующих ядер этот метод эффективен, даже если данные в базовом пространстве признаков неразделимы линейно. Метод широко применяется для решения проблем классификации текстов, где обычно используются пространства очень высокой размерности. В отличие от метода с использованием лесов, метод SVM был создан в качестве двухклассового классификатора. Тем не менее, недавно он был адаптирован для работы с несколькими классами. Мы можем использовать метод обучения по схеме "один по всем", чтобы создать многоклассовый классификатор, который может быть менее оптимальным. Так как SVM может обрабатывать только двухклассовые выходные данные (такие как категориальная выходная переменная с многообразием 2) с N классами, этот метод обучает N экземпляров SVM (SVM 1 изучает "Output==1" по "Output != 1", SVM 2 изучает "Output==2" по "Output != 2",..., SVM N изучает "Output==N" по "Output != N"). Затем, чтобы спрогнозировать выходные данные для новых входных данных, создаются прогнозы для каждой модели SVM и определяется прогноз, которой наиболее приближен к области положительных результатов.

<a name="anchor-5d"></a>
**Преимущества и недостатки упрощенного алгоритма Байеса**   
[Классификаторы упрощенного алгоритма Байеса](http://www.aaai.org/Papers/FLAIRS/2004/Flairs04-097.pdf) часто применяются для решения проблем классификации. Они предполагают, что признаки независимы. Именно поэтому этот алгоритм называется 'naive'. Если предположение условной независимости этого алгоритма подтверждается, классификатор упрощенного алгоритма Байеса сойдется быстрее, чем избирательные модели, такие как логистическая регрессия. Поэтому в таких случаях требуется меньше данных для обучения. 

Даже если предположение этого алгоритма не подтверждается, классификатор упрощенного алгоритма Байеса все равно зачастую дает эффективные результаты.  Как оказалось, упрощенный алгоритм Байеса подходит не только для независимых признаков, но и для признаков со схожими зависимостями. Таким образом, упрощенный алгоритм Байеса - хороший выбор, если нужен быстрый, простой и эффективный метод.

Его основной недостаток заключается в том, что он не может изучить взаимодействия признаков (например, его нельзя обучить тому, что хотя человеку нравятся фильмы с Брэдом Питтом и Toмом Крузом, он не выносит фильмы, в которых играют оба эти актера).


<a name="anchor-5e"></a>
**Один по всем**  
"Один по всем" - это стратегия, которая позволяет устранить одну серьезную проблему, связанную с многоклассовой классификацией, сведя ее к набору нескольких задач двоичной классификации. Эта стратегия предусматривает обучение одного классификатора каждого класса, при котором примеры одного класса рассматриваются как положительные, а примеры всех других классов - как отрицательные. Для этой стратегии требуются не только метка класса, но и базовые классификаторы для получения реальной оценки достоверности решения. Дискретные метки классов могут привести к неоднозначности, когда для одного примера прогнозируются несколько классов.	


<a name="anchor-6"></a>
##**Регрессия**  
При регрессионном анализе новые значения прогнозируются на основе последнего вывода. Новые значения зависимой переменной вычисляются на основе значения одного или нескольких измеренных атрибутов. Ниже приведены различные алгоритмы регрессии, доступные в Машинном обучении Azure:

![screenshot_of_experiment](./media/machine-learning-algorithm-choice/help4.png)

В зависимости от сценария использования и имеющихся данных одному алгоритму или модулю отдается предпочтение над другим. Далее приведено описание некоторых алгоритмов регрессии и основных сценариев их использования.

<a name="anchor-6b"></a>
**Байесовская линейная регрессия**                      
Байесовская линейная регрессия - это подход к линейной регрессии, который предусматривает выполнение статистического анализа в контексте байесовского вывода. Явные результаты доступны для апостериорных распределений вероятности параметров модели при гауссовском распределении ошибок модели, если можно предположить определенную форму предыдущего распределения.

<a name="anchor-6f"></a>
**Регрессия увеличивающегося дерева принятия решений**  
Регрессия позволяет вычислить связь между предиктором и переменными ответа. Структура дерева регрессии аналогична структуре дерева классификации. Конечные узлы являются спрогнозированными значениями функции (модели). Спрогнозированные значения ограничены значениями на конечных узлах. Ниже приведены некоторые преимущества использования деревьев принятия решений: 

* правила принятия решений легко интерпретировать; 
* они непараметрические, поэтому можно легко включить диапазон уровней числовых или категориальных данных и не нужно использовать унимодальные данные для обучения; 
* они надежные с точки зрения выбросов в данных для обучения; 
* после создания правил можно выполнять быструю классификацию. 

Недостатки использования деревьев принятия решений заключаются в том, что для них характерно возникновение лжевзаимосвязей в данных для обучения, что может привести к неудовлетворительным результатам при применении к полному набору данных, а также в том, что для переменной ответа в данных для обучения можно предсказать только верхний и нижний пределы.

<a name="anchor-6g"></a>
**Регрессия леса принятия решений**  
Леса принятия решений можно использовать для классификации (категориальные переменные) и регрессии (непрерывные переменные). Леса регрессии можно использовать для нелинейной регрессии зависимых переменных, если заданы независимые входные данные. Как входные, так и выходные данные могут быть многомерными. Леса регрессии используются не так часто, как аналогичные методы классификации. Основное отличие заключается в том, что метка выходных данных лесов принятия решений, которая будет связана с входными данными, а значит и обучающие метки, должны быть непрерывными. Следовательно, целевую функцию необходимо адаптировать соответствующим образом. Преимущества лесов регрессии во многом схожи с преимуществами лесов классификации, например эффективность и гибкость.

<a name="anchor-6a"></a>
**Линейная регрессия**  
Линейная регрессия широко используется для моделирования связи между скалярной зависимой переменной Y и одной или несколькими независимыми переменными, обозначаемыми X. Как правило, ее используют для прогнозирования и сокращения объема. Линейную регрессию можно использовать, чтобы установить соответствие модели прогнозирования наблюдаемому набору данных значений Y и X. Этот тип регрессии предполагает, что базовая структура Y - это линейная комбинация переменных X. Если дополнительное значение X предоставляется без сопутствующего значения y, для прогнозирования этого значения Y можно использовать соответствующую модель линейной регрессии. Чтобы установить соответствие модели линейной регрессии, обычно используется подход наименьших квадратов, но для этого существуют и другие способы.

<a name="anchor-6c"></a>
**Регрессия нейронной сети**  
Нейронные сети - удобный статистический инструмент для непараметрической регрессии. Непараметрическая регрессия устраняет проблему попытки установить соответствие модели для переменной Y на основе набора возможных независимых переменных X1; : : : ;XP, где связь между X и Y может быть сложнее, чем простая линейная связь.

<a name="anchor-6d"></a>
**Порядковая регрессия**   
Порядковая регрессия - это тип регрессионного анализа, используемый для моделирования и прогнозирования порядковой зависимой переменной. Для порядковых зависимых переменных можно ранжировать значения, однако фактическое расстояние между категориями неизвестно. Важна только относительная упорядоченность разных значений. Так как метки или целевые значения имеют естественный порядок или ранжирование, в качестве порядковой цели можно использовать любой числовой столбец. Для ранжирования используется естественный порядок чисел. Болезни размещаются на шкале в порядке от наименее тяжелой до наиболее тяжелой. Участники опроса выбирают один из двух ответов на шкале: они могут быть полностью согласны или категорически не согласны. Учащиеся размещаются на шкале в порядке от А до F. Однако, по существу, порядковая регрессия является расширением логистической регрессии, которая основана на модели пропорциональных вероятностей.


<a name="anchor-6e"></a>
**Регрессия Пуассона**  
Регрессия Пуассона часто используется для моделирования счетных данных. Этот метод предполагает, что переменная ответа распределяется по модели Пуассона. Данные, распределенные по модели Пуассона, по своей сути являются целыми значениями (дискретными и положительными), что логично для счетных данных. В случае регрессии Пуассона на основе заданного набора данных для обучения предпринимается попытка найти оптимальные значения путем максимального увеличения логарифмического правдоподобия параметров при заданных входных данных. Правдоподобие параметров - это вероятность того, что данные для обучения были отобраны из распределения с использованием этих параметров. Например, регрессию Пуассона удобно использовать в следующих случаях: 

* моделирование числа случаев простуды при авиаперелетах; 
* оценка количества звонков, связанных с мероприятием или рекламной акцией; 
* создание таблиц вероятностей.



<!--HONumber=49-->