<properties urlDisplayName="Cassandra with Linux" pageTitle="Запуск Cassandra под управлением Linux в Azure" metaKeywords="" description="Описывается, как запустить кластер Cassandra под управлением Linux в виртуальных машинах Azure." metaCanonical="" services="virtual-machines" documentationCenter="nodejs" title="Запуск Cassandra под управлением Linux в Azure и доступ к нему из Node.js" authors="hanuk" solutions="" manager="timlt" editor="" />

<tags ms.service="virtual-machines" ms.workload="infrastructure-services" ms.tgt_pltfrm="vm-linux" ms.devlang="na" ms.topic="article" ms.date="01/01/1900" ms.author="hanuk" />

# <span></span></a>Запуск Cassandra под управлением Linux в Azure и доступ к нему из Node.js

**Автор:** Хану Коммалапати (Hanu Kommalapati)

## Оглавление

-   [Обзор][Обзор]
-   [Схема развертывания Cassandra][Схема развертывания Cassandra]
-   [Составное развертывание][Составное развертывание]
-   [Развертывание виртуальных машин Azure][Развертывание виртуальных машин Azure]
-   [Задача 1. Развертывание кластера Linux][Задача 1. Развертывание кластера Linux]
-   [Задача 2. Настройка Cassandra на каждой виртуальной машине][Задача 2. Настройка Cassandra на каждой виртуальной машине]
-   [Задача 3. Доступ к кластеру Cassandra из Node.js][Задача 3. Доступ к кластеру Cassandra из Node.js]
-   [Заключение][Заключение]

## <span id="overview"></span> </a>Обзор

Azure предоставляет службу базы данных NoSQL через хранилище таблиц Azure, что позволяет реализовать хранение бизнес-объектов без схемы. Эту службу можно использовать из Node.JS, .NET, Java и других языков, которые поддерживают HTTP и REST. Однако существуют другие популярные базы данных NoSQL, такие как Cassandra и Couchbase, которые нельзя запустить в PaaS на основе Azure из-за модели облачной службы без учета состояния. Теперь виртуальные машины Azure позволяют выполнять такие базы данных NoSQL в Azure без внесения изменений в базу кода. Задача данного материала — показать, как запустить кластер Cassandra на виртуальных машинах и получить доступ к нему из Node.js. Он не охватывает развертывание Cassandra для реальных производственных сред, где для кластера Cassandra необходимо предусмотреть стратегии резервного копирования и восстановления. В этом упражнении мы будем использовать Linux в виде Ubuntu 12.04 и Cassandra 1.0.10, однако данный процесс можно подстроить под любой дистрибутив Linux.

## <span id="schematic"></span> </a>Схема развертывания Cassandra

Функция виртуальных машин Azure позволяет запускать базы данных NoSQL, такие как [Cassandra][Cassandra], в общедоступном облаке Майкрософт так же просто, как и работать с ними в среде частного облака, за исключением одной особенности конфигурации виртуальной сети, связанной с инфраструктурой виртуальных машин Azure. На момент написания этого материала продукт Cassandra не предоставляется в качестве управляемой службы в Azure, поэтому в этой статье мы рассмотрим настройку кластера Cassandra на виртуальных машинах и доступ к нему из другого экземпляра Linux, также размещенного внутри этих виртуальных машин. Могут также использоваться фрагменты кода node.js из размещенного веб-приложения или веб-службы PaaS. Одной из самых сильных сторон Azure является возможность использования составной модели приложения, которая позволяет взять самое лучшее как от IaaS, так и от PaaS.

Существует две модели развертывания, которые подходят для среды приложения Cassandra: автономное развертывание виртуальных машин и составное развертывание. При составном развертывании размещенный в виртуальных машинах кластер Cassandra будет использоваться из размещенного по принципу PaaS веб-приложения (или веб-службы) Azure с помощью интерфейса Thrift, реализованного через подсистему балансировки нагрузки. Хотя каждый узел Cassandra передает запрос на другие одноранговые узлы в случае сбоя пространства ключей, подсистема балансировки нагрузки помогает осуществлять начальную балансировку запросов. Кроме того, подсистема балансировки нагрузки создает защищенную брандмауэром изолированную среду для улучшения управления данными.

## <span id="composite"></span> </a> Составное развертывание

Цель составного развертывания — добиться максимального использования PaaS, при этом сохраняя минимальный размер виртуальной машины в целях экономии на издержках, обусловленных управлением инфраструктурой виртуальных машин. Из-за издержек на управление серверами развертывайте только те компоненты, учитывающие состояние, которые не могут быть легко изменены по разным причинам, включая время выхода на рынок, отсутствие четкого представления об исходном коде и низкоуровневого доступа к операционной системе.

![Схема составного развертывания][Схема составного развертывания]

## <span id="deployment"></span> </a>Развертывание виртуальных машин Azure

![Развертывание виртуальных машин][Развертывание виртуальных машин]

Согласно приведенным выше схемам, состоящий из 4 узлов кластер Cassandra развертывается внутри виртуальных машин позади подсистемы балансировки нагрузки, настроенной на разрешение трафика Thrift. Размещенное в Azure приложение PaaS осуществляет доступ к кластеру с помощью соответствующих языку библиотек Thrift. Доступны библиотеки для таких языков, как Java, C#, Node.js, Python и C++. Показанное на второй схеме автономное развертывание виртуальных машин получает за счет приложений, которые запущены внутри другой облачной службы, размещенной на виртуальных машинах.

## <span id="task1"></span> </a>Задача 1. Развертывание кластера Linux

Типичная последовательность создания кластера:

![Схема последовательности действий по созданию кластера][Схема последовательности действий по созданию кластера]

**Шаг 1: создание пары ключей SSH**

Azure требует открытый ключ X509, который во время подготовки был закодирован в формат PEM или DER. Создайте пару из открытого и закрытого ключей с помощью инструкций, приведенных в разделе [Как использовать SSH с Linux на Azure][Как использовать SSH с Linux на Azure]. Если вы планируете использовать putty.exe в качестве клиента SSH в Windows или Linux, необходимо преобразовать закодированный в PEM закрытый ключ RSA в формат PPK с помощью puttygen.exe. Соответствующие указания см. в разделе [Создание пары ключей SSH для развертывания виртуальных машин Linux в Windows Azure][Создание пары ключей SSH для развертывания виртуальных машин Linux в Windows Azure].

**Шаг 2: создание виртуальной машины Ubuntu**

Чтобы создать первую виртуальную машину Ubuntu, войдите на предварительную версию портала Azure, выберите **Создать**, **Виртуальная машина**, **Из коллекции** и **Сервер Unbuntu Server 12.xx** и нажмите кнопку со стрелкой вправо. Руководство, в котором описывается создание виртуальной машины с Linux, см. в разделе [Создание виртуальной машины с ОС Linux][Создание виртуальной машины с ОС Linux].

Затем введите следующую информацию на экране "Конфигурация виртуальной машины":

<table>
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Имя поля</th>
<th align="left">Значение поля</th>
<th align="left">Примечания</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Имя виртуальной машины</td>
<td align="left">hk-cas1</td>
<td align="left">Это имя узла виртуальной машины</td>
</tr>
<tr class="even">
<td align="left">Новое имя пользователя</td>
<td align="left">localadmin</td>
<td align="left">&quot;admin&quot; — это зарезервированное имя пользователя в Ubuntu 12. xx</td>
</tr>
<tr class="odd">
<td align="left">Новый пароль</td>
<td align="left"><em>надежный пароль</em></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">Подтверждение пароля</td>
<td align="left"><em>надежный пароль</em></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Размер</td>
<td align="left">Малый</td>
<td align="left">Выберите виртуальную машину с учетом потребностей в операциях ввода-вывода.</td>
</tr>
<tr class="even">
<td align="left">Безопасное использование ключа SSH для проверки подлинности</td>
<td align="left">Щелкните флажок</td>
<td align="left">Установите флажок, если хотите безопасно пользоваться ключом SSH</td>
</tr>
<tr class="odd">
<td align="left">Сертификат</td>
<td align="left"><em>имя файла сертификата открытого ключа</em></td>
<td align="left">Открытый ключ SSH в кодировке PEM или DER, созданный с помощью OpenSSL или других средств</td>
</tr>
</tbody>
</table>

Введите следующую информацию на экране "Режим виртуальной машины":

| Имя поля                                       | Значение поля                          | Примечания                                                                                                                                     |
|------------------------------------------------|----------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------|
| Автономная виртуальная машина                  | Установите переключатель               | Это первая виртуальная машина для последующих виртуальных машин, мы будем использовать параметр "Подключить к существующей виртуальной машине" |
| DNS-имя                                        | *уникальное имя*.cloudapp.net          | Назначьте имя независимой подсистеме балансировки нагрузки                                                                                     |
| Учетная запись хранения                        | *учетная запись хранения по умолчанию* | Используйте стандартную учетную запись хранения, которую вы создали                                                                            |
| Регион/Территориальная группа/Виртуальная сеть | Запад США                              | Выберите регион, из которого веб-приложений осуществляют доступ к кластеру Cassandra                                                           |

Повторите описанный процесс для всех виртуальных машин, которые будут входить в состав кластера Cassandra. На этом этапе все машины будут частью одной сети и смогут проверять связь друг с другом. Если проверка связи не работает, проверьте настройки брандмауэра (например, iptables) виртуальной машины и убедитесь, что разрешен ICMP. После успешного тестирования сетевого подключения не забудьте отключить ICMP для уменьшения вероятности атаки.

**Шаг 3: добавление сбалансированной по нагрузке конечной точки Thrift**

После выполнения действий 1 и 2 каждая виртуальная машина уже должна иметь заданную конечную точку SSH. Теперь давайте добавим сбалансированную по нагрузке конечную точку Thrift с общим портом 9160. Ниже приведена последовательность соответствующих действий:

а) Из представления данных первой виртуальной машины щелкните элемент «Добавить конечную точку».

б) В окне «Добавить конечную точку для виртуальной машины» выберите переключатель «Добавить конечную точку».

в) Щелкните стрелку вправо.

г) В окне «Укажите сведения о конечной точке» введите следующие данные.

<table>

<tr>

<th>
Имя поля

</th>

<th>
Значение поля

</th>

<th>
Примечания

</th>

</tr>

<tr>

<td>
Имя

</td>

<td>
cassandra

</td>

<td>
Подойдет любое уникальное имя конечной точки

</td>

</tr>

<tr>

<td>
Протокол

</td>

<td>
TCP

</td>

<td>
</td>

</tr>

<tr>

<td>
Общий порт

</td>

<td>
9160

</td>

<td>
Порт Thrift по умолчанию.

</td>

</tr>

<tr>

<td>
Закрытый порт

</td>

<td>
9160

</td>

<td>
Если только вы не изменили это значение в cassandra.yaml

</td>

</tr>

</table>
После завершения описанной выше работы первая виртуальная машина отображает конечную точку cassandra, а в поле «БАЛАНСИРОВКА НАГРУЗКИ ВКЛЮЧЕНА» стоит значение «НЕТ». Пока проигнорируйте это, так как значение изменится на "ДА", когда мы добавим эту конечную точку в последующие виртуальные машины

</p>
д) Теперь выберите вторую виртуальную машину и добавьте конечную точку, повторив описанный выше процесс за тем лишь небольшим отличием, что нужно выбрать «Выполнять балансировку нагрузки трафика на существующей конечной точке» и указать в раскрывающемся списке значение «cassandra-960». На этом этапе сопоставление конечной точки с обеими виртуальными машинами приведет к изменению параметра "БАЛАНСИРОВКА НАГРУЗКИ ВКЛЮЧЕНА" со значения "НЕТ" на значение "ДА".

Повторите пункт "e" для последующих узлов в кластере.

Теперь, когда виртуальные машины готовы, пора настроить Cassandra на каждой из них. Поскольку Cassandra не является стандартным компонентом многих дистрибутивов Linux, давайте прибегнем к процессу развертывания вручную.

[Обратите внимание, что мы используем ручной подход для установки программного обеспечения на каждой виртуальной машине. Однако этот процесс можно значительно ускорить, настроив полнофункциональную виртуальную машину Cassandra, записав ее как базовый образ и создав из него дополнительные экземпляры. Указания по записи образа Linux см. в разделе [Как записать образ виртуальной машины под управлением Linux][Обратите внимание, что мы используем ручной подход для установки программного обеспечения на каждой виртуальной машине. Однако этот процесс можно значительно ускорить, настроив полнофункциональную виртуальную машину Cassandra, записав ее как базовый образ и создав из него дополнительные экземпляры. Указания по записи образа Linux см. в разделе [Как записать образ виртуальной машины под управлением Linux].]

## <span id="task2"></span> </a>Задача 2. Настройка Cassandra на каждой виртуальной машине

**Шаг 1: установка необходимых компонентов**

Для Cassandra требуется виртуальная машина Java, поэтому установим последнюю версию JRE с помощью следующей команды для производных от Debian продуктов, включая Ubuntu:

    sudo add-apt-repository ppa:webupd8team/java
    sudo apt-get update
    sudo apt-get install oracle-java7-installer

**Шаг 2: установка Cassandra**

1.  Используя SSH, выполните вход в экземпляр виртуальной машины под управлением Linux (Ubuntu).

2.  Используйте wget, чтобы скачать код Cassandra с зеркала, предложенного по адресу (<http://cassandra.apache.org/download/>)[<http://cassandra.apache.org/download/>], в каталог ~/downloads с именем apache-cassandra-bin.tar.gz. Обратите внимание, что номер версии в загруженный файл не включен, этим обеспечивается независимость публикации от конкретной версии.

3.  Распакуйте пакет в каталог входа по умолчанию, выполнив следующую команду:

        tar -zxvf downloads/apache-cassandra-bin.tar.gz

    Это позволит распаковать архив в каталог apache-cassandra- [версия].

4.  Создайте два следующих стандартных каталога для хранения данных и журналов:

        $ sudo chown -R /var/lib/cassandra
        $ sudo chown -R /var/log/cassandra

5.  Предоставьте удостоверению пользователя, под которым будет выполняться Cassandra, разрешения на запись

        a.  sudo chown -R <user>:<group> /var/lib/cassandra
        b.  sudo chown -R <user>:<group> /var/log/cassandra
        To use current user context, replace the <user> and <group> with $USER and $GROUP

6.  Запустите Cassandra из каталога apache-cassandra-[версия]/bin, используя следующую команду:

        $ ./cassandra

Это позволяет запустить узел Cassandra в фоновом режиме. Используйте параметр -cassandra "f", чтобы запустить процесс в режиме переднего плана.

Журнал должен отображать ошибку mx4j. Cassandra будет прекрасно работать и без mx4j, но этот компонент необходим для управления установкой Cassandra. Перед выполнением следующего шага завершите процесс Cassandra.

**Шаг 3: установка mx4j**

    a)  Download mx4j: wget [http://sourceforge.net/projects/mx4j/files/MX4J%20Binary/3.0.2/mx4j-3.0.2.tar.gz/download](http://sourceforge.net/projects/mx4j/files/MX4J%20Binary/3.0.2/mx4j-3.0.2.tar.gz/download) -O mx4j.tar.gz
    b)  tar -zxvf mx4j.tar.gz
    c)  cp mx4j-23.0.2/lib/*.jar ~/apache-cassandra-<version>/lib
    d)  rm -rf mx4j-23.0.2
    e)  rm mx4j.tar.gz

На этом этапе следует перезапустить процесс Cassandra.

**Шаг 4: тестирование установки Cassandra**

Выполните следующую команду из каталога bin в Cassandra для подключения с использованием клиента thrift:

    cassandra-cli -h localhost -p 9160

**Шаг 5: разрешение внешних подключений к Cassandra**

По умолчанию кластер Cassandra настроен только на прослушивание петлевого адреса, поэтому для реализации внешних подключений это изменение является обязательным.

Отредактируйте conf/cassandra.yaml, изменив **listen\_address** и **rpc\_address** на IP-адрес или имя узла сервера таким образом, чтобы текущий узел был видимым для других узлов, а также для внешних подсистем балансировки нагрузки.

Повторите шаги с 1 по 5 всех узлов в кластере.

Теперь, когда все отдельные виртуальные машины подготовлены с использованием необходимого программного обеспечения, настало время установить связь между узлами за счет настройки начальных значений. Ознакомьтесь со сведениями в разделе [][]<http://wiki.apache.org/cassandra/MultinodeCluster></a>, чтобы подробнее узнать о конфигурации кластера с несколькими узлами.

**Шаг 6: настройка кластера с несколькими узлами**

Отредактируйте cassandra.yaml, изменив следующие свойства во всех виртуальных машинах:

**а) cluster\_name**

В качестве имени кластера по умолчанию установлено значение "Test Cluster", измените его на то, что лучше соответствует вашему приложению. Пример: «AppStore». Если вы уже запустили кластер с именем "Test Cluster" для тестирования во время установки, возникает ошибка несовпадения имен кластера. Чтобы устранить эту ошибку, удалите все файлы из каталога /var/lib/cassandra/data/system.

**б) seeds**

Указанные здесь IP-адреса используются новыми узлами для получения сведений о кольцевой топологии. Установите наиболее надежные узлы в качестве начальных значений в формате с разделителями-запятыми: "*узел1*,*узел2*». Пример настройки: «hk-ub1,hk-ub2».

Мы будет принимать маркеры по умолчанию, предоставляемые серверами начальных значений, как данное упражнение посвящено не этому. Сведения о создании оптимальных маркеров см. в скрипте python, расположенном по адресу:
[][1]<http://wiki.apache.org/cassandra/GettingStarted></a>.

Перезапустите Cassandra на всех узлах, чтобы применить указанные выше изменения.

**Шаг 7: тестирование кластера с несколькими узлами**

Средство Nodetool, установленное в каталог bin Cassandra, поможет управлять кластером. Мы будем использовать nodetool для проверки настройки с помощью команд следующего вида:

    $ bin/nodetool -h <hostname> -p 7199 ring

Если конфигурация верна, отображаемая информация должна соответствовать приведенным ниже сведениям для 3-узлового кластера.

<table>
<colgroup>
<col width="12%" />
<col width="12%" />
<col width="12%" />
<col width="12%" />
<col width="12%" />
<col width="12%" />
<col width="12%" />
<col width="12%" />
</colgroup>
<tbody>
<tr class="odd">
<td align="left">Адрес</td>
<td align="left">КОНТРОЛЛЕР ДОМЕНА</td>
<td align="left">Стойка</td>
<td align="left">Состояние</td>
<td align="left">Состояние</td>
<td align="left">Загрузка</td>
<td align="left">Владение</td>
<td align="left">Маркер</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">149463697837832744402916220269706844972</td>
</tr>
<tr class="odd">
<td align="left">10.26.196.68</td>
<td align="left">datacenter1</td>
<td align="left">rack1</td>
<td align="left">Вверх</td>
<td align="left">Обычный</td>
<td align="left">15,69 KB</td>
<td align="left">25.98%</td>
<td align="left">114445918355431753244435008039926455424</td>
</tr>
<tr class="even">
<td align="left">10.26.198.81</td>
<td align="left">datacenter1</td>
<td align="left">rack1</td>
<td align="left">Вверх</td>
<td align="left">Обычный</td>
<td align="left">15,69 KB</td>
<td align="left">53.44%</td>
<td align="left">70239176883275351288292106998553981501</td>
</tr>
<tr class="odd">
<td align="left">10.26.198.84</td>
<td align="left">datacenter1</td>
<td align="left">rack1</td>
<td align="left">Вверх</td>
<td align="left">Обычный</td>
<td align="left">18,35 KB</td>
<td align="left">25.98%</td>
<td align="left">149463697837832744402916220269706844972</td>
</tr>
</tbody>
</table>

На этом этапе кластер готов к использованию клиентов Thrift через URL-адрес облачной службы (DNS-имя, назначенное при создании первой виртуальной машины), созданный во время выполнения задачи "Развертывание кластера Linux".

## <span id="task3"></span> </a>Задача 3. Доступ к кластеру Cassandra из Node.js

Создайте виртуальную машину Linux в Azure, используя процесс, описанный в предыдущих задачах. Убедитесь, что эта виртуальная машина является автономной, так как мы будем использовать ее в качестве клиента для доступа к кластеру Cassandra. Но перед подключением к кластеру Cassandra из этой виртуальной машины мы установим Node.js, NPM и [cassandra-client][cassandra-client] из github:

**Шаг 1: установка Node.js и NPM**

а) Установка необходимых компонентов.

    sudo apt-get install g++ libssl-dev apache2-utils make

б) Мы будем использовать исходный код из GitHub для компиляции и установки. Прежде чем мы сможет клонировать репозиторий, необходимо установить базовую среду выполнения git:

    sudo apt-get install git-core

в) Клонирование репозитория Node.

    git clone git://github.com/joyent/node.git

г) Это действие создаст каталог с именем «node». Выполните следующую последовательность команд для компиляции и установки node.js:

    cd node
    ./configure
    make
    sudo make install

д) Установите NPM из стабильных двоичных файлов, выполнив следующую команду:

    curl http://npmjs.org/install.sh | sh

**Шаг 2: установка пакета cassandra-client**

    npm cassandra-client 

**Шаг 3: подготовка хранилища Cassandra**

Хранилище Cassandra использует понятия ПРОСТРАНСТВА КЛЮЧЕЙ и СЕМЕЙСТВА СТОЛБЦОВ, которые можно приблизительно сравнить со структурами БАЗ ДАННЫХ и ТАБЛИЦ в терминах реляционных СУБД. ПРОСТРАНСТВО КЛЮЧЕЙ будет содержать набор определений СЕМЕЙСТВА СТОЛБЦОВ. Каждое СЕМЕЙСТВО СТОЛБЦОВ будет содержать несколько строк, и в свою очередь каждая строка содержит несколько столбцов, как показано на комплексном представлении ниже:

![Строки и столбцы][Строки и столбцы]

Мы используем ранее развернутый кластер Cassandra для демонстрации доступа node.js путем создания указанных выше структур данных и выполнения запросов для них. Мы создадим простой скрипт node.js, который выполняет основную подготовку кластера к хранению данных клиентов. Методы, представленные в этом скрипте, можно легко использовать в веб-приложении или веб-службах node.js. Помните о том, что фрагменты кода являются единственным средством показать, как все работает, и для реального применения приведенный код имеет слишком много упущенных моментов (например, ведение журнала безопасности, масштабируемость и т. д.), которые нуждаются в доработке.

Давайте определим нужные переменные в области скрипта, чтобы они включали в себя PooledConnection из модуля cassandra-client, часто используемое имя пространства ключей и параметры подключения к пространству ключей:

    casdemo.js: 
    var pooledCon = require('cassandra-client').PooledConnection;
    var ksName = "custsupport_ks";
    var ksConOptions = { hosts: ['<azure_svc_name>.cloudapp.net:9160'], 
                         keyspace: ksName, use_bigints: false };

В процессе подготовки к хранению данных клиентов необходимо сначала создать ПРОСТРАНСТВО КЛЮЧЕЙ, используя приведенный ниже скрипт:

    casdemo.js: 
    function createKeyspace(callback){
       var cql = 'CREATE KEYSPACE ' + ksName + ' WITH 
       strategy_class=SimpleStrategy AND strategy_options:replication_factor=1';
       var sysConOptions = { hosts: ['<azure_svc_name>.cloudapp.net:9160'],  
                             keyspace: 'system', use_bigints: false };
       var con = new pooledCon(sysConOptions);
       con.execute(cql,[],function(err) {
       if (err) {
         console.log("Failed to create Keyspace: " + ksName);
         console.log(err);
       }
       else {
         console.log("Created Keyspace: " + ksName);
         callback(ksConOptions, populateCustomerData);
       }
       });
       con.shutdown();
    } 

Функция createKeysapce принимает функцию обратного вызова в качестве аргумента, что необходимо для выполнения функции создания СЕМЕЙСТВА СТОЛБЦОВ, так как ПРОСТРАНСТВО КЛЮЧЕЙ является необходимым условием для создания семейства столбцов. Обратите внимание, что мы должны подключиться к ПРОСТРАНСТВУ КЛЮЧЕЙ "system" для определения ПРОСТРАНСТВА КЛЮЧЕЙ приложения. [Язык запросов Cassandra (CQL)][Язык запросов Cassandra (CQL)] всегда используется для взаимодействия с кластером через эти фрагменты кода. Поскольку код CQL в приведенном выше сценарии не имел никаких маркеров параметров, мы используем пустую коллекцию параметров («[]») для метода PooledConnection.execute().

После успешного создания пространства ключей выполняется функция createColumnFamily(), показанная в следующем фрагменте, для создания необходимых определений СЕМЕЙСТВА СТОЛБЦОВ:

    casdemo.js: 
    //Creates COLUMNFAMILY
    function createColumnFamily(ksConOptions, callback){
      var params = ['customers_cf','custid','varint','custname',
                    'text','custaddress','text'];
      var cql = 'CREATE COLUMNFAMILY ? (? ? PRIMARY KEY,? ?, ? ?)';
    var con =  new pooledCon(ksConOptions);
      con.execute(cql,params,function(err) {
          if (err) {
             console.log("Failed to create column family: " + params[0]);
             console.log(err);
          }
          else {
             console.log("Created column family: " + params[0]);
             callback();
          }
      });
      con.shutdown();
    } 

Параметризованный шаблон CQL будет объединен с объектом params для формирования допустимого кода CQL для создания СЕМЕЙСТВА СТОЛБЦОВ. После успешного создания СЕМЕЙСТВА СТОЛБЦОВ выполняется обратный вызов, в этом случае в составе цепочки асинхронного вызова вызывается populateCustomerData().

    casdemo.js: 
    //populate Data
    function populateCustomerData() {
       var params = ['John','Infinity Dr, TX', 1];
       updateCustomer(ksConOptions,params);

       params = ['Tom','Fermat Ln, WA', 2];
       updateCustomer(ksConOptions,params);
    }

    //update will also insert the record if none exists
    function updateCustomer(ksConOptions,params)
    {
      var cql = 'UPDATE customers_cf SET custname=?,custaddress=? where 
                 custid=?';
      var con = new pooledCon(ksConOptions);
      con.execute(cql,params,function(err) {
          if (err) console.log(err);
          else console.log("Inserted customer : " + params[0]);
      });
      con.shutdown();
    }

populateCustomerData() вставляет несколько строк в СЕМЕЙСТВО СТОЛБЦОВ под именем customers\_cf. Инструкция UPDATE в языке запросов Cassandra вставляет запись, если она отсутствует в данном процессе, что делает инструкцию INSERT CQL избыточной.

На данный момент мы выстроили следующую цепочку обратного вызова: от createKeyspace() к createColumnFamily() к populateCustomerData(). Теперь настало время выполнить код в следующем фрагменте:

    casdemo.js:
    var pooledCon = require('cassandra-client').PooledConnection;
    var ksName = "custsupport_ks";
    var ksConOptions = { hosts: ['<azure_svc_name>.cloudapp.net:9160'], 
                         keyspace: ksName, use_bigints: false };

    createKeyspace(createColumnFamily);
    //rest of the not shown

Выполните следующую команду из командной строки, чтобы запустить сценарий:

    //the following command will create the KEYSPACE, COLUMNFAMILY and //inserts two customer records
    $ node casdemo.js

Метод readCustomer() осуществит доступ к размещенному в Azure кластеру и отобразит фрагмент JSON, полученных после выполнения запроса CQL:

    casdemo.js: 
    //read the two rows inserted above
    function readCustomer(ksConOptions)
    {
      var cql = 'SELECT * FROM customers_cf WHERE custid IN (1,2)';
      var con = new pooledCon(ksConOptions);
      con.execute(cql,[],function(err,rows) {
          if (err) 
             console.log(err);
          else 
             for (var i=0; i<rows.length; i++)
                console.log(JSON.stringify(rows[i]));
        });
       con.shutdown();
    } 

Измените casdemo.js, чтобы добавить приведенную выше функцию, и выполните вызов после комментирования ранее вызванного метода createKeyspace(), как показано ниже:

    casdemo.js: 
    var pooledCon = require('cassandra-client').PooledConnection;
    var ksName = "custsupport_ks";
    var ksConOptions = { hosts: ['<azure_svc_name>.cloudapp.net:9160'], 
                         keyspace: ksName, use_bigints: false };

    //createKeyspace(createColumnFamily);
    readCustomer(ksConOptions)
    //rest of the code below not shown
        

## <span id="conclusion"></span> </a>Заключение

Возможность виртуальных машин Azure позволяет создавать виртуальные машины на базе Linux (образы, предоставляемые партнерами корпорации Майкрософт) и Windows, которые позволяют осуществлять миграцию существующих серверных продуктов и приложений с нулевыми изменениями. Одним из примеров таких продуктов является рассмотренный здесь сервер базы данных Cassandra NoSQL. Настроенный здесь кластер Cassandra доступен для размещенных облачных служб Azure, сторонних общедоступных облаков и частных облаков из сред Windows и Linux. В этой статье мы рассмотрели использование node.js в качестве клиента, однако доступ к Cassandra может осуществляться из .NET, Java и других языковых сред.

  [Обзор]: #overview
  [Схема развертывания Cassandra]: #schematic
  [Составное развертывание]: #composite
  [Развертывание виртуальных машин Azure]: #deployment
  [Задача 1. Развертывание кластера Linux]: #task1
  [Задача 2. Настройка Cassandra на каждой виртуальной машине]: #task2
  [Задача 3. Доступ к кластеру Cassandra из Node.js]: #task3
  [Заключение]: #conclusion
  [Cassandra]: http://wiki.apache.org/cassandra/
  [Схема составного развертывания]: ./media/virtual-machines-linux-nodejs-running-cassandra/cassandra-linux1.png
  [Развертывание виртуальных машин]: ./media/virtual-machines-linux-nodejs-running-cassandra/cassandra-linux2.png
  [Схема последовательности действий по созданию кластера]: ./media/virtual-machines-linux-nodejs-running-cassandra/cassandra-linux4.png
  [Как использовать SSH с Linux на Azure]: http://www.windowsazure.com/ru-ru/manage/linux/how-to-guides/ssh-into-linux/
  [Создание пары ключей SSH для развертывания виртуальных машин Linux в Windows Azure]: http://blogs.msdn.com/b/hanuk/archive/2012/06/07/generating-ssh-key-pair-for-linux-vm-deployment-on-windows-azure.aspx
  [Создание виртуальной машины с ОС Linux]: http://www.windowsazure.com/ru-ru/manage/linux/tutorials/virtual-machine-from-gallery/
  [Как записать образ виртуальной машины под управлением Linux]: https://www.windowsazure.com/ru-ru/manage/linux/how-to-guides/capture-an-image/
  []: http://wiki.apache.org/cassandra/MultinodeCluster
  [1]: http://wiki.apache.org/cassandra/GettingStarted
  [cassandra-client]: https://github.com/racker/node-cassandra-client
  [Строки и столбцы]: ./media/virtual-machines-linux-nodejs-running-cassandra/cassandra-linux3.png
  [Язык запросов Cassandra (CQL)]: http://cassandra.apache.org/doc/cql/CQL.html
