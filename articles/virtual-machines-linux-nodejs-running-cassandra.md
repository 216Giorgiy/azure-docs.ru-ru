<properties linkid="services-linux-cassandra-with-linux" urlDisplayName="Cassandra под управлением Linux" pageTitle="Запуск Cassandra под управлением Linux в Azure" metaKeywords="" description="Пояснения по запуску кластера Cassandra под управлением Linux в виртуальных машинах Azure." metaCanonical="" services="virtual-machines" documentationCenter="Node.js" title="Запуск Cassandra под управлением Linux в Azure и доступ к нему из Node.js" authors="" solutions="" manager="" editor="" />





<h1><a id = ""></a>Запуск Cassandra под управлением Linux в Azure и доступ к нему из Node.js </h1>
**Автор:** Хану Коммалапати (Hanu Kommalapati)

## Оглавление##

- [Обзор] []
- [Схема развертывания Cassandra] []
- [Составное развертывание] []
- [Развертывание виртуальной машины Azure] []
- [Задача 1. Развертывание кластера Linux] []
- [Задача 2. Настройка Cassandra на каждой виртуальной машине] []
- [Задача 3. Доступ к кластеру Cassandra из Node.js] []
- [Заключение] []


##<a id="overview"> </a>Обзор ##

Azure предоставляет службу базы данных NoSQL через хранилище таблиц Azure, что позволяет реализовать хранение бизнес-объектов без схемы. Эту службу можно использовать из Node.JS, .NET, Java и других языков, которые поддерживают HTTP и REST.  Однако существуют другие популярные базы данных NoSQL, такие как Cassandra и Couchbase, которые нельзя запустить в PaaS на основе Azure из-за модели облачной службы без учета состояния.  Теперь виртуальные машины Azure позволяют выполнять такие базы данных NoSQL в Azure без внесения изменений в базу кода. Задача данного материала — показать, как запустить кластер Cassandra на виртуальных машинах и получить доступ к нему из Node.js. Он не охватывает развертывание Cassandra для реальных производственных сред, где для кластера Cassandra необходимо предусмотреть стратегии резервного копирования и восстановления. В этом упражнении мы будем использовать Linux в виде Ubuntu 12.04 и Cassandra 1.0.10, однако данный процесс можно подстроить под любой дистрибутив Linux. 

## <a id="schematic"> </a>Схема развертывания Cassandra ##

Функция виртуальных машин Azure позволяет запускать базы данных NoSQL, такие как [Cassandra](http://wiki.apache.org/cassandra/), в общедоступном облаке Майкрософт так же просто, как и работать с ними в среде частного облака, за исключением одной особенности конфигурации виртуальной сети, связанной с инфраструктурой виртуальных машин Azure. На момент написания этого материала продукт Cassandra не предоставляется в качестве управляемой службы в Azure, поэтому в этой статье мы рассмотрим настройку кластера Cassandra на виртуальных машинах и доступ к нему из другого экземпляра Linux, также размещенного внутри этих виртуальных машин. Могут также использоваться фрагменты кода node.js из размещенного веб-приложения или веб-службы PaaS. Одной из самых сильных сторон Azure является возможность использования составной модели приложения, которая позволяет взять самое лучшее как от IaaS, так и от PaaS. 

Существует две модели развертывания, которые подходят для среды приложения Cassandra: автономное развертывание виртуальных машин и составное развертывание.  При составном развертывании размещенный в виртуальных машинах кластер Cassandra будет использоваться из размещенного по принципу PaaS веб-приложения (или веб-службы) Azure с помощью интерфейса Thrift, реализованного через подсистему балансировки нагрузки.  Хотя каждый узел Cassandra передает запрос на другие одноранговые узлы в случае сбоя пространства ключей, подсистема балансировки нагрузки помогает осуществлять начальную балансировку запросов.  Кроме того, подсистема балансировки нагрузки создает защищенную брандмауэром изолированную среду для улучшения управления данными. 

## <a id="composite"> </a> Составное развертывание ##

Цель составного развертывания — добиться максимального использования PaaS, при этом сохраняя минимальный размер виртуальной машины в целях экономии на издержках, обусловленных управлением инфраструктурой виртуальных машин. Из-за издержек на управление серверами развертывайте только те компоненты, учитывающие состояние, которые не могут быть легко изменены по разным причинам, включая время выхода на рынок, отсутствие четкого представления об исходном коде и низкоуровневого доступа к операционной системе. 

![Схема составного развертывания](./media/virtual-machines-linux-nodejs-running-cassandra/cassandra-linux1.png)

##<a id="deployment"> </a>Развертывание виртуальных машин Azure##

![Развертывание виртуальных машин](./media/virtual-machines-linux-nodejs-running-cassandra/cassandra-linux2.png)

Согласно приведенным выше схемам, состоящий из 4 узлов кластер Cassandra развертывается внутри виртуальных машин позади подсистемы балансировки нагрузки, настроенной на разрешение трафика Thrift. Размещенное в Azure приложение PaaS осуществляет доступ к кластеру с помощью соответствующих языку библиотек Thrift. Доступны библиотеки для таких языков, как Java, C#, Node.js, Python и C++. Показанное на второй схеме автономное развертывание виртуальных машин получает за счет приложений, которые запущены внутри другой облачной службы, размещенной на виртуальных машинах. 

##<a id="task1"> </a>Задача 1. Развертывание кластера Linux##

Чтобы в условиях предварительного выпуска все виртуальные машины Linux стали частью одной виртуальной сети, все эти машины нужно развернуть в одной облачной службе. Типичная последовательность создания кластера: 

![Схема последовательности действий по созданию кластера](./media/virtual-machines-linux-nodejs-running-cassandra/cassandra-linux4.png)

**Шаг 1. Создание пары ключей SSH**

Azure требует открытый ключ X509, который во время подготовки был закодирован в формат PEM или DER. Создайте пару из открытого и закрытого ключей с помощью инструкций, приведенных в разделе [Как использовать SSH с Linux на Azure](http://www.windowsazure.com/ru-ru/manage/linux/how-to-guides/ssh-into-linux/).  Если вы планируете использовать putty.exe в качестве SSH-клиента в Windows или Linux, необходимо преобразовать закодированный в PEM закрытый ключ RSA в формат PPK с помощью puttygen.exe.  Соответствующие инструкции см. в разделе [Создание пары ключей SSH для развертывания виртуальных машин Linux в Azure](http://blogs.msdn.com/b/hanuk/archive/2012/06/07/generating-ssh-key-pair-for-linux-vm-deployment-on-windows-azure.aspx).

**Шаг 2. Создание виртуальной машины Ubuntu**

Чтобы создать первую виртуальную машину Ubuntu, войдите на предварительную версию портала Azure, выберите **Создать**, **Виртуальная машина**, **Из коллекции** и **Сервер Unbuntu Server 12.xx** и нажмите кнопку со стрелкой вправо. Руководство, в котором описывается создание виртуальной машины с Linux, см. в разделе [Создание виртуальной машины с ОС Linux](http://www.windowsazure.com/ru-ru/manage/linux/tutorials/virtual-machine-from-gallery/).

Затем введите следующую информацию на экране "Конфигурация виртуальной машины":

<table>
	<tr>
		<th>Имя поля</th>
		<th>Значение поля</th>
		<th>Примечания</th>
	</tr>
	<tr>
		<td>Имя виртуальной машины</td>
		<td>hk-cas1</td>
		<td>Это имя узла виртуальной машины</td>
	</tr>
	<tr>
		<td>Новое имя пользователя</td>
		<td>localadmin</td>
		<td>"admin" — это зарезервированное имя пользователя в Ubuntu 12. xx</td>
	</tr>
	<tr>
		<td>Новый пароль</td>
		<td><i>надежный пароль</i></td>
		<td></td>
	</tr>
	<tr>
		<td>Подтверждение пароля</td>
		<td><i>надежный пароль</i></td>
		<td></td>
	</tr>
	<tr>
		<td>Размер</td>
		<td>Малый</td>
		<td>Выберите виртуальную машину с учетом потребностей в операциях ввода-вывода. </td>
	</tr>
	<tr>
		<td>Безопасное использование ключа SSH для проверки подлинности</td>
		<td>Щелкните флажок</td>
		<td>Установите флажок, если хотите безопасно пользоваться ключом SSH</td>
	</tr>
	<tr>
		<td>Сертификат</td>
		<td><i>имя файла сертификата открытого ключа</i></td>
		<td>Открытый ключ SSH в кодировке PEM или DER, созданный с помощью OpenSSL или других средств</td>
	</tr>
</table>

Введите следующую информацию на экране "Режим виртуальной машины":


<table>
	<tr>
		<th>Имя поля</th>
		<th>Значение поля</th>
		<th>Примечания</th>
	</tr>
	<tr>
		<td>Автономная виртуальная машина</td>
		<td>Установите переключатель</td>
		<td>Это первая виртуальная машина для последующих виртуальных машин, мы будем использовать параметр "Подключить к существующей виртуальной машине"</td>
	</tr>
	<tr>
		<td>DNS-имя</td>
		<td><i>уникальное имя</i>.cloudapp.net</td>
		<td>Назначьте имя независимой подсистеме балансировки нагрузки</td>
	</tr>
	<tr>
		<td>Учетная запись хранения</td>
		<td><i>учетная запись хранения по умолчанию</i></td>
		<td>Используйте стандартную учетную запись хранения, которую вы создали</td>
	</tr>
	<tr>
		<td>Регион/Территориальная группа/Виртуальная сеть</td>
		<td>Запад США</td>
		<td>Выберите регион, из которого веб-приложений осуществляют доступ к кластеру Cassandra</td>
	</tr>
</table>

Повторите описанный процесс для всех виртуальных машин, которые будут входить в состав кластера Cassandra.  На этом этапе все машины будут частью одной сети и смогут проверять связь друг с другом.  Если проверка связи не работает, проверьте настройки брандмауэра (например, iptables) виртуальной машины и убедитесь, что разрешен ICMP. После успешного тестирования сетевого подключения не забудьте отключить ICMP для уменьшения вероятности атаки. 

**Шаг 3. Добавление сбалансированной по нагрузке конечной точки Thrift** 

После выполнения действий 1 и 2 каждая виртуальная машина уже должна иметь заданную конечную точку SSH. Теперь давайте добавим сбалансированную по нагрузке конечную точку Thrift с общим портом 9160. Ниже приведена последовательность соответствующих действий:

а)	Из представления сведений первой виртуальной машины щелкните элемент "Добавить конечную точку"

б)	На экране "Добавить конечную точку для виртуальной машины" выберите переключатель "Добавить конечную точку"

в)	Щелкните стрелку вправо 

г)	На экране "Укажите сведения о конечной точке" введите следующие данные
<table>
	<tr>
		<th>Имя поля</th>
		<th>Значение поля</th>
		<th>Примечания</th>
	</tr>
	<tr>
		<td>Имя</td>
		<td>cassandra</td>
		<td>Подойдет любое уникальное имя конечной точки</td>
	</tr>
	<tr>
		<td>Протокол</td>
		<td>TCP</td>
		<td></td>
	</tr>
	<tr>
		<td>Общий порт</td>
		<td>9160</td>
		<td>Порт Thrift по умолчанию. </td>
	</tr>
	<tr>
		<td>Закрытый порт</td>
		<td>9160</td>
		<td>Если только вы не изменили это значение в cassandra.yaml</td>
	</tr>
</table>
После завершения описанной выше работы первая виртуальная машина отображает конечную точку cassandra, а в поле "БАЛАНСИРОВКА НАГРУЗКИ ВКЛЮЧЕНА" стоит значение "НЕТ". Пока проигнорируйте это, так как значение изменится на "ДА", когда мы добавим эту конечную точку в последующие виртуальные машины

д)	Теперь выберите вторую виртуальную машину и добавьте конечную точку, повторив описанный выше процесс за тем лишь небольшим отличием, что нужно выбрать "Выполнять балансировку нагрузки трафика на существующей конечной точке" и указать в раскрывающемся списке значение "cassandra-960".  На этом этапе сопоставление конечной точки с обеими виртуальными машинами приведет к изменению параметра "БАЛАНСИРОВКА НАГРУЗКИ ВКЛЮЧЕНА" со значения "НЕТ" на значение "ДА". 

Повторите пункт "e" для последующих узлов в кластере. 

Теперь, когда виртуальные машины готовы, пора настроить Cassandra на каждой из них. Поскольку Cassandra не является стандартным компонентом многих дистрибутивов Linux, давайте прибегнем к процессу развертывания вручную.  

[Обратите внимание, что мы используем ручной подход для установки программного обеспечения на каждой виртуальной машине. Однако этот процесс можно значительно ускорить, настроив полнофункциональную виртуальную машину Cassandra, записав ее как базовый образ и создав из него дополнительные экземпляры. Инструкции по записи образа Linux см. в разделе [Как записать образ виртуальной машины под управлением Linux](https://www.windowsazure.com/ru-ru/manage/linux/how-to-guides/capture-an-image/).] 

##<a id="task2"> </a>Задача 2. Настройка Cassandra на каждой виртуальной машине##

**Шаг 1. Установка необходимых компонентов**

Cassandra требует виртуальную машину Java, поэтому установим последнюю версию JRE с помощью следующей команды для производных от Debian продуктов, включая Ubuntu:
         
	sudo add-apt-repository ppa:webupd8team/java
    sudo apt-get update
    sudo apt-get install oracle-java7-installer

**Шаг 2. Установка Cassandra**

1.	Используя SSH, выполните вход в экземпляр виртуальной машины под управлением Linux (Ubuntu).

2.	Используйте wget для загрузки кода Cassandra с зеркала, предложенного по адресу (http://cassandra.apache.org/download/)[http://cassandra.apache.org/download/], в каталог "~/downloads" с именем apache-cassandra-bin.tar.gz. Обратите внимание, что номер версии в загруженный файл не включен, этим обеспечивается независимость публикации от конкретной версии.

3.	Распакуйте пакет в каталог входа по умолчанию, выполнив следующую команду: 
	
		tar -zxvf downloads/apache-cassandra-bin.tar.gz
Это позволит распаковать архив в каталог apache-cassandra- [версия].   

4. Создайте два следующих стандартных каталога для хранения данных и журналов:

		$ sudo chown -R /var/lib/cassandra
		$ sudo chown -R /var/log/cassandra
5.	Предоставьте удостоверению пользователя, под которым будет выполняться Cassandra, разрешения на запись	

		a.	sudo chown -R <user>:<group> /var/lib/cassandra
		b.	sudo chown -R <user>:<group> /var/log/cassandra
		Для использования текущего контекста пользователя замените <user> и <group> на $USER и $GROUP
6.	Запустите Cassandra из каталога apache-cassandra-[версия]/bin, используя следующую команду: 

		$ ./cassandra

Это позволяет запустить узел Cassandra в фоновом режиме. Используйте параметр -cassandra "f", чтобы запустить процесс в режиме переднего плана.

Журнал должен отображать ошибку mx4j. Cassandra будет прекрасно работать и без mx4j, но этот компонент необходим для управления установкой Cassandra.  Перед выполнением следующего шага завершите процесс Cassandra.

**Шаг 3. Установка mx4j**

	a)	Загрузите mx4j: wget [http://sourceforge.net/projects/mx4j/files/MX4J%20Binary/3.0.2/mx4j-3.0.2.tar.gz/download](http://sourceforge.net/projects/mx4j/files/MX4J%20Binary/3.0.2/mx4j-3.0.2.tar.gz/download) -O mx4j.tar.gz
	b)	tar -zxvf mx4j.tar.gz
	c)	cp mx4j-23.0.2/lib/*.jar ~/apache-cassandra-<version>/lib
	d)	rm -rf mx4j-23.0.2
	e)	rm mx4j.tar.gz
На этом этапе следует перезапустить процесс Cassandra

**Шаг 4. Тестирование установки Cassandra**

Выполните следующую команду из каталога bin в Cassandra для подключения с использованием клиента thrift:

	cassandra-cli -h localhost -p 9160

**Шаг 5. Разрешение внешних подключений к Cassandra**

По умолчанию кластер Cassandra настроен только на прослушивание петлевого адреса, поэтому для реализации внешних подключений это изменение является обязательным. 

Отредактируйте "conf/cassandra.yaml", изменив **listen\_address** и **rpc\_address** на IP-адрес или имя узла сервера таким образом, чтобы текущий узел был видимым для других узлов, а также для внешних подсистем балансировки нагрузки.

Повторите шаги с 1 по 5 всех узлов в кластере.

Теперь, когда все отдельные виртуальные машины подготовлены с использованием необходимого программного обеспечения, настало время установить связь между узлами за счет настройки начальных значений. Ознакомьтесь со сведениями в разделе [http://wiki.apache.org/cassandra/MultinodeCluster](http://wiki.apache.org/cassandra/MultinodeCluster), чтобы подробнее узнать о конфигурации кластера с несколькими узлами. 

**Шаг 6. Настройка кластера с несколькими узлами**

Отредактируйте cassandra.yaml, изменив следующие свойства во всех виртуальных машинах:

**а)	cluster_name**

В качестве имени кластера по умолчанию установлено значение "Test Cluster", измените его на то, что лучше соответствует вашему приложению.  Например, "AppStore". Если вы уже запустили кластер с именем "Test Cluster" для тестирования во время установки, возникает ошибка несовпадения имен кластера. Чтобы устранить эту ошибку, удалите все файлы из каталога /var/lib/cassandra/data/system.

**б)	seeds**

Указанные здесь IP-адреса используются новыми узлами для получения сведений о кольцевой топологии. Установите наиболее надежные узлы в качестве начальных значений в формате с разделителями-запятыми: "*узел1*,*узел2*". Пример настройки: "hk-ub1,hk-ub2".

Мы будет принимать маркеры по умолчанию, предоставляемые серверами начальных значений, как данное упражнение посвящено не этому. Сведения о создании оптимальных маркеров см. в скрипте python, расположенном по адресу: 
[http://wiki.apache.org/cassandra/GettingStarted](http://wiki.apache.org/cassandra/GettingStarted).

Перезапустите Cassandra на всех узлах, чтобы применить указанные выше изменения. 

**Шаг 7. Тестирование кластера с несколькими узлами**

Средство Nodetool, установленное в каталог bin Cassandra, поможет управлять кластером. Мы будем использовать nodetool для проверки настройки с помощью команд следующего вида: 

	$ bin/nodetool -h <hostname> -p 7199 ring

Если конфигурация верна, отображаемая информация должна соответствовать приведенным ниже сведениям для 3-узлового кластера.

<table>
	<tr>
		<td>Адрес</td>
		<td>КОНТРОЛЛЕР ДОМЕНА</td>
		<td>Стойка</td>
		<td>Состояние</td>
		<td>Состояние</td>
		<td>Загрузка</td>
		<td>Владение</td>
		<td>Маркер</td>
	</tr>
	<tr>
		<td></td>
		<td></td>	
		<td></td>	
		<td></td>	
		<td></td>	
		<td></td>	
		<td></td>	
		<td>149463697837832744402916220269706844972</td>	
	</tr>
	<tr>
		<td>10.26.196.68</td>
		<td>datacenter1</td>	
		<td>rack1</td>	
		<td>Вверх</td>	
		<td>Обычный</td>	
		<td>15,69 КБ</td>	
		<td>25.98%</td>	
		<td>114445918355431753244435008039926455424</td>	
	</tr>
	<tr>
		<td>10.26.198.81</td>
		<td>datacenter1</td>	
		<td>rack1</td>	
		<td>Вверх</td>	
		<td>Обычный</td>	
		<td>15,69 КБ</td>	
		<td>53.44%</td>	
		<td>70239176883275351288292106998553981501</td>	
	</tr>
	<tr>
		<td>10.26.198.84</td>
		<td>datacenter1</td>	
		<td>rack1</td>	
		<td>Вверх</td>	
		<td>Обычный</td>	
		<td>18,35 КБ</td>	
		<td>25.98%</td>	
		<td>149463697837832744402916220269706844972</td>	
	</tr>
</table>

На этом этапе кластер готов к использованию клиентов Thrift через URL-адрес облачной службы (DNS-имя, назначенное при создании первой виртуальной машины), созданный во время выполнения задачи "Развертывание кластера Linux". 

##<a id="task3"> </a>Задача 3. Доступ к кластеру Cassandra из Node.js##

Создайте виртуальную машину Linux в Azure, используя процесс, описанный в предыдущих задачах.  Убедитесь, что эта виртуальная машина является автономной, так как мы будем использовать ее в качестве клиента для доступа к кластеру Cassandra. Но перед подключением к кластеру Cassandra из этой виртуальной машины мы установим Node.js, NPM и [cassandra-client](https://github.com/racker/node-cassandra-client) из github: 

**Шаг 1. Установка Node.js и NPM**

а)	Установка необходимых компонентов 

	sudo apt-get install g++ libssl-dev apache2-utils make
б)	Мы будем использовать исходный код из GitHub для компиляции и установки. Прежде чем мы сможет клонировать репозиторий, необходимо установить базовую среду выполнения git:

	sudo apt-get install git-core
в)	Клонирование репозитория Node

	git clone git://github.com/joyent/node.git
г)	Это создает каталог с именем "node". Выполните следующую последовательность команд для компиляции и установки node.js:

	cd node
	./configure
	make
	sudo make install

д)	Установите NPM из стабильных двоичных файлов, выполнив следующую команду:

	curl http://npmjs.org/install.sh | sh

**Шаг 2. Установка пакета cassandra-client**

	npm cassandra-client 

**Шаг 3. Подготовка хранилища Cassandra**

Хранилище Cassandra использует понятия ПРОСТРАНСТВА КЛЮЧЕЙ и СЕМЕЙСТВА СТОЛБЦОВ, которые можно приблизительно сравнить со структурами БАЗ ДАННЫХ и ТАБЛИЦ в терминах реляционных СУБД. ПРОСТРАНСТВО КЛЮЧЕЙ будет содержать набор определений СЕМЕЙСТВА СТОЛБЦОВ. Каждое СЕМЕЙСТВО СТОЛБЦОВ будет содержать несколько строк, и в свою очередь каждая строка содержит несколько столбцов, как показано на комплексном представлении ниже:

![Строки и столбцы](./media/virtual-machines-linux-nodejs-running-cassandra/cassandra-linux3.png)

Мы используем ранее развернутый кластер Cassandra для демонстрации доступа node.js путем создания указанных выше структур данных и выполнения запросов для них.  Мы создадим простой скрипт node.js, который выполняет основную подготовку кластера к хранению данных клиентов. Методы, представленные в этом скрипте, можно легко использовать в веб-приложении или веб-службах node.js. Помните о том, что фрагменты кода являются единственным средством показать, как все работает, и для реального применения приведенный код имеет слишком много упущенных моментов (например, ведение журнала безопасности, масштабируемость и т. д.), которые нуждаются в доработке. 

Давайте определим нужные переменные в области скрипта, чтобы они включали в себя PooledConnection из модуля cassandra-client, часто используемое имя пространства ключей и параметры подключения к пространству ключей:

	casdemo.js: 
	var pooledCon = require('cassandra-client').PooledConnection;
	var ksName = "custsupport_ks";
	var ksConOptions = { hosts: ['<azure_svc_name>.cloudapp.net:9160'], 
	                     keyspace: ksName, use_bigints: false };

В процессе подготовки к хранению данных клиентов необходимо сначала создать ПРОСТРАНСТВО КЛЮЧЕЙ, используя приведенный ниже скрипт: 

	casdemo.js: 
	function createKeyspace(callback){
	   var cql = 'CREATE KEYSPACE ' + ksName + ' WITH 
	   strategy_class=SimpleStrategy AND strategy_options:replication_factor=1';
	   var sysConOptions = { hosts: ['<azure_svc_name>.cloudapp.net:9160'],  
	                         keyspace: 'system', use_bigints: false };
	   var con = new pooledCon(sysConOptions);
	   con.execute(cql,[],function(err) {
	   if (err) {
	     console.log("Failed to create Keyspace: " + ksName);
	     console.log(err);
	   }
	   else {
	     console.log("Created Keyspace: " + ksName);
	     callback(ksConOptions, populateCustomerData);
	   }
	   });
	   con.shutdown();
	} 
	
Функция createKeysapce принимает функцию обратного вызова в качестве аргумента, что необходимо для выполнения функции создания СЕМЕЙСТВА СТОЛБЦОВ, так как ПРОСТРАНСТВО КЛЮЧЕЙ является необходимым условием для создания семейства столбцов.  Обратите внимание, что мы должны подключиться к ПРОСТРАНСТВУ КЛЮЧЕЙ "system" для определения ПРОСТРАНСТВА КЛЮЧЕЙ приложения.  [Язык запросов Cassandra (CQL)](http://cassandra.apache.org/doc/cql/CQL.html) всегда используется для взаимодействия с кластером через эти фрагменты кода. Поскольку код CQL в приведенном выше скрипте не имел никаких маркеров параметров, мы используем пустую коллекцию параметров ("[]") для метода PooledConnection.execute(). 

После успешного создания пространства ключей выполняется функция createColumnFamily(), показанная в следующем фрагменте, для создания необходимых определений СЕМЕЙСТВА СТОЛБЦОВ:

	casdemo.js: 
	//Creates COLUMNFAMILY
	function createColumnFamily(ksConOptions, callback){
	  var params = ['customers_cf','custid','varint','custname',
	                'text','custaddress','text'];
	  var cql = 'CREATE COLUMNFAMILY ? (? ? PRIMARY KEY,? ?, ? ?)';
	var con =  new pooledCon(ksConOptions);
	  con.execute(cql,params,function(err) {
	      if (err) {
	         console.log("Failed to create column family: " + params[0]);
	         console.log(err);
	      }
	      else {
	         console.log("Created column family: " + params[0]);
	         callback();
	      }
	  });
	  con.shutdown();
	} 

Параметризованный шаблон CQL будет объединен с объектом params для формирования допустимого кода CQL для создания СЕМЕЙСТВА СТОЛБЦОВ. После успешного создания СЕМЕЙСТВА СТОЛБЦОВ выполняется обратный вызов, в этом случае в составе цепочки асинхронного вызова вызывается populateCustomerData().

	casdemo.js: 
	//populate Data
	function populateCustomerData() {
	   var params = ['John','Infinity Dr, TX', 1];
	   updateCustomer(ksConOptions,params);
	
	   params = ['Tom','Fermat Ln, WA', 2];
	   updateCustomer(ksConOptions,params);
	}
	
	//update will also insert the record if none exists
	function updateCustomer(ksConOptions,params)
	{
	  var cql = 'UPDATE customers_cf SET custname=?,custaddress=? where 
	             custid=?';
	  var con = new pooledCon(ksConOptions);
	  con.execute(cql,params,function(err) {
	      if (err) console.log(err);
	      else console.log("Inserted customer : " + params[0]);
	  });
	  con.shutdown();
	}

populateCustomerData() вставляет нескольких строк в СЕМЕЙСТВО СТОЛБЦОВ под именем customers_cf. Инструкция UPDATE в языке запросов Cassandra вставляет запись, если она отсутствует в данном процессе, что делает инструкцию INSERT CQL избыточной. 

На данный момент мы выстроили следующую цепочку обратного вызова: от createKeyspace() к createColumnFamily() к populateCustomerData(). Теперь настало время выполнить код в следующем фрагменте:

	casdemo.js:
	var pooledCon = require('cassandra-client').PooledConnection;
	var ksName = "custsupport_ks";
	var ksConOptions = { hosts: ['<azure_svc_name>.cloudapp.net:9160'], 
	                     keyspace: ksName, use_bigints: false };
	
	createKeyspace(createColumnFamily);
	//rest of the not shown

Выполните следующую команду из командной строки для запуска скрипта:

	//the following command will create the KEYSPACE, COLUMNFAMILY and //inserts two customer records
	$ node casdemo.js

Метод readCustomer() осуществит доступ к размещенному в Azure кластеру и отобразит фрагмент JSON, полученных после выполнения запроса CQL:

	casdemo.js: 
	//read the two rows inserted above
	function readCustomer(ksConOptions)
	{
	  var cql = 'SELECT * FROM customers_cf WHERE custid IN (1,2)';
	  var con = new pooledCon(ksConOptions);
	  con.execute(cql,[],function(err,rows) {
	      if (err) 
	         console.log(err);
	      else 
	         for (var i=0; i<rows.length; i++)
	            console.log(JSON.stringify(rows[i]));
	    });
	   con.shutdown();
	} 

Измените casdemo.js, добавив упомянутую выше функцию, и вызовите ее, закомментировав предварительно вызванный метод createKeyspace(), как показано ниже:

	casdemo.js: 
	var pooledCon = require('cassandra-client').PooledConnection;
	var ksName = "custsupport_ks";
	var ksConOptions = { hosts: ['<azure_svc_name>.cloudapp.net:9160'], 
	                     keyspace: ksName, use_bigints: false };
	
	//createKeyspace(createColumnFamily);
	readCustomer(ksConOptions)
	//rest of the code below not shown
		
##<a id="conclusion"> </a>Заключение##

Возможность виртуальных машин Azure позволяет создавать виртуальные машины на базе Linux (образы, предоставляемые партнерами корпорации Майкрософт) и Windows, которые позволяют осуществлять миграцию существующих серверных продуктов и приложений с нулевыми изменениями. Одним из примеров таких продуктов является рассмотренный здесь сервер базы данных Cassandra NoSQL. Настроенный здесь кластер Cassandra доступен для размещенных облачных служб Azure, сторонних общедоступных облаков и частных облаков из сред Windows и Linux. В этой статье мы рассмотрели использование node.js в качестве клиента, однако доступ к Cassandra может осуществляться из .NET, Java и других языковых сред. 

[Обзор]: #overview
[Схема развертывания Cassandra]: #schematic
[Составное развертывание]: #composite
[Развертывание виртуальной машины Azure]: #deployment
[Задача 1. Развертывание кластера Linux]: #task1
[Задача 2. Настройка Cassandra на каждой виртуальной машине]: #task2
[Задача 3. Доступ к кластеру Cassandra из Node.js]: #task3
[Заключение]: #conclusion



