<properties 
	pageTitle="Ядра, доступные для записных книжек Jupyter в кластерах HDInsight Spark на платформе Linux | Microsoft Azure" 
	description="Узнайте о дополнительных ядрах, доступных для записных книжек Jupyter с кластерами Spark в HDInsight на платформе Linux." 
	services="hdinsight" 
	documentationCenter="" 
	authors="nitinme" 
	manager="paulettm" 
	editor="cgronlun"
	tags="azure-portal"/>

<tags 
	ms.service="hdinsight" 
	ms.workload="big-data" 
	ms.tgt_pltfrm="na" 
	ms.devlang="na" 
	ms.topic="article" 
	ms.date="06/06/2016" 
	ms.author="nitinme"/>


# Ядра, доступные для использования записными книжками Jupyter с кластерами Apache Spark в HDInsight на платформе Linux

Кластер Apache Spark в HDInsight (Linux) включает записные книжки Jupyter, которые вы можете использовать для тестирования приложений. Ядра — это программа, которая выполняет и интерпретирует ваш код. Кластеры HDInsight Spark предоставляют два ядра, которые вы можете использовать с записными книжками Jupyter. а именно:

1. **PySpark** (для приложений, написанных на языке Python).
2. **Spark** (для приложений, написанных на языке Scala);

В этой статье вы узнаете, как использовать эти ядра и какие преимущества они дают.

**Предварительные требования:**

Необходимо следующее:

- Подписка Azure. См. [Бесплатная пробная версия Azure](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/).
- Кластер Apache Spark в HDInsight на платформе Linux. Инструкции см. в разделе [Создание кластеров Apache Spark в Azure HDInsight](hdinsight-apache-spark-jupyter-spark-sql.md).

## Как использовать ядра? 

1. На начальной панели [портала Azure](https://portal.azure.com/) щелкните элемент кластера Spark (если он закреплен на начальной панели). Кроме того, вы можете перейти к кластеру, выбрав пункты **Просмотреть все** и **Кластеры HDInsight**.   

2. В колонке кластера Spark щелкните **Быстрые ссылки**, затем в колонке **Панель мониторинга кластера** выберите **Jupyter Notebook**. При появлении запроса введите учетные данные администратора для кластера.

	> [AZURE.NOTE] Также можно открыть Jupyter Notebook для своего кластера, открыв следующий URL-адрес в браузере. Замените __CLUSTERNAME__ именем кластера.
	>
	> `https://CLUSTERNAME.azurehdinsight.net/jupyter`

2. Создание новой записной книжки с новыми ядрами. Щелкните **Создать**, а затем выберите пункт **Pyspark** или **Spark**. Для приложений Scala следует использовать ядро Spark, а для приложений Python — ядро PySpark.

	![Создание новой записной книжки Jupyter](./media/hdinsight-apache-spark-jupyter-notebook-kernels/jupyter-kernels.png "Создание новой записной книжки Jupyter")

3. Записная книжка должна открыться с выбранным ядром.

## Почему следует использовать ядра PySpark или Spark?

Ниже приведены несколько преимуществ использования новых ядер.

1. **Предустановленные контексты**. Благодаря ядрам **PySpark** и **Spark**, которые предоставляются с записными книжками Jupyter, вам не требуется явно настраивать контексты Spark или Hive перед началом работы с разрабатываемым приложением; они доступны по умолчанию. а именно:

	* **sc** для контекста Spark;
	* **sqlContext** для контекста Hive.


	Это значит, что для настройки этих контекстов вам не придется выполнять операторы следующего вида:

		###################################################
		# YOU DO NOT NEED TO RUN THIS WITH THE NEW KERNELS
		###################################################
		sc = SparkContext('yarn-client')
		sqlContext = HiveContext(sc)

	Вместо этого вы сможете сразу использовать в своем приложении предустановленные контексты.
	
2. **Волшебные команды**. Ядро PySpark предоставляет несколько "волшебных команд". Это специальные команды, которые можно вызывать с `%%` (например `%%MAGIC` <args>). Волшебная команда должна быть первым словом в ячейке кода и может состоять из нескольких строк содержимого. Волшебное слово должно быть первым словом в ячейке. Любые другие слова перед волшебной командой, даже комментарии, приведут к ошибке. Дополнительные сведения о волшебных командах см. [здесь](http://ipython.readthedocs.org/en/stable/interactive/magics.html).

	В следующей таблице перечислены различные волшебные команды, доступные для ядер.

	| Волшебная команда | Пример | Описание |
	|-----------|---------------------------------|--------------|
	| help | `%%help` | Формирует таблицу из всех доступных волшебных слов с примерами и описанием. |
	| info | `%%info` | Выводит сведения о сеансе для текущей конечной точки Livy. |
	| configure | `%%configure -f`<br>`{"executorMemory": "1000M"`,<br>`"executorCores": 4`} | Настраивает параметры для создания сеанса. Флаг force (-f) является обязательным, если сеанс уже был создан, иначе сеанс будет удален и создан заново. Список допустимых параметров приведен в разделе, посвященном [телу запроса сеансов POST Livy](https://github.com/cloudera/livy#request-body). Параметры должны передаваться в виде строки JSON, следующей после волшебной команды, как показано в столбце примера. |
	| sql | `%%sql -o <variable name>`<br> `SHOW TABLES` | Выполняет запрос Hive к sqlContext. Если передан параметр `-o`, результат запроса сохраняется в контексте Python %%local в качестве таблицы данных [Pandas](http://pandas.pydata.org/). |
	| local | `%%local`<br>`a=1` | Весь код в последующих строках будет выполнен локально. В качестве кода должен быть указан корректный код Python. |
	| журналы | `%%logs` | Выводит журналы для текущего сеанса Livy. |
	| удалить | `%%delete -f -s <session number>` | Удаляет указанный сеанс для текущей конечной точки Livy. Обратите внимание, что нельзя удалить сеанс, который был инициирован самим ядром. |
	| cleanup | `%%cleanup -f` | Удаляет все сеансы для текущей конечной точки Livy, включая сеанс этой записной книжки. Флаг -f является обязательным. |

3. **Автоматическая визуализация**. Ядро **Pyspark** автоматически визуализирует выходные данные запросов Hive и SQL. Вы можете выбрать различные типы средства визуализации, включая таблицы, круговые диаграммы, графики, диаграммы с областями и линейчатые диаграммы.

## Параметры, поддерживаемые волшебной командой %%sql

Волшебная команда %%sql поддерживает различные параметры, позволяющие управлять результатом выполнения запросов. Возможные результаты показаны в следующей таблице.

| Параметр | Пример | Описание |
|-----------|---------------------------------|--------------|
| -o | `-o <VARIABLE NAME>` | При использовании этого параметра результат запроса сохраняется в контексте Python %%local как таблица данных [Pandas](http://pandas.pydata.org/). Именем переменной таблицы данных служит указанное вами имя переменной. |
| -q | `-q` | Позволяет отключить визуализации для ячейки. Если вам не нужна автоматическая визуализация содержимого ячейки и вы хотите только записать ее как таблицу данных, используйте параметр `-q -o <VARIABLE>`. Если вы хотите отключить визуализацию, не записывая результаты (например, для выполнения запроса SQL с побочными эффектами, такого как инструкция `CREATE TABLE`), используйте параметр `-q` без аргумента `-o`. |
| -m | `-m <METHOD>` | Параметр **METHOD** имеет значение **take** или **sample** (по умолчанию используется значение **take**). Если используется метод **take**, ядро выбирает элементы из верхней части результирующего набора данных, который определяется параметром MAXROWS (описывается далее в этой таблице). Если используется метод **sample**, ядро выбирает элементы из набора данных случайным образом в соответствии с параметром `-r`, описанным далее в этой таблице. |
| -r | `-r <FRACTION>` | Здесь **FRACTION** — это число с плавающей запятой от 0,0 до 1,0. Если для SQL-запроса используется метод выборки `sample`, ядро выбирает заданную долю элементов из результирующего набора случайным образом, например при выполнении SQL-запроса с аргументами `-m sample -r 0.01` из результирующего набора данных случайным образом отбирается 1 % строк. |
| -n | `-n <MAXROWS>` | Значение **MAXROWS** должно быть выражено целым числом. Число выходных строк для параметра **MAXROWS** ограничивается ядром. Если значение параметра **MAXROWS** выражено отрицательным числом, например **-1**, число строк в результирующем наборе ограничиваться не будет. |

**Пример**

	%%sql -q -m sample -r 0.1 -n 500 -o query2 
	SELECT * FROM hivesampletable

Приведенная выше инструкция делает следующее:

* Выбирает все записи из таблицы **hivesampletable**.
* Отключает автоматическую визуализацию, так как включает параметр -q.
* Случайным образом выбирает 10 % строк из таблицы hivesampletable и ограничивает размер результирующего набора 500 строками, так как включает параметр `-m sample -r 0.1 -n 500`.
* И, наконец, сохраняет выходные данные в таблицу данных **query2**, так как включает параметр `-o query2`.
	

## Рекомендации по использованию новых ядер

Какое бы ядро вы ни использовали (PySpark или Spark), работающие записные книжки будут потреблять ресурсы кластера. При использовании этих ядер (поскольку контексты заданы предварительно) при простом выходе из записных книжек контекст не завершается, а значит ресурсы кластера будут продолжать использоваться. При работе с ядрами PySpark и Spark рекомендуется использовать параметр **Закрыть и остановить** в меню **Файл** записной книжки. Программа аннулирует контекст и закроет записную книжку.


## Примеры

Открыв записную книжку Jupyter, вы увидите в корневом каталоге две папки.

* Папка **PySpark** содержит примеры записных книжек, в которых используется новое ядро **Python**.
* Папка **Scala** содержит примеры записных книжек, в которых используется новое ядро **Spark**.

Чтобы получить представление о различных волшебных командах, вы можете открыть записную книжку **00 - [READ ME FIRST] Spark Magic Kernel Features** из каталога **PySpark** или **Spark**. Также можно использовать другие примеры записных книжек в этих каталогах, чтобы узнать, как реализовать различные сценарии с помощью записных книжек Jupyter с кластерами HDInsight Spark.

## Где хранятся записные книжки?

Записные книжки Jupyter хранятся в учетной записи хранения, связанной с кластером в папке **/HdiNotebooks**. Доступ к записным книжкам, текстовым файлам и папкам, создаваемым в Jupyter, можно получить через WASB. Например, если Jupyter используется для создания папки **myfolder** и записной книжки **myfolder/mynotebook.ipynb**, доступ к этой записной книжке можно получить по адресу `wasb:///HdiNotebooks/myfolder/mynotebook.ipynb`. Верно и обратное: если вы загружаете записную книжку непосредственно в свою учетную запись в `/HdiNotebooks/mynotebook1.ipynb`, эта записная книжка также отображается в Jupyter. Записные книжки хранятся в учетной записи хранения даже после удаления кластера.

Записные книжки сохраняются в учетной записи хранения как в HDFS. Таким образом, подключаясь к кластеру по SSH, вы можете использовать следующие команды управления файлами:

	hdfs dfs -ls /HdiNotebooks             				  # List everything at the root directory – everything in this directory is visible to Jupyter from the home page
	hdfs dfs –copyToLocal /HdiNotebooks    				# Download the contents of the HdiNotebooks folder
	hdfs dfs –copyFromLocal example.ipynb /HdiNotebooks   # Upload a notebook example.ipynb to the root folder so it’s visible from Jupyter


Если с доступом к учетной записи хранения для кластера возникнут проблемы, записные книжки можно будет также найти в головном узле `/var/lib/jupyter`.

## Поддерживаемый браузер
Записные книжки Jupyter, выполняемые в кластерах HDInsight Spark, поддерживаются только браузером Google Chrome.

## Отзыв

Новые ядра находятся в стадии развития и будут улучшаться со временем. Кроме того, это может означать, что по мере развития этих ядер API могут измениться. Мы будем признательны вам за любые отзывы о работе с новыми ядрами. Ваши комментарии помогут нам оформить финальную версию этих ядер. Отзывы и замечания оставляйте в разделе **Комментарии** под данной статьей.


## <a name="seealso"></a>См. также:


* [Обзор: Apache Spark в Azure HDInsight](hdinsight-apache-spark-overview.md)

### Сценарии

* [Использование Spark со средствами бизнес-аналитики. Выполнение интерактивного анализа данных с использованием Spark в HDInsight с помощью средств бизнес-аналитики](hdinsight-apache-spark-use-bi-tools.md)

* [Использование Spark с машинным обучением. Использование Spark в HDInsight для анализа температуры в здании на основе данных системы кондиционирования](hdinsight-apache-spark-ipython-notebook-machine-learning.md)

* [Использование Spark с машинным обучением. Использование Spark в HDInsight для прогнозирования результатов контроля качества пищевых продуктов](hdinsight-apache-spark-machine-learning-mllib-ipython.md)

* [Потоковая передача Spark. Использование Spark в HDInsight для сборки приложений потоковой передачи данных в режиме реального времени](hdinsight-apache-spark-eventhub-streaming.md)

* [Анализ журнала веб-сайта с использованием Spark в HDInsight](hdinsight-apache-spark-custom-library-website-log-analysis.md)

### Создание и запуск приложений

* [Создание автономного приложения с использованием Scala](hdinsight-apache-spark-create-standalone-application.md)

* [Удаленный запуск заданий с помощью Livy в кластере Spark](hdinsight-apache-spark-livy-rest-interface.md)

### Средства и расширения

* [Использование подключаемого модуля средств HDInsight для IntelliJ IDEA для создания и отправки приложений Spark Scala](hdinsight-apache-spark-intellij-tool-plugin.md)

* [Use HDInsight Tools Plugin for IntelliJ IDEA to debug Spark applications remotely on HDInsight Spark Linux cluster](hdinsight-apache-spark-intellij-tool-plugin-debug-jobs-remotely.md) (Удаленная отладка приложений Spark в кластере HDInsight Spark Linux с помощью подключаемого модуля средств HDInsight для IntelliJ IDEA)

* [Использование записных книжек Zeppelin с кластером Spark в HDInsight](hdinsight-apache-spark-use-zeppelin-notebook.md)

* [Использование внешних пакетов с записными книжками Jupyter](hdinsight-apache-spark-jupyter-notebook-use-external-packages.md)

* [Установка записной книжки Jupyter на компьютере и ее подключение к кластеру Apache Spark в Azure HDInsight (предварительная версия)](hdinsight-apache-spark-jupyter-notebook-install-locally.md)

### Управление ресурсами

* [Управление ресурсами кластера Apache Spark в Azure HDInsight](hdinsight-apache-spark-resource-manager.md)

* [Отслеживание и отладка заданий в кластере Apache Spark в HDInsight на платформе Linux](hdinsight-apache-spark-job-debugging.md)

<!---HONumber=AcomDC_0608_2016-->