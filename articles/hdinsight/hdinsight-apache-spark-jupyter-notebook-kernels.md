<properties 
	pageTitle="Ядра, доступные для записных книжек Jupyter в кластерах HDInsight Spark на платформе Linux | Microsoft Azure" 
	description="Узнайте о дополнительных ядрах, доступных для записных книжек Jupyter с кластерами Spark в HDInsight на платформе Linux." 
	services="hdinsight" 
	documentationCenter="" 
	authors="nitinme" 
	manager="paulettm" 
	editor="cgronlun"
	tags="azure-portal"/>

<tags 
	ms.service="hdinsight" 
	ms.workload="big-data" 
	ms.tgt_pltfrm="na" 
	ms.devlang="na" 
	ms.topic="article" 
	ms.date="02/05/2016" 
	ms.author="nitinme"/>


# Ядра, доступные для использования записными книжками Jupyter с кластерами Spark в HDInsight (Linux)

Кластер Apache Spark в HDInsight (Linux) включает записные книжки Jupyter, которые вы можете использовать для тестирования приложений. По умолчанию записная книжка Jupyter предоставляется с ядром **Python2**. Кластеры Spark в HDInsight предоставляют два дополнительных ядра, которые вы можете использовать с записной книжкой Jupyter, а именно:

1. **Spark** (для приложений, написанных на языке Scala);
2. **PySpark** (для приложений, написанных на языке Python).

В этой статье вы узнаете, как использовать эти ядра и какие преимущества они дают.

**Предварительные требования:**

Необходимо следующее:

- Подписка Azure. См. [Бесплатная пробная версия Azure](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/).
- Кластер Apache Spark в HDInsight на платформе Linux. Инструкции см. в разделе [Создание кластеров Apache Spark в Azure HDInsight](hdinsight-apache-spark-jupyter-spark-sql.md).

## Как использовать ядра? 

1. На начальной панели [портала предварительной версии Azure](https://portal.azure.com/) щелкните элемент кластера Spark (если он закреплен на начальной панели). Кроме того, вы можете перейти к кластеру, выбрав пункты **Просмотреть все** и **Кластеры HDInsight**.   

2. В колонке кластера Spark щелкните **Быстрые ссылки**, затем в колонке **Панель мониторинга кластера** выберите **Jupyter Notebook**. При появлении запроса введите учетные данные администратора для кластера.

	> [AZURE.NOTE] Также можно открыть Jupyter Notebook для своего кластера, открыв следующий URL-адрес в браузере. Замените __CLUSTERNAME__ именем кластера.
	>
	> `https://CLUSTERNAME.azurehdinsight.net/jupyter`

2. Создание новой записной книжки с новыми ядрами. Щелкните **Создать**, а затем выберите пункт **Pyspark** или **Spark**. Для приложений Scala следует использовать ядро Spark, а для приложений Python — ядро PySpark.

	![Создание новой записной книжки Jupyter](./media/hdinsight-apache-spark-jupyter-notebook-kernels/jupyter-kernels.png "Создание новой записной книжки Jupyter")

3. Записная книжка должна открыться с выбранным ядром.

## Зачем использовать новые ядра?

Новые ядра обеспечивают ряд преимуществ.

1. С ядром **Python2**, которое предоставляется по умолчанию, перед началом работы с разрабатываемым приложением вам нужно будет настроить контексты Spark, SQL или Hive. При использовании новых ядер (**Spark** или **PySpark**) эти контексты будут доступны вам по умолчанию, а именно:

	* **sc** для контекста Spark;
	* **sqlContext** для контекста SQL;
	* **hiveContext** для контекста Hive.


	Это значит, что для настройки этих контекстов вам не придется выполнять операторы следующего вида:

		###################################################
		# YOU DO NOT NEED TO RUN THIS WITH THE NEW KERNELS
		###################################################
		sc = SparkContext('yarn-client')
		sqlContext = SQLContext(sc)
		hiveContext = HiveContext(sc)

	Вместо этого вы сможете сразу использовать в своем приложении предустановленные контексты.
	
2. Для выполнения запросов SQL или Hive вы можете соответственно использовать манипуляции **%sql** и **%hive**. Они будут работать сразу, без каких-либо вводных фрагментов кода.

		%hive
		SELECT * FROM hivesampletable LIMIT 10

## Рекомендации по использованию новых ядер

Какое бы ядро вы ни использовали (Python2, PySpark или Spark), работающие записные книжки будут задействовать ресурсы кластера. Поскольку в случае записной книжки Python2 контексты создаются напрямую, при выходе из приложения их можно аннулировать.

В случае с ядрами PySpark и Spark контексты предустановлены, а значит, их нельзя аннулировать явным образом. Это значит, что, если вы выйдете из записной книжки, контекст может остаться запущенным, отнимая ресурсы кластера. При работе с ядрами PySpark и Spark рекомендуется использовать параметр **Закрыть и остановить** в меню **Файл** записной книжки. Программа аннулирует контекст и закроет записную книжку.


## Примеры

Открыв записную книжку Jupyter, вы увидите в корневом каталоге две папки.

* Папка **Python** содержит примеры записных книжек, в которых используется ядро **Python2** по умолчанию.
* Папка **Scala** содержит примеры записных книжек, в которых используется новое ядро **Spark**.

Теперь откройте одну и ту же записную книжку (например, **READ ME FIRST — Learn the Basics of Spark on HDInsight**) в каждой из этих папок: вы увидите, что записная книжка Python2 всегда запускает настройку необходимых контекстов, а записная книжка Spark использует предустановленные контексты.

## Отзыв

Новые ядра находятся в начальной стадии разработки и со временем будут развиваться. Кроме того, это может означать, что по мере развития этих ядер API могут измениться. Мы будем признательны вам за любые отзывы о работе с новыми ядрами. Ваши комментарии помогут нам оформить финальную версию этих ядер. Отзывы и замечания оставляйте в разделе **Комментарии** под данной статьей.


## <a name="seealso"></a>См. также:


* [Обзор: Apache Spark в Azure HDInsight](hdinsight-apache-spark-overview.md)

### Сценарии

* [Использование Spark со средствами бизнес-аналитики. Выполнение интерактивного анализа данных с использованием Spark в HDInsight с помощью средств бизнес-аналитики](hdinsight-apache-spark-use-bi-tools.md)

* [Использование Spark с машинным обучением. Использование Spark в HDInsight для анализа температуры в здании на основе данных системы кондиционирования](hdinsight-apache-spark-ipython-notebook-machine-learning.md)

* [Использование Spark с машинным обучением. Использование Spark в HDInsight для прогнозирования результатов контроля качества пищевых продуктов](hdinsight-apache-spark-machine-learning-mllib-ipython.md)

* [Потоковая передача Spark. Использование Spark в HDInsight для сборки приложений потоковой передачи данных в режиме реального времени](hdinsight-apache-spark-eventhub-streaming.md)

* [Анализ журнала веб-сайта с использованием Spark в HDInsight](hdinsight-apache-spark-custom-library-website-log-analysis.md)

### Создание и запуск приложений

* [Создание автономного приложения с использованием Scala](hdinsight-apache-spark-create-standalone-application.md)

* [Удаленный запуск заданий с помощью Livy в кластере Spark](hdinsight-apache-spark-livy-rest-interface.md)

### Средства и расширения

* [Использование подключаемого модуля средств HDInsight для IntelliJ IDEA для создания и отправки приложений Spark Scala](hdinsight-apache-spark-intellij-tool-plugin.md)

* [Использование записных книжек Zeppelin с кластером Spark в HDInsight](hdinsight-apache-spark-use-zeppelin-notebook.md)

### Управление ресурсами

* [Управление ресурсами кластера Apache Spark в Azure HDInsight](hdinsight-apache-spark-resource-manager.md)

<!---HONumber=AcomDC_0211_2016-->