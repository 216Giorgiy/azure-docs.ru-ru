---
title: Основные сведения об Apache Spark в Azure HDInsight | Документация Майкрософт
description: В этой статье представлены общие сведения о Spark в HDInsight и различные сценарии, в которых вы можете использовать кластер Spark в HDInsight.
services: hdinsight
documentationcenter: ''
author: mumian
manager: cgronlun
editor: cgronlun
tags: azure-portal
ms.assetid: 82334b9e-4629-4005-8147-19f875c8774e
ms.service: hdinsight
ms.custom: hdinsightactive,mvc
ms.devlang: na
ms.topic: overview
ms.date: 05/07/2018
ms.author: jgao
ms.openlocfilehash: a18777694677ab4958c88b5610844726f80868cb
ms.sourcegitcommit: 0c490934b5596204d175be89af6b45aafc7ff730
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 06/27/2018
ms.locfileid: "37054006"
---
# <a name="what-is-apache-spark-in-azure-hdinsight"></a>Apache Spark в Azure HDInsight

*Apache Spark* — это платформа параллельной обработки, которая поддерживает обработку в памяти, чтобы повысить производительность приложений для анализа больших данных. Apache Spark в Azure HDInsight — это реализация Apache Hadoop в облаке, предоставляемая корпорацией Майкрософт. HDInsight упрощает создание и настройку кластера Spark в Azure. Кластеры Spark в HDInsight совместимы со службой хранилища Azure и с Azure Data Lake Store. Поэтому эти кластеры можно использовать для обработки данных, хранящихся в Azure. Дополнительные сведения о компонентах и версиях см. в статье [Что представляют собой компоненты и версии Hadoop, доступные в HDInsight?](../hdinsight-component-versioning.md).

![Spark: единая платформа](./media/apache-spark-overview/hdinsight-spark-overview.png)


## <a name="what-is-spark"></a>Что такое Spark?

Spark предоставляет примитивы для кластерных вычислений в памяти. Задание Spark может загрузить данные, поместить их в кэш в памяти и запрашивать их неоднократно. Вычисления в памяти выполняются намного быстрее, чем в приложениях, использующих диски, таких как приложение Hadoop, которое предоставляет доступ к данным через HDFS. Spark также интегрируется в язык программирования Scala, что дает возможность управлять распределенными наборами данных, такими как локальные коллекции. Нет необходимости структурировать обмен данными как операции сопоставления и редукции.

![Сравнительная характеристика традиционной модели MapReduce и Spark](./media/apache-spark-overview/mapreduce-vs-spark.png)

Кластеры Spark в HDInsight предлагают полностью управляемую службу Spark. Ниже приведены преимущества создания кластера Spark в HDInsight.

| Функция | ОПИСАНИЕ |
| --- | --- |
| Простота создания |Создание кластера Spark в HDInsight с помощью портала Azure, Azure PowerShell или пакета SDK для HDInsight .NET занимает всего несколько минут. См. инструкции по [началу работы с кластером Spark в HDInsight](apache-spark-jupyter-spark-sql.md). |
| Простота использования |Кластер Spark в HDInsight включает записные книжки Jupyter и Zeppelin. Их можно использовать для интерактивной обработки и визуализации данных.|
| Интерфейсы API REST |Кластеры Spark в HDInsight включают [Livy](https://github.com/cloudera/hue/tree/master/apps/spark/java#welcome-to-livy-the-rest-spark-server), сервер заданий Spark на основе API REST, который позволяет пользователям удаленно отправлять задания и отслеживать их. |
| Поддержка хранилища озера данных Azure | Кластеры Spark в HDInsight могут использовать Azure Data Lake Store как основное и дополнительное хранилище. Дополнительные сведения о Data Lake Store см. в [обзоре Azure Data Lake Store](../../data-lake-store/data-lake-store-overview.md). |
| Интеграция со службами Azure |Кластер Spark в HDInsight поставляется с соединителем для концентраторов событий Azure. Вы можете создавать приложения потоковой передачи с помощью концентраторов событий (в дополнение к системе [Kafka](http://kafka.apache.org/), которая уже входит в состав Spark). |
| Поддержка ML Server | Поддержка ML Server в HDInsight предоставляется в рамках типа кластера **Служб машинного обучения**. В кластере Служб машинного обучения можно настроить выполнение распределенных вычислений в среде R со скоростью, заявленной для кластера Spark. Дополнительные сведения см. в статье [Начало работы с кластером R Server в Azure HDInsight](../r-server/r-server-get-started.md). |
| Интеграция со сторонними IDE | HDInsight предоставляет несколько подключаемых модулей IDE, которые можно использовать для создания приложений и их отправки в кластер HDInsight Spark. Дополнительные сведения см. в статьях [Создание приложений Spark для кластера HDInsight с помощью набора средств Azure для IntelliJ](apache-spark-intellij-tool-plugin.md), [Использование средств Azure HDInsight для Visual Studio Code](../hdinsight-for-vscode.md) и [Создание приложений Spark для кластера HDInsight с помощью набора средств Azure для Eclipse](apache-spark-eclipse-tool-plugin.md).|
| Параллельные запросы |Кластеры Spark в HDInsight поддерживают параллельные запросы. Благодаря этому несколько запросов от одного пользователя или несколько запросов от разных пользователей и из различных приложений могут использовать одни и те же ресурсы кластера. |
| Кэширование на накопители SSD |Можно выбрать кэширование данных в памяти или на накопители SSD, подключенные к узлам кластера. Кэширование в память обеспечивает наилучшую производительность запросов, однако может оказаться ресурсоемким. Кэширование на накопители SSD предоставляет возможность повысить производительность запросов без необходимости создания кластера такого размера, который необходим для размещения всего набора данных в памяти. |
| Интеграция со средствами бизнес-аналитики |В состав кластеров Spark для HDInsight входят соединители для инструментов бизнес-аналитики, таких как [Power BI](http://www.powerbi.com/) для анализа данных. |
| Предварительно загруженные библиотеки Anaconda |Кластеры Spark в HDInsight поставляются с предустановленными библиотеками Anaconda. [Anaconda](http://docs.continuum.io/anaconda/) содержит порядка 200 библиотек для машинного обучения, анализа данных, визуализации и т. д. |
| Масштабируемость | В HDInsight можно изменить количество узлов кластера. Кроме того, кластеры Spark можно удалить без потери данных, так как все данные хранятся в службе хранилища Azure или Data Lake Store. |
| Соглашение об уровне обслуживания |Для кластеров Spark в HDInsight предоставляется круглосуточная и ежедневная поддержка и соглашения об уровне обслуживания, гарантирующие время бесперебойной работы на уровне 99,9 %. |

Кластеры Spark в HDInsight включают следующие компоненты, доступные в кластерах по умолчанию.

* [Ядро Spark](https://spark.apache.org/docs/1.5.1/). Включает ядро Spark, Spark SQL, потоковые API-интерфейсы Spark, GraphX и MLlib.
* [Anaconda](http://docs.continuum.io/anaconda/)
* [Livy](https://github.com/cloudera/hue/tree/master/apps/spark/java#welcome-to-livy-the-rest-spark-server)
* [Записная книжка Jupyter](https://jupyter.org)
* [Записная книжка Zeppelin](http://zeppelin-project.org/)

Кроме того, кластеры Spark в HDInsight включают [драйвер ODBC](http://go.microsoft.com/fwlink/?LinkId=616229) для подключения к кластерам Spark в HDInsight из таких инструментов бизнес-аналитики, как Microsoft Power BI.

## <a name="spark-cluster-architecture"></a>Архитектура кластера Spark

![Архитектура HDInsight Spark](./media/apache-spark-overview/spark-architecture.png)

Чтобы разобраться с компонентами Spark, нужно понять принцип работы Spark в кластерах HDInsight.

Приложения Spark выполняются как независимые наборы процессов в кластере, координируемые объектом SparkContext в основной программе (называемой программой драйвера).

SparkContext может подключаться к нескольким типам диспетчеров кластеров, которые распределяют ресурсы между приложениями. К этим диспетчерам кластеров относятся Apache Mesos, Apache YARN и Spark. В HDInsight Spark выполняется с использованием диспетчера кластеров YARN. После подключения Spark получает исполнителей на рабочих узлах кластера. Исполнители представляют собой процессы,которые выполняют вычисления и хранят данные для приложения. Затем Spark отправляет исполнителям код приложения (определенный в JAR- или Python-файлах, переданных в SparkContext). Наконец, SparkContext отправляет исполнителям задачи для выполнения.

SparkContext выполняет основную функцию пользователя и осуществляет различные параллельные операции на рабочих узлах. Затем SparkContext собирает результаты операций. Рабочие узлы считывают данные из распределенной файловой системы Hadoop (HDFS) и записывают их в нее. Кроме того, рабочие узлы помещают преобразованные данные в кэш в памяти как устойчивые распределенные наборы данных (RDD).

SparkContext подключается к главному узлу Spark и отвечает за преобразование приложения в ориентированный граф (DAG) для отдельных задач, которые выполняются в рамках процесса исполнителя в рабочих узлах. Каждое приложение получает отдельные процессы исполнителя, которые остаются активными во время выполнения приложения и обрабатывают задачи в нескольких потоках.

## <a name="spark-in-hdinsight-use-cases"></a>Варианты использования Spark в HDInsight

Ниже представлены сценарии для использования кластеров Spark в HDInsight.

- Интерактивный анализ данных и бизнес-аналитика

    Apache Spark в HDInsight хранит данные в службе хранилища Azure или Azure Data Lake Store. Бизнес-эксперты и лица, ответственные за принятие решений, могут анализировать и создавать отчеты на основе этих данных, а также создавать интерактивные отчеты из проанализированных данных с помощью средств Microsoft Power BI. Аналитики могут использовать неструктурированные или полуструктурированные данные в хранилище кластеров, определить схему для данных с помощью записных книжек, а затем создать модели данных с помощью средств Microsoft Power BI. Кластеры Spark в HDInsight также поддерживают ряд инструментов бизнес-аналитики сторонних разработчиков, таких как Tableau. Поэтому Spark является лучшим вариантом для аналитиков, бизнес-экспертов и лиц, ответственных за принятие решений.

    [Руководство по визуализации данных Spark с помощью Power BI](apache-spark-use-bi-tools.md)
- Машинное обучение Spark

    В состав Apache Spark входит [MLlib](http://spark.apache.org/mllib/), библиотека машинного обучения, созданная на основе Spark, которую вы можете использовать из кластера Spark в HDInsight. Кластер Spark в HDInsight также включает библиотеку Anaconda, распространяемую Python и содержащую различные пакеты для машинного обучения. Все это дополнено встроенной поддержкой записных книжек Jupyter и Zeppelin — в итоге вы получаете в свое распоряжение среду для создания приложений машинного обучения.

    [Руководство по прогнозированию температуры в зданиях с помощью данных системы кондиционирования](apache-spark-ipython-notebook-machine-learning.md) [Руководство по прогнозированию результатов контроля качества пищевых продуктов](apache-spark-machine-learning-mllib-ipython.md)    
- Потоковая передача и анализ данных в режиме реального времени в Spark

    Кластеры Spark в HDInsight обладают широкой поддержкой для создания решений для аналитики в режиме реального времени. Поскольку в состав Spark уже входят соединители для приема данных из различных источников, таких как Flume, Kafka, Twitter, ZeroMQ или сокеты TCP, Spark в HDInsight позволяет реализовать первоклассную поддержку для приема данных из концентраторов событий Azure. Концентраторы событий — это наиболее часто используемые службы очередей в Azure. Встроенная поддержка концентраторов событий делает кластеры Spark в HDInsight идеальной платформой для создания конвейеров аналитики в режиме реального времени.
    
## <a name="where-do-i-start"></a>С чего начать?

Дополнительные сведения о Spark в HDInsight см. в следующих руководствах:

- [Краткое руководство по созданию кластера Spark в HDInsight и выполнению интерактивных запросов с помощью Jupyter](./apache-spark-jupyter-spark-sql.md)
- [Руководство по запуску заданий Spark с помощью Jupyter](./apache-spark-load-data-run-query.md)
- [Руководство по анализу данных с помощью инструментов бизнес-аналитики](./apache-spark-use-bi-tools.md)
- [Руководство по созданию приложений машинного обучения с использованием Spark](./apache-spark-ipython-notebook-machine-learning.md)
- [Руководство по созданию приложения Scala Maven с помощью IntelliJ](./apache-spark-create-standalone-application.md)

## <a name="next-steps"></a>Дальнейшие действия

В этой обзорной статье вы получили некоторые основные сведения об Apache Spark в Azure HDInsight. Из следующей статьи вы узнаете, как создать кластер HDInsight Spark и выполнить некоторые запросы Spark SQL.

- [Создание кластера Apache Spark в Azure HDInsight](./apache-spark-jupyter-spark-sql.md)

