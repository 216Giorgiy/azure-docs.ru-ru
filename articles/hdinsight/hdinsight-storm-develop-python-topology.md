<properties
   pageTitle="Использование компонентов Python в топологии Storm на HDinsight | Microsoft Azure"
   description="Узнайте, как можно использовать компоненты Python с Apache Storm на Azure HDInsight. Вы узнаете, как использовать компоненты Python из топологии Storm как на основе Java, так и на основе Clojure."
   services="hdinsight"
   documentationCenter=""
   authors="Blackmist"
   manager="paulettm"
   editor="cgronlun"/>

<tags
   ms.service="hdinsight"
   ms.devlang="python"
   ms.topic="article"
   ms.tgt_pltfrm="na"
   ms.workload="big-data"
   ms.date="02/01/2016"
   ms.author="larryfr"/>

#Разработка топологий Apache Storm с помощью Python в HDInsight

Apache Storm поддерживает несколько языков и даже позволяет объединять компоненты из нескольких языков в одной топологии. Из этого документа вы узнаете, как использовать компоненты Python в топологиях Storm на базе Java и Clojure в HDInsight.

##Предварительные требования

* Python 2.7 или выше;

* Java JDK 1.7 или выше.

* [Leiningen](http://leiningen.org/)

##Многоязыковая поддержка Storm

Storm предназначен для работы с компонентами, написанными на любом языке программирования. Однако эти компоненты должны уметь работать с [определением Thrift для Storm](https://github.com/apache/storm/blob/master/storm-core/src/storm.thrift). В рамках проекта Apache Storm предоставляется модуль для Python, который позволяет легко взаимодействовать со Storm. Этот модуль можно найти по адресу [https://github.com/apache/storm/blob/master/storm-multilang/python/src/main/resources/resources/storm.py](https://github.com/apache/storm/blob/master/storm-multilang/python/src/main/resources/resources/storm.py).

Так как Apache Storm представляет собой процесс Java, выполняемый на виртуальной машине Java (JVM), компоненты, написанные на других языках, выполняются как подпроцессы. Биты Storm, работающие в JVM, взаимодействуют с этими подпроцессами с помощью сообщений JSON, отправляемых через стандартный ввод-вывод. Дополнительные сведения о связи между компонентами можно найти в документации по [многоязыковому протоколу](https://storm.apache.org/documentation/Multilang-protocol.html).

###Модуль Storm

Модуль storm (https://github.com/apache/storm/blob/master/storm-multilang/python/src/main/resources/resources/storm.py,) предоставляет элементы, необходимые для создания компонентов Python, которые работают со Storm.

Предоставляются такие элементы, как `storm.emit` для выдачи кортежей и `storm.logInfo` для записи в журналы. Рекомендуется прочитать этот файл и понять, какие элементы он предоставляет.

##Сложности

С помощью модуля __storm.py__ можно создавать воронки Python, которые потребляют данные, и сита, которые используют данные. Однако общее определение топологии Storm, обеспечивающее взаимодействие между компонентами, все равно пишется на языке Java или Clojure. Кроме того, если вы используете Java, необходимо создать также компоненты Java, служащие интерфейсом для компонентов Python.

Кроме того, так как кластеры Storm выполняются распределенно, необходимо обеспечить на всех рабочих узлах кластера доступность любых модулей, необходимых для компонентов Python. У Storm нет простого способа это сделать для многоязыковых ресурсов. Необходимо либо включить все зависимости в JAR-файл для топологии, либо вручную установить зависимости на каждом рабочем узле в кластере.

Есть проекты, которые пытаются преодолеть эти недостатки, например [Pyleus](https://github.com/Yelp/pyleus) и [Streamparse](https://github.com/Parsely/streamparse). Хотя оба этих проекта можно запускать в кластерах HDInsight под управлением Linux, они не являются основной темой данного документа, так как требуют пользовательских настроек во время установки кластера и не полностью протестированы в кластерах HDInsight. Примечания по использованию этих платформ с HDInsight приведены в конце этого документа.

###Определение топологии: Java против Clojure

Из двух способов определения топологии Clojure — несомненно самый простой и четкий, так как он позволяет непосредственно обращаться к компонентам Python в определении топологии. Для определений топологии на основе Java необходимо определить также компоненты Java, обрабатывающие такие действия, как объявление полей в кортежах, возвращаемых из компонентов Python.

В этом документе описаны оба метода вместе с примерами проектов.

##Компоненты Python с топологией Java

> [AZURE.NOTE] Этот пример доступен по адресу [https://github.com/Azure-Samples/hdinsight-python-storm-wordcount](https://github.com/Azure-Samples/hdinsight-python-storm-wordcount) в каталоге __JavaTopology__. Это проект на основе Maven. Если вы незнакомы с Maven, см. статью [Разработка топологий на основе Java с помощью Apache Storm в HDInsight](hdinsight-storm-develop-java-topology.md) для получения дополнительных сведений о создании проекта Maven для топологии Storm.

Топология на основе Java, использующая Python (или другие языковые компоненты JVM), использует, на первый взгляд, компоненты Java. Однако, если присмотреться к каждой из воронок или сит Java, вы увидите код, подобный следующему:

    public SplitBolt() {
        super("python", "countbolt.py");
    }

Здесь Java вызывает Python и запускает сценарий, содержащий фактическую логику сита. Воронки и сита Java (в данном примере) просто объявляют поля в кортеже, которые выдаст базовый компонент Python.

Фактические файлы Python в этом примере хранятся в каталоге `/multilang/resources`. Каталог `/multilang` указывается в файле __pom.xml__:

<resources> <resource> <!-- Where the Python bits are kept --> <directory>${basedir}/multilang</directory> </resource> </resources>

Сюда входят все файлы в папке `/multilang` JAR-файла, который будет построен из этого проекта.

> [AZURE.IMPORTANT] Обратите внимание, что здесь указывается только каталог `/multilang`, а не `/multilang/resources`. Storm ожидает не относящиеся к JVM ресурсы в каталоге `resources`, поэтому внутри него уже производится поиск. Помещение компонентов в эту папку позволяет просто обращаться к ним по имени в коде Java. Пример: `super("python", "countbolt.py");`. Можно представить это иначе: Storm воспринимает каталог `resources` как корневой (/) при доступе к многоязыковым ресурсам.
>
> В этом примере проекта модуль `storm.py` включен в каталог `/multilang/resources`.

###Построение и запуск проекта

Чтобы запустить этот проект локально, просто выполните следующую команду Maven для построения и запуска в локальном режиме:

    mvn compile exec:java -Dstorm.topology=com.microsoft.example.WordCount

Остановить процесс можно сочетанием клавиш CTRL + C.

Чтобы развернуть проект в кластере HDInsight под управлением Apache Storm, выполните следующие действия:

1. Постройте uber jar:

        mvn package

    При этом будет создан файл с именем __WordCount--1.0-SNAPSHOT.jar__ в каталоге `/target` данного проекта.

2. Загрузите JAR-файл в кластер Hadoop с помощью одного из следующих методов.

    * Для кластеров HDInsight __под управлением Linux__: используйте `scp WordCount-1.0-SNAPSHOT.jar USERNAME@CLUSTERNAME-ssh.azurehdinsight.net:WordCount-1.0-SNAPSHOT.jar` для копирования JAR-файла в кластер, заменив USERNAME именем пользователя SSH, а CLUSTERNAME именем кластера HDInsight.

        После завершения передачи файла подключитесь к кластеру с помощью SSH и запустите топологию с помощью `storm jar WordCount-1.0-SNAPSHOT.jar com.microsoft.example.WordCount wordcount`.

    * Для кластеров HDInsight __под управлением Windows__: подключитесь к панели мониторинга Storm, перейдя по адресу HTTPS://CLUSTERNAME.azurehdinsight.net/ в браузере. Замените CLUSTERNAME именем кластера HDInsight и укажите имя администратора и пароль в ответ на запрос.

        Используя форму, выполните следующие действия.

        * __JAR-файл__: щелкните __Обзор__, а затем выберите файл __WordCount-1.0-SNAPSHOT.jar__.
        * __Имя класса__: введите `com.microsoft.example.WordCount`.
        * __Дополнительные параметры__: введите понятное имя, например `wordcount`, для определения топологии.

        Наконец, нажмите кнопку __Отправить__ для запуска топологии.

> [AZURE.NOTE] После запуска топология Storm выполняется до тех пор, пока она не будет остановлена (завершена). Чтобы остановить топологию, используйте либо команду `storm kill TOPOLOGYNAME` из командной строки (например, сеанса SSH для кластера Linux), либо с помощью пользовательского интерфейса Storm выберите топологию, а затем нажмите кнопку __Kill__ (Удалить).

##Компоненты Python с топологией Clojure

> [AZURE.NOTE] Этот пример доступен по адресу [https://github.com/Azure-Samples/hdinsight-python-storm-wordcount](https://github.com/Azure-Samples/hdinsight-python-storm-wordcount) в каталоге __ClojureTopology__.

Эта топология создана с помощью [Leiningen](http://leiningen.org) для [создания нового проекта Clojure](https://github.com/technomancy/leiningen/blob/stable/doc/TUTORIAL.md#creating-a-project). После этого в шаблонный проект были внесены следующие изменения.

* __project.clj__: добавлены зависимости для Storm и исключения для элементов, которые могут вызвать проблемы при развертывании на сервере HDInsight.
* __resources/resources__: Leiningen создает каталог по умолчанию `resources`, однако хранящиеся в нем файлы добавляются в корень JAR-файла, созданного из этого проекта, а Storm ожидает файлы во вложенном каталоге с именем `resources`. Поэтому добавлен вложенный каталог, и файлы Python хранятся в `resources/resources`. Во время выполнения этот каталог будет считаться корневым (/) для доступа к компонентам Python.
* __src/wordcount/core.clj__: этот файл содержит определение топологии. Кроме того, на него ссылается файл __project.clj__. Дополнительные сведения об использовании Clojure для определения топологии Storm см. в статье [Clojure DSL](https://storm.apache.org/documentation/Clojure-DSL.html).

###Построение и запуск проекта

__Чтобы построить и запустить проект локально__, выполните следующую команду:

    lein do clean, run

Чтобы остановить топологию, нажмите сочетание клавиш __CTRL + C__.

__Чтобы построить uberjar и развернуть в HDInsight__, выполните следующие действия:

1. Создайте uberjar, содержащий топологию и необходимые зависимости:

        lein uberjar

    Будет создан новый файл с именем `wordcount-1.0-SNAPSHOT.jar` в каталоге `target\uberjar+uberjar`.
    
2. С помощью одного из следующих методов разверните и запустите топологию в кластер HDInsight:

    * __HDInsight под управлением Linux__
    
        1. Скопируйте файл в головной узел кластера HDInsight с помощью `scp`. Например:
        
                scp wordcount-1.0-SNAPSHOT.jar USERNAME@CLUSTERNAME-ssh.azurehdinsight.net:wordcount-1.0-SNAPSHOT.jar
                
            Замените USERNAME именем пользователя SSH для вашего кластера, а CLUSTERNAME — именем кластера HDInsight.
            
        2. После копирования файла в кластер подключитесь к кластеру с помощью SSH и отправьте задание. Сведения об использовании SSH с HDInsight см. в одной из следующих статей.
        
            * [Использование SSH с HDInsight под управлением Linux в Linux, Unix или OS X.](hdinsight-hadoop-linux-use-ssh-unix.md)
            * [Использование SSH с HDInsight под управлением Linux в Windows.](hdinsight-hadoop-linux-use-ssh-windows.md)
            
        3. После подключения запустите топологию следующей командой:
        
                storm jar wordcount-1.0-SNAPSHOT.jar wordcount.core wordcount
    
    * __HDInsight под управлением Windows__
    
        1. Подключитесь к панели мониторинга Storm, перейдя по адресу HTTPS://CLUSTERNAME.azurehdinsight.net/ в браузере. Замените CLUSTERNAME именем кластера HDInsight и укажите имя администратора и пароль в ответ на запрос.

        2. Используя форму, выполните следующие действия.

            * __JAR-файл__: щелкните __Обзор__, а затем выберите файл __wordcount-1.0-SNAPSHOT.jar__.
            * __Имя класса__: введите `wordcount.core`.
            * __Дополнительные параметры__: введите понятное имя, например `wordcount`, для определения топологии.

            Наконец, нажмите кнопку __Отправить__ для запуска топологии.

> [AZURE.NOTE] После запуска топология Storm выполняется до тех пор, пока она не будет остановлена (завершена). Чтобы остановить топологию, используйте либо команду `storm kill TOPOLOGYNAME` из командной строки (сеанса SSH для кластера Linux), либо с помощью пользовательского интерфейса Storm выберите топологию, а затем нажмите кнопку __Kill__ (Удалить).

##Платформа Pyleus

[Pyleus](https://github.com/Yelp/pyleus) — это платформа, которая призвана оптимизировать использование Python со Storm. Она предоставляет следующие возможности.

* __Определения топологий на основе YAML__: простой способ определения топологии, не требующий знаний Java или Clojure.
* __Сериализатор на основе MessagePack__: MessagePack используется в качестве сериализации по умолчанию вместо JSON. Это может ускорить обмен сообщениями между компонентами.
* __Управление зависимостями__: Virtualenv позволяет гарантировать развертывание зависимостей Python на все рабочие узлы. Для этого требуется установка Virtualenv на рабочие узлы.

> [AZURE.IMPORTANT] Для Pyleus требуется наличие Storm в среде разработки. Использование базового дистрибутива Apache Storm 0.9.3 приводит к получению JAR-файлов, несовместимых с версией Storm, которая поставляется с HDInsight. Поэтому в следующих действиях в качестве среды разработки используется кластер HDInsight.

Вы можете успешно построить примеры топологий Pyleus, используя головной узел HDInsight в качестве среды сборки.

1. Подготавливая новый модуль Storm в кластере HDInsight, убедитесь в наличии Python Virtualenv на узлах кластера. Во время создания нового кластера HDInsight под управлением Linux используйте следующие параметры действия сценария с [настройкой кластера](hdinsight-hadoop-customize-cluster.md).

    * __Имя__: просто укажите понятное имя.
    * \_\_ Универсальный код ресурса (URI) сценария\_\_: в качестве значения используйте `https://hditutorialdata.blob.core.windows.net/customizecluster/pythonvirtualenv.sh`. Этот сценарий установит Python Virtualenv на узлы.
    
        > [AZURE.NOTE] Кроме того, он создаст некоторые каталоги, используемые платформой Streamparse далее в этом документе.
        
    * __Nimbus__: проверьте эту запись, чтобы сценарий применялся к узлам Nimbus (головным).
    * __Контролер__: проверьте эту запись, чтобы сценарий применялся к контрольным (рабочим) узлам.
    
    Остальные записи оставьте пустыми.

1. После создания кластера подключитесь с помощью SSH.

    * [Использование SSH с HDInsight под управлением Linux в Linux, Unix или OS X.](hdinsight-hadoop-linux-use-ssh-unix.md)
    * [Использование SSH с HDInsight под управлением Linux в Windows.](hdinsight-hadoop-linux-use-ssh-windows.md)

2. Подключившись с помощью SSH, используйте следующие команды для создания новой виртуальной среды и установки Pyleus.

        virtualenv pyleus_venv
        source pyleus_venv
        pip install pyleus

3. Затем загрузите GIT-репозиторий Pyleus и выполните построение примера WordCount.

        sudo apt-get install git
        git clone https://github.com/Yelp/pyleus.git
        pyleus build pyleus/examples/word_count/pyleus_topology.yaml
    
    После завершения построения у вас будет новый файл с именем `word_count.jar` в текущем каталоге.
    
4. Чтобы отправить топологию в кластер Storm, используйте следующую команду:

        pyleus submit -n localhost word_count.jar
    
    Параметром `-n` указывается хост Nimbus. Так как мы находимся на головном узле, можно использовать `localhost`.
    
    Кроме того, вы можете использовать команду `pyleus` для выполнения других действий Storm. Используйте следующую команду для вывода списка запущенных топологий и последующей остановки топологии `word_count`:
    
        pyleus list -n localhost
        pyleus kill -n localhost word_count

##Платформа Streamparse

[Streamparse](https://github.com/Parsely/streamparse) — это платформа, которая призвана оптимизировать использование Python со Storm, предоставляя следующие возможности.

* __Формирование шаблонов__: позволяет легко создать шаблон для проекта, а затем изменить файлы для добавления логики.
* __Функции Clojure DSL__: уменьшают уровень детализации при использовании компонентов Python в определении топологии Clojure.
* __Управление зависимостями__: Virtualenv позволяет гарантировать развертывание зависимостей Python на все рабочие узлы. Для этого требуется установка Virtualenv на рабочие узлы.
* __Удаленное развертывание__: Streamparse может использовать автоматизацию SSH для развертывания компонентов на рабочих узлах, а также может создать туннель SSH для взаимодействия с Nimbus. Поэтому вы можете легко выполнять развертывание из среды разработки в кластер под управлением Linux, например HDInsight.

> [AZURE.IMPORTANT] Streamparse зависит от компонентов, которые ожидают [сигналов Unix](https://en.wikipedia.org/wiki/Unix_signal), которые не доступны в Windows. Средой разработки должна быть Linux, Unix или OS X, а кластер HDInsight должен работать под управлением Linux.

1. Подготавливая новый модуль Storm в кластере HDInsight, убедитесь в наличии Python Virtualenv на узлах кластера. Во время создания нового кластера HDInsight под управлением Linux используйте следующие параметры действия сценария с [настройкой кластера](hdinsight-hadoop-customize-cluster.md).

    * __Имя__: просто укажите понятное имя.
    * \_\_ Универсальный код ресурса (URI) сценария\_\_: в качестве значения используйте `https://hditutorialdata.blob.core.windows.net/customizecluster/pythonvirtualenv.sh`. Этот сценарий установит Python Virtualenv на узлы, а также создаст каталоги, используемые платформой Streamparse.
    * __Nimbus__: проверьте эту запись, чтобы сценарий применялся к узлам Nimbus (головным).
    * __Контролер__: проверьте эту запись, чтобы сценарий применялся к контрольным (рабочим) узлам.
    
    Остальные записи оставьте пустыми.
    
    > [AZURE.WARNING] Кроме того, необходимо использовать __открытый ключ__ для защиты пользователя SSH в кластере HDInsight с целью удаленного развертывания с помощью Streamparse.
    >
    > Дополнительные сведения об использовании ключей с SSH в HDInsight см. в одном из следующих документов.
    >
    > * [Использование SSH с HDInsight под управлением Linux в Linux, Unix или OS X.](hdinsight-hadoop-linux-use-ssh-unix.md)
    > * [Использование SSH с HDInsight под управлением Linux в Windows.](hdinsight-hadoop-linux-use-ssh-windows.md)

2. Во время подготовки кластера установите Streamparse в вашей среде разработки, используя следующую команду:

        pip install streamparse
        
3. После установки Streamparse используйте следующую команду для создания примера проекта:

        sparse quickstart wordcount
        
    Будет создан новый каталог с именем `wordcount`. Он будет заполнен примером проекта «Статистика».

4. Переключитесь на каталог `wordcount` и запустите топологию в локальном режиме.

        cd wordcount
        sparse run

    Для остановки топологии нажмите сочетание клавиш CTRL + C.

###Развертывание топологии

После создания кластера HDInsight под управлением Linux выполните следующие действия для развертывания топологии в кластере.

1. С помощью следующей команды найдите полные доменные имена рабочих узлов для кластера:

        curl -u admin:PASSWORD -G https://CLUSTERNAME.azurehdinsight.net/api/v1/clusters/CLUSTERNAME/hosts | grep '"host_name" : "worker'
    
    При этом будут получены сведения о хостах для кластера. Они будут направлены в grep и возвращены в записях рабочих узлов. Вы должны увидеть результаты, аналогичные приведенным ниже.
    
        "host_name" : "workernode0.1kft5e4nx2tevg5b2pdwxqx1fb.jx.internal.cloudapp.net"
    
    Сохраните информацию `"workernode0.1kft5e4nx2tevg5b2pdwxqx1fb.jx.internal.cloudapp.net"`, так как она будет использоваться на следующем шаге.

2. Откройте файл __config.json__ в каталоге `wordcount` и измените следующие записи.

    * __user__: укажите имя учетной записи пользователя SSH, настроенное для кластера HDInsight. Оно будет использоваться для проверки подлинности в кластере при развертывании проекта.
    * __nimbus__: задайте значение `CLUSTERNAME-ssh.azurehdinsight.net`. Параметр CLUSTERNAME нужно заменить именем кластера. Этот параметр используется при взаимодействии с узлом Nimbus, который является головным узлом кластера.
    * __workers__: заполните запись workers именами хостов рабочих узлов, которые были получены с помощью curl. Например:
    
        ```
"workers": [
    "workernode0.1kft5e4nx2tevg5b2pdwxqx1fb.jx.internal.cloudapp.net",
    "workernode1.1kft5e4nx2tevg5b2pdwxqx1fb.jx.internal.cloudapp.net"
    ]
        ```
    
    * __virtualenv\_root__: задайте значение "/virtualenv"
    
    После этого будет настроен проект для кластера HDInsight, включая каталог `/virtualenv`, который был создан во время подготовки действием сценария.

4. Так как для развертывания Streamparse в HDInsight требуется перенаправлять проверку подлинности через головной узел на рабочие узлы, на рабочей станции необходимо запустить `ssh-agent`. Для большинства операционных систем он запускается автоматически. Выполните следующую команду, чтобы проверить его работу:

        echo "$SSH_AUTH_SOCK"
    
    Если `ssh-agent` работает, эта команда должна вернуть ответ следующего вида:
    
        /tmp/ssh-rfSUL1ldCldQ/agent.1792
    
    > [AZURE.NOTE] Полный путь может отличаться в зависимости от операционной системы. Например, в OS X путь может быть похожим на этот: `/private/tmp/com.apple.launchd.vq2rfuxaso/Listeners`. Однако если агент работает, должен возвращаться любой путь.
    
    Если путь не возвращается, запустите агент командой `ssh-agent`.
    
5. Проверьте, известен ли агенту ключ, который вы используете для проверки подлинности на сервере HDInsight. Чтобы получить список ключей, доступных для агента, используйте следующую команду:

        ssh-add -L
    
    Будут возвращены закрытые ключи, добавленные в агент. Вы можете сравнить результаты с содержимым закрытого ключа, сформированного при создании SSH-ключа для проверки подлинности в HDInsight.
    
    Если данные не возвращаются или не соответствуют закрытому ключу, добавьте закрытый ключ в агент следующей командой:
    
        ssh-add /path/to/key/file
    
    Пример: `ssh-add ~/.ssh/id_rsa`.

4. Кроме того, необходимо настроить SSH, чтобы ему было известно о необходимости использовать переадресацию для кластера HDInsight. Добавьте следующий код в файл `~/.ssh/config`. Если этот файл не существует, создайте его и используйте следующие значения в качестве содержимого:

        Host *.azurehdinsight.net
          ForwardAgent yes
        
        Host *.internal.cloudapp.net
          ProxyCommand ssh CLUSTERNAME-ssh.azurehdinsight.net nc %h %p
    
    Параметр CLUSTERNAME нужно заменить именем кластера HDInsight.
    
    После этого будет настроен агент SSH на рабочей станции, который включит переадресацию ваших учетных данных SSH через любую систему *. azurehdinsight.net, к которой вы подключаетесь. В данном случае это будет головной узел кластера. Далее настраивается команда, используемая для перенаправления SSH-трафика от головного узла к отдельным рабочим узлам (internal.cloudapp.net). Это позволяет платформе Streamparse подключиться к головному узлу, а затем от него — к каждому из рабочих узлов, используя проверку подлинности ключа для учетной записи SSH.
    
5. Наконец, используйте следующую команду для отправки топологии из локальной среды разработки в кластер HDInsight:

        sparse submit
    
    При этом будет выполнено подключение к кластеру HDInsight, развертывание топологии и возможных зависимостей Python и последующий запуск топологии.

##Дальнейшие действия

В этом документе рассмотрено, как использовать компоненты Python из топологии Storm. Чтобы узнать о других способах использования Python с HDInsight, см. следующие документы.

* [Использование Python для потоковой передачи заданий MapReduce.](hdinsight-hadoop-streaming-python.md)
* [Использование определяемых пользователем функций Python (UDF) в Pig и Hive.](hdinsight-python.md)

<!---HONumber=AcomDC_0204_2016-->