---
title: Планирование емкости для Azure Stack | Документы Майкрософт
description: Сведения о планировании емкости для развертываний Azure Stack.
services: azure-stack
documentationcenter: ''
author: jeffgilb
manager: femila
editor: ''
ms.assetid: ''
ms.service: azure-stack
ms.workload: na
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 02/12/2019
ms.author: jeffgilb
ms.reviewer: prchint
ms.lastreviewed: 09/18/2018
ms.openlocfilehash: b8bd57953845278aa75e8cbdf41ae28300edad58
ms.sourcegitcommit: 301128ea7d883d432720c64238b0d28ebe9aed59
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 02/13/2019
ms.locfileid: "56184913"
---
# <a name="azure-stack-capacity-planning"></a>Планирование емкости для Azure Stack
При оценке решения Azure Stack необходимо выбрать конфигурацию оборудования, которая непосредственно влияет на общую емкость облака Azure Stack. Можно выбирать среди классических вариантов ЦП, плотности памяти, конфигурации хранилища и общего масштаба решения или числа серверов. В отличие от традиционных решений виртуализации, простой подсчет этих компонентов для определения емкости не сработает. Первая причина в том, что решение Azure Stack спроектировано для размещения компонентов инфраструктуры и управления в пределах самого решения. Вторая причина в том, что часть ресурсов решений резервируется для поддержки устойчивости. Устойчивость — это обновление программного обеспечения, при котором перерыв в рабочих нагрузках клиентов сводится к минимуму.

> [!IMPORTANT]
> Сведения о планировании емкости и прилагаемая таблица предоставляются руководством лишь для справки, с целью оказания помощи в принятии решений, связанных с планированием и конфигурацией Azure Stack. Они не должны заменять ваши собственные наблюдения и анализ. 

## <a name="compute-and-storage-capacity-planning"></a>Планирование вычислительной емкости и емкости хранилища
Решение Azure Stack строится как гиперконвергентный кластер вычислительных и сетевых ресурсов, а также ресурсов хранилища. Это позволяет эффективно использовать, в том числе совместно, все аппаратные ресурсы кластера, называемого единицей масштабирования для Azure Stack, с высоким уровнем доступности и масштабируемости. Все программное обеспечение инфраструктуры размещается в наборе виртуальных машин на тех же физических серверах, что и виртуальные машины клиента. Управление всеми виртуальными машинами осуществляется с помощью технологий кластеризации Windows Server единицы масштабирования и отдельных экземпляров Hyper-V. Такой подход упрощает приобретение решения Azure Stack и управление им, а также обеспечивает возможность переноса и масштабируемость всех служб (клиента и инфраструктуры) в пределах всей единицы масштабирования.

Единственный физический ресурс, который не допускает избыточной подготовки в решении Azure Stack, — это память сервера. Другие ресурсы (ядра ЦП, пропускная способность сети и емкость хранилища) могут подготавливаться избыточно для наиболее эффективного использования доступных ресурсов. При расчете доступной решению емкости основной вклад вносит физическая память сервера. Использование других ресурсов будет зависеть от отношения возможного уровня избыточной подготовки и уровня избыточной подготовки, допустимого для планируемой рабочей нагрузки.

Для размещения инфраструктуры Azure Stack используются приблизительно 28 виртуальных машин, которые в совокупности потребляют примерно 208 ГБ памяти и 124 виртуальных ядра.  Такое количество виртуальных машин обосновывается необходимостью в разделении служб с целью удовлетворить требования к безопасности, масштабируемости, обслуживанию и применению исправлений. Внутренняя структура служб позволяет вводить в будущем новые службы инфраструктуры по мере их разработки.

Для поддержки автоматического обновления всех компонентов физических серверов и программного обеспечения инфраструктуры, а также применения исправлений размещаемые виртуальные машины инфраструктуры и пользовательские виртуальные машины потребляют не все ресурсы памяти единицы масштабирования. Часть общего объема памяти на всех серверах единицы масштабирования высвобождается для обеспечения требований решения к устойчивости. Например, на время обновления образа Windows Server на физическом сервере размещенные на нем виртуальные машины переносятся в другое место единицы масштабирования. По завершении обновления сервер перезапускается и возобновляет обслуживание рабочих нагрузок. Цель исправления и обновления решения Azure Stack — устранить необходимость в остановке размещенных виртуальных машин. Для ее достижения высвобождается минимальный объем памяти, равный размеру памяти на одном сервере. Это позволяет перемещать виртуальные машины в пределах единицы масштабирования. Процедуры размещения и перемещения относятся как к виртуальным машинам инфраструктуры, так и к виртуальным машинам, созданным от имени пользователя или клиента решения Azure Stack. В результате такой реализации объем памяти, зарезервированный для перемещения виртуальных машин, может значительно превышать размер памяти отдельного сервера из-за разницы в требованиях, предъявляемых виртуальными машинами к памяти. Дополнительные накладные расходы в плане использования памяти создаются самим экземпляром Windows Server. Базовый экземпляр операционной системы для каждого сервера расходует память для самой операционной системы и ее таблиц виртуальных страниц. Кроме того, память расходуется на управление каждой размещенной виртуальной машиной посредством Hyper-V.

Дополнительные сложности, возникающие при вычислении емкости, описываются далее в этом разделе. В этой вводной статье предлагаются примеры, помогающие понять, какая доступная емкость требуется для решений разного размера. Они основаны на приблизительных расчетах, и в них делаются предположения в отношении использования памяти виртуальными машинами клиента, которые могут не подтверждаться в рабочей среде. В этой таблице используется размер Standard D2 виртуальной машины Azure. Виртуальные машины Azure Standard D2 имеют 2 виртуальных ЦП и 7 ГБ памяти.

|     |Емкость на сервер|| Емкость на единицу масштабирования|  |  |||
|-----|-----|-----|-----|-----|-----|-----|-----|
|     | Память | Ядра ЦП | Количество серверов | Память | Ядра ЦП | Виртуальные машины клиента <sup>1</sup>     | Соотношение ядер <sup>2</sup>    |
|Пример 1|256 ГБ|28|4.|1024 ГБ| 112 | 54 |4:3|
|Пример 2|512 ГБ|28|4.|2024 ГБ|112|144|4:1|
|Пример 3|384 ГБ|28|12|4608 ГБ|336|432|3:1|
|     |     |     |     |     |     |     |     |

> <sup>1</sup> Виртуальные машины Standard D2.

> <sup>2</sup> Соотношение числа виртуальных и физических ядер.

Как упоминалось выше, емкость виртуальных машин определяется доступной памятью. Соотношение числа физических и виртуальных ядер служит примером того, как плотность виртуальных машин влияет на доступную емкость ресурсов ЦП, если в решении не предусмотрено большее число физических ядер (выбран другой ЦП). То же самое верно и для емкости хранилища и емкости кэша.

Приведенные выше примеры для плотности виртуальных машин служат лишь для пояснения. Вычисление емкости сопряжено и с другими трудностями. Чтобы получить полное понимание всех вариантов, доступных при планировании емкости, а также оценить итоговую доступную емкость, необходимо обратиться в корпорацию Майкрософт или к партнеру по решению.

В оставшейся части этого раздела описываются требования к вычислительным ресурсам и ресурсам хранилища для развертывания Azure Stack. С помощью этих сведений вы можете получить базовое представление о требуемых компонентах и минимальных значениях конфигурации. Кроме того, описывается настройка решения в плане доступной емкости и возможные ограничения системы в плане емкости клиента и производительности.

> [!NOTE]
> Требования к планированию ресурсов для сетевого взаимодействия минимальны, так как настраивается только размер общедоступного виртуального IP-адреса. Сведения о добавлении дополнительных общедоступных IP-адресов в Azure Stack см. в статье [Добавление общедоступных IP-адресов](azure-stack-add-ips.md).


## <a name="next-steps"></a>Дополнительная информация
[Планирование ресурсов компьютера](capacity-planning-compute.md)
