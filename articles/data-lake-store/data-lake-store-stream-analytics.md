---
title: "Потоковая передача данных из Stream Analytics в Data Lake Store | Документация Майкрософт"
description: "Использование Azure Stream Analytics для потоковой передачи данных в хранилище озера данных Azure"
services: data-lake-store,stream-analytics
documentationcenter: 
author: nitinme
manager: jhubbard
editor: cgronlun
ms.assetid: edb58e0b-311f-44b0-a499-04d7e6c07a90
ms.service: data-lake-store
ms.devlang: na
ms.topic: article
ms.tgt_pltfrm: na
ms.workload: big-data
ms.date: 07/07/2016
ms.author: nitinme
translationtype: Human Translation
ms.sourcegitcommit: 73d3e5577d0702a93b7f4edf3bf4e29f55a053ed
ms.openlocfilehash: 2864fbf1fc4f070cb4d88d1bb3efbaaf408c68ce


---
# <a name="stream-data-from-azure-storage-blob-into-data-lake-store-using-azure-stream-analytics"></a>Потоковая передача данных из большого двоичного объекта службы хранилища Azure в хранилище озера данных с помощью Azure Stream Analytics
Из этой статьи вы узнаете, как использовать хранилище озера данных Azure в качестве источника выходных данных для задания Azure Stream Analytics. В этой статье показан простой сценарий, в котором данные считываются из большого двоичного объекта службы хранилища Azure (входные данные) и записываются в хранилище озера данных (выходные данные).

> [!NOTE]
> В настоящий момент создание и настройка выходных данных Data Lake Store для Stream Analytics поддерживается только на [классическом портале Azure](https://manage.windowsazure.com). Поэтому в некоторых частях этого учебника будет использоваться классический портал Azure.
>
>

## <a name="prerequisites"></a>Предварительные требования
Перед началом работы с этим учебником необходимо иметь следующее:

* **Подписка Azure**. Ознакомьтесь с [бесплатной пробной версией Azure](https://azure.microsoft.com/pricing/free-trial/).
* **Настройте свою подписку Azure** для использования общедоступной предварительной версии Data Lake Store. Ознакомьтесь с [инструкциями](data-lake-store-get-started-portal.md).
* **Учетная запись хранения Azure.** Контейнер больших двоичных объектов из этой учетной записи будет использоваться для ввода данных для задания Stream Analytics. Для работы с этим руководством необходимо создать учетную запись хранилища с именем **datalakestoreasa**, а в ней — контейнер с именем **datalakestoreasacontainer**. После создания контейнера отправьте в него образец файла данных. Его можно получить из [репозитория Git Azure Data Lake](https://github.com/Azure/usql/tree/master/Examples/Samples/Data/AmbulanceData/Drivers.txt). Чтобы передать данные в контейнер больших двоичных объектов, можно использовать различные клиенты, например [обозреватель хранилищ Azure](http://storageexplorer.com/).

  > [!NOTE]
  > При создании учетной записи на портале Azure ее необходимо создать с **классической** моделью развертывания. Это обеспечит доступность учетной записи хранения с классического портала Azure, поскольку именно его мы используем для создания задания Stream Analytics. Инструкции по созданию учетной записи хранения на портале Azure с помощью классической модели развертывания см. в разделе [Создайте учетную запись хранения](../storage/storage-create-storage-account.md#create-a-storage-account).
  >
  > Кроме того, учетную запись хранения можно создать на классическом портале Azure.
  >
  >
* **Учетная запись хранилища озера данных Azure**. Следуйте инструкциям в разделе [Приступая к работе с хранилищем озера данных Azure на портале Azure](data-lake-store-get-started-portal.md).  

## <a name="create-a-stream-analytics-job"></a>Создание задания Stream Analytics
Для начала нужно создать задание Stream Analytics с источником входных данных и целевым объектом для выходных данных. В этом руководстве источником является контейнер больших двоичных объектов Azure, а целевым объектом — хранилище озера данных.

1. Войдите на [классический портал Azure](https://manage.windowsazure.com).
2. В правой нижней части окна последовательно щелкните **Создать**, **Службы данных**, **Stream Analytics**, **Быстрое создание**. Укажите значения, показанные на рисунке ниже, и выберите команду **Создать задание Stream Analytics**.

    ![Создание задания Stream Analytics](./media/data-lake-store-stream-analytics/create.job.png "Create a Stream Analytics job")

## <a name="create-a-blob-input-for-the-job"></a>Создание входных данных большого двоичного объекта для задания
1. Откройте страницу задания Stream Analytics, перейдите на вкладку **Входные данные** и выберите команду **Добавление входных данных**, чтобы запустить мастер.
2. На странице **Добавление входных данных в задание** выберите параметр **Поток данных** и щелкните стрелку "Далее".

    ![Добавление входных данных в задание](./media/data-lake-store-stream-analytics/create.input.1.png "Add an input to your job")
3. На странице **Добавление потока данных в задание** выберите параметр **Хранилище BLOB-объектов** и щелкните стрелку "Далее".

    ![Добавление потока данных в задание](./media/data-lake-store-stream-analytics/create.input.2.png "Add a data stream to the job")
4. На странице **Настройки хранилища BLOB-объектов** укажите данные для хранилища, которое будет использоваться в качестве источника входных данных.

    ![Указание параметров хранилища BLOB-объекта](./media/data-lake-store-stream-analytics/create.input.3.png "Provide the blob storage settings")

   * **Введите псевдоним входных данных**. Это уникальное имя для входных данных задания.
   * **Выберите учетную запись хранения.** Убедитесь, что учетная запись хранения находится в той же области, что и задание Stream Analytics. Иначе вам придется заплатить за перемещение данных между регионами.
   * **Укажите контейнер хранилища**. Можно создать новый контейнер или выбрать уже существующий.

     Щелкните стрелку «Далее».
5. На странице **Параметры сериализации** выберите формат сериализации **CSV**, для разделителя выберите значение **табуляция**, а для кодировки — **UTF8**. Затем нажмите кнопку с галочкой.

    ![Указание параметров сериализации](./media/data-lake-store-stream-analytics/create.input.4.png "Provide the serialization settings")
6. После завершения работы мастера входные данные большого двоичного объекта будут добавлены на вкладку **Входные данные**, а в столбце **Диагностика** должно отображаться значение **OК**. Можно также непосредственно проверить подключение ко входным данным. Для этого нажмите кнопку **Проверить подключение** внизу.

## <a name="create-a-data-lake-store-output-for-the-job"></a>Создание выходных данных хранилища озера данных для задания
1. Откройте страницу задания Stream Analytics, перейдите на вкладку **Выходные данные** и выберите команду **Добавить выходные данные**, чтобы запустить мастер.
2. На странице **Добавление выходных данных в задание** выберите параметр **Data Lake Store** и щелкните стрелку "Далее".

    ![Добавление выходных данных в задание](./media/data-lake-store-stream-analytics/create.output.1.png "Add an output to your job")
3. Если учетная запись Data Lake Store уже создана, на странице **Авторизация подключения** нажмите кнопку **Авторизовать сейчас**. Если у вас нет учетной записи, щелкните ссылку **Зарегистрируйтесь сейчас** , чтобы создать учетную запись. Предположим, у вас уже есть учетная запись хранилища озера данных (как указано в предварительных требованиях). Будет выполнена автоматическая авторизация с использованием учетных данных, которые использовались для входа на классический портал Azure.

    ![Авторизация хранилища озера данных](./media/data-lake-store-stream-analytics/create.output.2.png "Authorize Data Lake Store")
4. На странице **Параметры хранилища озера данных** введите сведения, как показано на снимке экрана ниже.

    ![Указание параметров хранилища озера данных](./media/data-lake-store-stream-analytics/create.output.3.png "Specify Data Lake Store settings")

   * **Введите псевдоним выходных данных**. Это уникальное имя для выходных данных задания.
   * **Укажите учетную запись хранилища озера данных**. Она должна быть уже создана, как указано в предварительных требованиях.
   * **Укажите шаблон префикса пути**. Он необходим для идентификации выходных файлов, которые записывает задание Stream Analytics в хранилище озера данных. Поскольку заголовки выходных данных, записываемых заданием, имеют формат GUID, добавление префикса поможет идентифицировать записанные выходные данные. Если в префикс необходимо включить метки даты и времени, добавьте в шаблон префикса элемент `{date}/{time}` . Если включить этот элемент, то поля **Формат даты** и **Формат времени** станут доступны, и вы сможете выбрать нужный формат.

     Щелкните стрелку «Далее».
5. На странице **Параметры сериализации** выберите формат сериализации **CSV**, для разделителя выберите значение **табуляция**, а для кодировки — **UTF8**. Затем нажмите кнопку с галочкой.

    ![Указание формата выходных данных](./media/data-lake-store-stream-analytics/create.output.4.png "Specify the output format")
6. После завершения работы мастера входные данные Data Lake Store будут добавлены на вкладку **Выходные данные**, а в столбце **Диагностика** должно отображаться значение **OК**. Можно также непосредственно проверить подключение к выходным данным. Для этого нажмите кнопку **Проверить подключение** внизу.

## <a name="run-the-stream-analytics-job"></a>Выполнение задания Stream Analytics
Чтобы выполнить задание Stream Analytics, необходимо выполнить запрос на вкладке "Запрос". В этом руководстве можно выполнить образец запроса, заменив заполнители псевдонимами входных и выходных данных, как показано на снимке экрана ниже.

![Выполнение запроса](./media/data-lake-store-stream-analytics/run.query.png "Run query")

Нажмите кнопку **Сохранить** в нижней части экрана, а затем выберите команду **Запустить**. В диалоговом окне нажмите кнопку **Настраиваемое время** и выберите дату в прошлом, например **1/1/2016**. Нажмите кнопку с галочкой, чтобы запустить задание. Для запуска задания может потребоваться несколько минут.

![Настройка времени задания](./media/data-lake-store-stream-analytics/run.query.2.png "Set job time")

После запуска задания перейдите на вкладку **Монитор** , чтобы просмотреть результат обработки данных.

![Отслеживание задания](./media/data-lake-store-stream-analytics/run.query.3.png "Monitor job")

Теперь с помощью [портала Azure](https://portal.azure.com) можно открыть учетную запись Data Lake Store и проверить, успешно ли записаны данные в эту учетную запись.

![Проверка выходных данных](./media/data-lake-store-stream-analytics/run.query.4.png "Verify output")

В области обозревателя данных можно увидеть, что выходные данные записаны в папку, указанную в параметрах выходных данных Data Lake Store (`streamanalytics/job/output/{date}/{time}`).  

## <a name="see-also"></a>Дополнительные материалы
* [Создание кластера HDInsight для работы с хранилищем озера данных](data-lake-store-hdinsight-hadoop-use-portal.md)



<!--HONumber=Nov16_HO3-->


