<properties
   pageTitle="Потоковая передача данных из Stream Analytics в хранилище озера данных | Azure"
   description="Использование Azure Stream Analytics для потоковой передачи данных в хранилище озера данных Azure"
   services="data-lake-store,stream-analytics" 
   documentationCenter=""
   authors="nitinme"
   manager="jhubbard"
   editor="cgronlun"/>

<tags
   ms.service="data-lake-store"
   ms.devlang="na"
   ms.topic="article"
   ms.tgt_pltfrm="na"
   ms.workload="big-data"
   ms.date="07/07/2016"
   ms.author="nitinme"/>

# Потоковая передача данных из большого двоичного объекта службы хранилища Azure в хранилище озера данных с помощью Azure Stream Analytics

Из этой статьи вы узнаете, как использовать хранилище озера данных Azure в качестве источника выходных данных для задания Azure Stream Analytics. В этой статье показан простой сценарий, в котором данные считываются из большого двоичного объекта службы хранилища Azure (входные данные) и записываются в хранилище озера данных (выходные данные).

>[AZURE.NOTE] В настоящий момент создание и настройка выходных данных Data Lake Store для Stream Analytics поддерживается только на [классическом портале Azure](https://manage.windowsazure.com). Поэтому в некоторых частях этого учебника будет использоваться классический портал Azure.

## Предварительные требования

Перед началом работы с этим учебником необходимо иметь следующее:

- **Подписка Azure.**. См. [Бесплатная пробная версия Azure](https://azure.microsoft.com/pricing/free-trial/).

- **Настройте свою подписку Azure** для использования общедоступной предварительной версии Data Lake Store. См. [инструкции](data-lake-store-get-started-portal.md#signup).

- **Учетная запись хранения Azure** Контейнер больших двоичных объектов из этой учетной записи будет использоваться для ввода данных для задания Stream Analytics. Для работы с этим учебником необходимо создать учетную запись хранилища с именем **datalakestoreasa**, а в ней — контейнер с именем **datalakestoreasacontainer**. После создания контейнера отправьте в него образец файла данных. Его можно получить из [репозитория Git озера данных Azure](https://github.com/Azure/usql/tree/master/Examples/Samples/Data/AmbulanceData/Drivers.txt). Чтобы отправить данные в контейнер больших двоичных объектов, можно использовать различные клиенты, например [обозреватель хранилищ Azure](http://storageexplorer.com/).

	>[AZURE.NOTE] При создании учетной записи на портале Azure ее необходимо создать с **классической** моделью развертывания. Это обеспечит доступность учетной записи хранения с классического портала Azure, поскольку именно его мы используем для создания задания Stream Analytics. Инструкции по созданию учетной записи хранения на портале Azure с помощью классической модели развертывания см. в разделе [Создание учетной записи хранения Azure](../storage/storage-create-storage-account/#create-a-storage-account).
	>
	> Кроме того, учетную запись хранения можно создать на классическом портале Azure.

- **Учетная запись хранилища озера данных Azure**. Следуйте инструкциям в разделе [Приступая к работе с хранилищем озера данных Azure на портале Azure](data-lake-store-get-started-portal.md).


## Создание задания Stream Analytics

Для начала нужно создать задание Stream Analytics с источником входных данных и целевым объектом для выходных данных. В этом руководстве источником является контейнер больших двоичных объектов Azure, а целевым объектом — хранилище озера данных.

1. Войдите на [классический портал Azure](https://manage.windowsazure.com).

2. В правой нижней части окна последовательно щелкните **Создать**, **Службы данных**, **Stream Analytics**, **Быстрое создание**. Укажите значения, показанные на рисунке ниже, и выберите команду **Создать задание Stream Analytics**.

	![Создание задания Stream Analytics](./media/data-lake-store-stream-analytics/create.job.png "Создание задания Stream Analytics")

## Создание входных данных большого двоичного объекта для задания

1. Откройте страницу задания Stream Analytics, перейдите на вкладку **Входные данные** и выберите команду **Добавить входные данные**, чтобы запустить мастер.

2. На странице **Добавление входных данных в задание** выберите параметр **Поток данных** и щелкните стрелку "Далее".

	![Добавление входных данных в задание](./media/data-lake-store-stream-analytics/create.input.1.png "Добавление входных данных в задание")

3. На странице **Добавление потока данных в задание** выберите параметр **Хранилище BLOB-объектов** и щелкните стрелку "Далее".

	![Добавление потока данных в задание](./media/data-lake-store-stream-analytics/create.input.2.png "Добавление потока данных в задание")

4. На странице **Настройки хранилища BLOB-объектов** укажите данные для хранилища, которое будет использоваться в качестве источника входных данных.

	![Указание параметров хранилища BLOB-объекта](./media/data-lake-store-stream-analytics/create.input.3.png "Указание параметров хранилища BLOB-объекта")

	* **Введите псевдоним входных данных**. Это уникальное имя для входных данных задания.
	* **Выберите учетную запись хранения**. Убедитесь, что учетная запись хранения находится в той же области, что и задание Stream Analytics. Иначе вам придется заплатить за перемещение данных между регионами.
	* **Укажите контейнер хранилища**. Можно создать новый контейнер или выбрать уже существующий.

	Щелкните стрелку «Далее».

5. На странице **Параметры сериализации** выберите формат сериализации **CSV**, для разделителя выберите значение **табуляция**, а для кодировки — **UTF8**. Затем нажмите кнопку с галочкой.

	![Указание параметров сериализации](./media/data-lake-store-stream-analytics/create.input.4.png "Указание параметров сериализации")

6. После завершения работы мастера входные данные большого двоичного объекта будут добавлены на вкладку **Входные данные**, а в столбце **Диагностика** должно отображаться значение **OК**. Можно также непосредственно проверить подключение ко входным данным. Для этого нажмите кнопку **Проверить подключение** внизу.

## Создание выходных данных хранилища озера данных для задания

1. Откройте страницу задания Stream Analytics, перейдите на вкладку **Выходные данные** и выберите команду **Добавить выходные данные**, чтобы запустить мастер.

2. На странице **Добавление выходных данных в задание** выберите параметр **Хранилище озера данных** и щелкните стрелку "Далее".

	![Добавление выходных данных в задание](./media/data-lake-store-stream-analytics/create.output.1.png "Добавление выходных данных в задание")

3. Если учетная запись хранилища озера данных уже создана, на странице **Авторизация подключения** нажмите кнопку **Авторизовать сейчас**. Если у вас нет учетной записи, щелкните ссылку **Зарегистрируйтесь сейчас**, чтобы создать учетную запись. Предположим, у вас уже есть учетная запись хранилища озера данных (как указано в предварительных требованиях). Будет выполнена автоматическая авторизация с использованием учетных данных, которые использовались для входа на классический портал Azure.

	![Авторизация хранилища озера данных](./media/data-lake-store-stream-analytics/create.output.2.png "Авторизация хранилища озера данных")

4. На странице **Параметры хранилища озера данных** введите сведения, как показано на снимке экрана ниже.

	![Указание параметров хранилища озера данных](./media/data-lake-store-stream-analytics/create.output.3.png "Указание параметров хранилища озера данных")

	* **Введите псевдоним выходных данных**. Это уникальное имя для выходных данных задания.
	* **Укажите учетную запись хранилища озера данных**. Она должна быть уже создана, как указано в предварительных требованиях.
	* **Укажите шаблон префикса пути**. Он необходим для идентификации выходных файлов, которые записывает задание Stream Analytics в хранилище озера данных. Поскольку заголовки выходных данных, записываемых заданием, имеют формат GUID, добавление префикса поможет идентифицировать записанные выходные данные. Если в префикс необходимо включить метки даты и времени, добавьте в шаблон префикса элемент `{date}/{time}`. Если включить этот элемент, то поля **Формат даты** и **Формат времени** станут доступны, и вы сможете выбрать нужный формат.

	Щелкните стрелку «Далее».

5. На странице **Параметры сериализации** выберите формат сериализации **CSV**, для разделителя выберите значение **табуляция**, а для кодировки — **UTF8**. Затем нажмите кнопку с галочкой.

	![Указание формата выходных данных](./media/data-lake-store-stream-analytics/create.output.4.png "Указание формата выходных данных")

6. После завершения работы мастера входные данные Data Lake Store будут добавлены на вкладку **Выходные данные**, а в столбце **Диагностика** должно отображаться значение **OК**. Можно также непосредственно проверить подключение к выходным данным. Для этого нажмите кнопку **Проверить подключение** внизу.

## Выполнение задания Stream Analytics

Чтобы выполнить задание Stream Analytics, необходимо выполнить запрос на вкладке "Запрос". В этом руководстве можно выполнить образец запроса, заменив заполнители псевдонимами входных и выходных данных, как показано на снимке экрана ниже.

![Выполнение запроса](./media/data-lake-store-stream-analytics/run.query.png "Выполнение запроса")

Нажмите кнопку **Сохранить** в нижней части экрана, а затем выберите команду **Запустить**. В диалоговом окне нажмите кнопку **Настраиваемое время** и выберите дату в прошлом, например **1/1/2016**. Нажмите кнопку с галочкой, чтобы запустить задание. Для запуска задания может потребоваться несколько минут.

![Настройка времени задания](./media/data-lake-store-stream-analytics/run.query.2.png "Настройка времени задания")

После запуска задания перейдите на вкладку **Монитор**, чтобы просмотреть результат обработки данных.

![Отслеживание задания](./media/data-lake-store-stream-analytics/run.query.3.png "Отслеживание задания")

Теперь с помощью [портала Azure](https://portal.azure.com) можно открыть учетную запись Data Lake Store и проверить, успешно ли записаны данные в эту учетную запись.

![Проверка выходных данных](./media/data-lake-store-stream-analytics/run.query.4.png "Проверка выходных данных")

В области обозревателя данных можно увидеть, что выходные данные записаны в папку, указанную в параметрах выходных данных Data Lake Store (`streamanalytics/job/output/{date}/{time}`).

## Дополнительные материалы

* [Создание кластера HDInsight для работы с хранилищем озера данных](data-lake-store-hdinsight-hadoop-use-portal.md)

<!---HONumber=AcomDC_0914_2016-->