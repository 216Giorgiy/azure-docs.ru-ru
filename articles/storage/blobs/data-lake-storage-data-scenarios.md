---
title: Сценарии работы с данными с использованием Azure Data Lake Storage 2-ого поколения | Документация Майкрософт
description: Сведения о разных сценариях и средствах для приема, обработки, скачивания и визуализации данных в Data Lake Storage 2-ого поколения (прежнее название — Azure Data Lake Store)
services: storage
author: normesta
ms.subservice: data-lake-storage-gen2
ms.service: storage
ms.topic: conceptual
ms.date: 12/06/2018
ms.author: normesta
ms.openlocfilehash: dfc47c40ce82eb8e9d414cb49c22bee033d000a6
ms.sourcegitcommit: 898b2936e3d6d3a8366cfcccc0fccfdb0fc781b4
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 01/30/2019
ms.locfileid: "55239038"
---
# <a name="using-azure-data-lake-storage-gen2-for-big-data-requirements"></a>Использование Azure Data Lake Storage 2-ого поколения для обеспечения соответствия требованиям больших данных

Процесс обработки больших данных состоит из указанных далее четырех ключевых этапов.

* Прием больших объемов данных в хранилище данных в режиме реального времени или в пакетах
* Обработка данных
* Загрузка данных
* Визуализация данных

В этой статье мы рассмотрим эти этапы применительно к Azure Data Lake Storage 2-го поколения, чтобы понять параметры и средства, доступные для удовлетворения потребностей в обработке больших данных.

## <a name="ingest-data-into-data-lake-storage-gen2"></a>Прием данных в Azure Data Lake Storage 2-го поколения
В этом разделе описываются различные источники данных и способы поступления этих данных в учетную запись Azure Data Lake Storage 2-го поколения.

![Прием данных в Data Lake Storage 2-го поколения](./media/data-lake-storage-data-scenarios/ingest-data.png "Прием данных в Data Lake Storage 2-го поколения")

### <a name="ad-hoc-data"></a>Специальные данные
Это небольшие наборы данных, которые используются для создания прототипов приложений для работы с большими данными. В зависимости от источника данных применяются разные способы приема специальных данных.

| источник данных | Средство для приема |
| --- | --- |
| Локальный компьютер |<ul> <li>[Обозреватель хранилища](https://azure.microsoft.com/features/storage-explorer/)</ul> |
| Большой двоичный объект хранилища Azure |<ul> <li>[Фабрика данных Azure](../../data-factory/connector-azure-data-lake-store.md)</li> <li>[Средство AzCopy ](../common/storage-use-azcopy-v10.md)</li><li>[DistCp, запущенный на кластере HDInsight](data-lake-storage-use-distcp.md)</li> </ul> |

### <a name="streamed-data"></a>Потоковые данные
Это данные, создаваемые различными источниками, такими как приложения, устройства, датчики и т. д. Для ввода этих данных в Data Lake Storage 2-го поколения можно использовать разные средства. Как правило, эти средства собирают и обрабатывают данные на основе событий в режиме реального времени, а затем записывают события в пакетном режиме в Data Lake Storage 2-го поколения для последующей обработки.

Ниже перечислены средства, которые можно использовать:

* [Azure HDInsight Storm](../../hdinsight/storm/apache-storm-write-data-lake-store.md): данные из кластера Storm можно напрямую записывать в Data Lake Storage 2-го поколения.

### <a name="relational-data"></a>Реляционные данные
Можно также извлекать данные из реляционных баз данных. В течение определенного периода времени реляционные базы данных собирают огромные объемы данных, которые после обработки с помощью конвейера больших данных могут предоставлять ценные сведения. Для перемещения таких данных в Data Lake Storage 2-го поколения можно использовать следующие средства.

* [Фабрика данных Azure](../../data-factory/copy-activity-overview.md)

### <a name="web-server-log-data-upload-using-custom-applications"></a>Данные журналов веб-сервера (отправка с помощью настраиваемых приложений)
Этот тип набора данных вызывается специально, так как анализ данных журналов веб-сервера часто используется в приложениях по работе с большими данными, и для его выполнения требуется отправка больших объемов файлов журналов в Data Lake Storage 2-го поколения. Для отправки таких данных воспользуйтесь следующими средствами или напишите собственные сценарии или приложения.

* [Фабрика данных Azure](../../data-factory/copy-activity-overview.md)

Отличным способом отправки данных журналов веб-сервера и других типов данных (например данных общественных мнений) является использование собственных написанных сценариев или приложений, поскольку вы можете включить компонент отправки данных в состав более масштабного приложения по работе с большими объемами данных. В одних случаях этот код может иметь форму сценария или простой программы командной строки. В других случаях код может использоваться для интеграции обработки больших данных в бизнес-приложение или решение.

### <a name="data-associated-with-azure-hdinsight-clusters"></a>Данные, связанные с кластерами HDInsight Azure
Большинство типов кластеров HDInsight (Hadoop, HBase, Storm) поддерживают Data Lake Storage 2-го поколения в качестве репозитория хранения данных. Кластеры HDInsight обращаются к данным из BLOB-объектов хранилища Azure (WASB). Для повышения производительности можно скопировать данные из WASB в учетную запись Data Lake Storage 2-го поколения, связанную с кластером. Для копирования данных можно использовать указанные далее средства.

* [Apache DistCp](data-lake-storage-use-distcp.md)
* [Средство AzCopy ](../common/storage-use-azcopy-v10.md)
* [Фабрика данных Azure](../../data-factory/connector-azure-data-lake-store.md)

### <a name="data-stored-in-on-premises-or-iaas-hadoop-clusters"></a>Данные, хранящиеся в локальных кластерах Hadoop или кластерах Hadoop в IaaS
Большие объемы данных могут храниться в кластерах Hadoop, размещенных локально на компьютерах, использующих HDFS. Кластеры Hadoop могут быть развернуты локально или работать в кластере IaaS в Azure. К копированию таких данных в Azure Data Lake Storage 2-го поколения могут предъявляться требования, в зависимости от того, является ли эта операция одноразовой или повторяющейся. Существуют различные возможности выполнить их. Ниже приведен список альтернативных вариантов и связанные с ними компромиссы.

| Подход | Сведения | Преимущества | Рекомендации |
| --- | --- | --- | --- |
| Использование Фабрики данных Azure (ADF) для копирования данных напрямую из кластеров Hadoop в Azure Data Lake Storage 2-го поколения |[ADF поддерживает HDFS в качестве источника данных.](../../data-factory/connector-hdfs.md) |ADF реализована готовая поддержка HDFS, а также первоклассные инструменты комплексного управления и мониторинга. |Требуется развернуть шлюз управления данными в локальном кластере или кластере IaaS. |
| Использование Distcp для копирования данных из Hadoop в службу хранилища Azure. Затем копирование данных из службы хранилища Azure в Data Lake Storage 2-го поколения с помощью соответствующего механизма. |Скопировать данные из службы хранилища Azure в Data Lake Storage 2-го поколения можно с помощью следующих элементов. <ul><li>[Фабрика данных Azure](../../data-factory/copy-activity-overview.md)</li><li>[Средство AzCopy ](../common/storage-use-azcopy-v10.md)</li><li>[Apache DistCp, запущенный в кластерах HDInsight](data-lake-storage-use-distcp.md)</li></ul> |Можно использовать инструменты с открытым кодом. |Многоэтапный процесс с использованием нескольких технологий. |

### <a name="really-large-datasets"></a>Очень большие наборы данных
Для отправки наборов данных размером в несколько терабайт использование описанных выше методов иногда может быть медленным и затратным процессом. В таких ситуациях будут уместны следующие варианты.

* **Использование Azure ExpressRoute.** Azure ExpressRoute позволяет создавать закрытые соединения между ЦОД Azure и вашей локальной инфраструктурой. Это надежный вариант для передачи больших объемов данных. Дополнительные сведения см. в [техническом обзоре ExpressRoute](../../expressroute/expressroute-introduction.md).

## <a name="process-data-stored-in-data-lake-storage-gen2"></a>Обработка данных, хранящихся в Data Lake Storage 2-го поколения
Данные, доступные в Data Lake Storage 2-го поколения, можно проанализировать с помощью поддерживаемых приложений для работы с большими данными. Сейчас для запуска заданий анализа для данных, хранящихся в Data Lake Storage 2-го поколения, можно использовать Azure HDInsight и Azure Databricks.

![Анализ данных в Data Lake Storage 2-го поколения](./media/data-lake-storage-data-scenarios/analyze-data.png "Анализ данных в Data Lake Storage 2-го поколения")


## <a name="download-data-from-data-lake-storage-gen2"></a>Скачивание данных из Data Lake Storage 2-го поколения
Возможно, вам потребуется скачать или переместить данные из Azure Data Lake Storage 2-го поколения в сценариях указанных далее.

* Перемещение данных в другие репозитории для взаимодействия с существующими конвейерами обработки данных. Например, можно переместить данные из Data Lake Storage 2-го поколения в Базу данных SQL Azure или на локальный сервер SQL Server.
* Загрузка данных на локальный компьютер для обработки в средах IDE при создании прототипов приложений.

![Вывод данных из Data Lake Storage 2-го поколения](./media/data-lake-storage-data-scenarios/egress-data.png "Вывод данных из Data Lake Storage 2-го поколения")

В таких ситуациях можно использовать любой из следующих вариантов:

* [Фабрика данных Azure](../../data-factory/copy-activity-overview.md)
* [Apache DistCp](data-lake-storage-use-distcp.md)

## <a name="visualize-data-in-data-lake-storage-gen2"></a>Визуализация данных в Data Lake Storage 2-го поколения
Для создания визуальных представлений данных, хранящихся в Data Lake Storage 2-го поколения, можно использовать сочетание служб.

![Визуализация данных в Data Lake Storage 2-го поколения](./media/data-lake-storage-data-scenarios/visualize-data.png "Визуализация данных в Data Lake Storage 2-го поколения")

* Сначала с помощью [Фабрики данных Azure переместите данные из Data Lake Storage 2-го поколения в Хранилище данных SQL Azure](../../data-factory/copy-activity-overview.md).
* После этого вы можете [интегрировать Power BI с хранилищем данных SQL Azure](../../sql-data-warehouse/sql-data-warehouse-get-started-visualize-with-power-bi.md) для создания визуального представления данных.
