<properties 
	pageTitle="Azure Stream Analytics: ключевые понятия | Azure" 
	description="Ознакомьтесь с ключевыми компонентами задания Stream Analytics, в том числе со сведениями о поддерживаемых входных и выходных данных, конфигурации задания и отображаемых метриках." 
	services="stream-analytics" 
	documentationCenter="" 
	authors="jeffstokes72" 
	manager="paulettm" 
	editor="cgronlun"/>

<tags 
	ms.service="stream-analytics" 
	ms.devlang="na" 
	ms.topic="article" 
	ms.tgt_pltfrm="na" 
	ms.workload="data-services" 
	ms.date="04/28/2015" 
	ms.author="jeffstok"/>


# Azure Stream Analytics: ключевые понятия 

Azure Stream Analytics является полностью управляемой службой, обеспечивающей низкую задержку, высокий уровень доступности и масштабируемую обработку сложных событий посредством потоковой передачи данных в облако. В предварительном выпуске Stream Analytics позволяет клиентам настраивать задания потоковой передачи данных для анализа потоков данных, а также осуществлять аналитику в квазиреальном режиме времени.


Области применения Stream Analytics:

- Выполнение сложной обработки событий с данными большого объема при высокой скорости передачи   
- Сбор данных о событиях из глобально распределенных ресурсов или оборудования, такого как подключенные автомобили или распределительные электросети 
- Обработка данных телеметрии для отслеживания и диагностики в квазиреальном режиме времени 
- Захват и архивирование событий в реальном времени для последующей обработки

Дополнительные сведения см. в статье [Введение в Azure Stream Analytics](stream.analytics.introduction).

Задания Stream Analytics состоят из одного или нескольких входных источников, запроса, передаваемого через входящие потоковые данные, и выходной цели.


## Входные данные

### Поток данных

Каждое определение задания Stream Analytics должно содержать по крайней мере один входной источник потока данных, используемый и изменяемый заданием. [Хранилище больших двоичных объектов Azure](azure.blob.storage) и [концентраторы событий Azure](azure.event.hubs) поддерживаются как входные источники потоков данных. Входные источники концентраторов событий используются для сбора потоков событий из различных устройств и служб, в то время как хранилище больших двоичных объектов может быть входным источником для приема данных большого объема. Так как большие двоичные объекты не выполняют потоковую передачу данных, задания Stream Analytics, работающие с такими объектами, не будут по своей природе временными, если только в большом двоичном объекте нет меток времени.

### Ссылочные данные
Служба Stream Analytics также поддерживает входные источники второго типа: ссылочные данные. Это вспомогательные данные, используемые для проверки взаимосвязи и подстановки. Данные здесь, как правило, не меняются или меняются редко. [Хранилище больших двоичных объектов](azure.blob.storage) — единственный поддерживаемый входной источник для ссылочных данных. Максимальный размер больших двоичных объектов в источнике ссылочных данных — 50 МБ.

Чтобы включить поддержку обновления ссылочных данных, необходимо указать список больших двоичных объектов во входной конфигурации, вставив маркеры {date} и {time} в шаблон пути. Задание обеспечит загрузку соответствующего большого двоичного объекта на основе даты и времени, закодированных в именах таких объектов с использованием часового пояса в формате UTC.

Например, если задание включает ссылочные входные данные, настроенные на портале с помощью шаблона пути (например, /sample/{date}/{time}/products.csv с форматом даты "ГГГГ-ММ-ДД" и форматом времени "ЧЧ: мм"), задание выберет файл /sample/2015-04-16/17:30/products.csv в 17:30, 16 апреля 2015 г. по часовому поясу в формате UTC (что эквивалентно 10:30, 16 апреля 2015 г. по часовому поясу в формате PST).


### Сериализация
Чтобы обеспечить правильное поведение запросов, Stream Analytics должен учитывать формат сериализации, применяемый к входящим потокам данных. В настоящее время для потоковой передачи данных поддерживаются форматы JSON, CSV и Avro, а для ссылочных данных — форматы CSV и JSON.

### Создаваемые свойства
В зависимости от используемого в задании входного типа будет создано несколько дополнительных полей с метаданными событий. Эти поля можно использовать в запросах так же, как и другие входные столбцы. Если в существующем событии есть поле с таким же именем, как одно из указанных ниже свойств, оно будет перезаписано входными метаданными.

<table border="1">
	<tr>
		<th></th>
		<th>Свойство</th>
		<th>Описание</th>
	</tr>
	<tr>
		<td rowspan="4" valign="top"><strong>Большой двоичный объект</strong></td>
		<td>BlobName</td>
		<td>Имя входного большого двоичного объекта, от которого поступило событие.</td>
	</tr>
	<tr>
		<td>EventProcessedUtcTime</td>
		<td>Дата и время обработки записи большого двоичного объекта.</td>
	</tr>
	<tr>
		<td>BlobLastModifiedUtcTime</td>
		<td>Дата и время последнего изменения большого двоичного объекта.</td>
	</tr>
	<tr>
		<td>PartitionId</td>
		<td>Идентификатор секции для входного адаптера (нумерация идет от нуля).</td>
	</tr>
	<tr>
		<td rowspan="3" valign="top"><b>Концентратор событий</b></td>
		<td>EventProcessedUtcTime</td>
		<td>Дата и время обработки события.</td>
	</tr>
	<tr>
		<td>EventEnqueuedUtcTime</td>
		<td>Дата и время получения события концентраторами событий.</td>
	</tr>
	<tr>
		<td>PartitionId</td>
		<td>Идентификатор секции для входного адаптера (нумерация идет от нуля).</td>
	</tr>
</table>

###Секции с медленно записываемыми входными данными или без входных данных.
Если данные считываются из входных источников, включающих несколько секций, и при этом в одной или нескольких секциях наблюдается задержка или нет данных, заданию потоковой передачи необходимо решить, как устранить эту проблему для дальнейшей поддержки потока событий в системе. Входной параметр "Максимально допустимая задержка прибытия" управляет этими действиями и по умолчанию ожидает поступления данных в течение неопределенного времени. Это значит, что метки времени событий не будут изменены, однако при этом события будут проходить с учетом скорости потока в самой медленной входной секции, останавливаясь в случае отсутствия данных в одной или нескольких входных секциях. Это полезно, если данные равномерно распределяются между входными секциями и крайне важно обеспечить согласованность событий по времени. Пользователь также может решить ожидать данные только в течение ограниченного времени. В этом случае параметр "Максимально допустимая задержка прибытия" определяет задержку, после которого задание будет продолжено. При этом запаздывающие входные секции пропускаются, а задание реагирует на события в соответствии со значением параметра "Действие с поздними событиями", удаляя события или изменяя их метки времени, если данные поступают позже. Это полезно, если задержка имеет первостепенное значение и при этом допускается смещение меток времени, однако входные данные могут распределяться неравномерно.

###Секции с неупорядоченными событиями
Если в запросе задания потоковой передачи используется ключевое слово TIMESTAMP BY, невозможно гарантировать порядок поступления событий во входной источник. Некоторые события в одной входной секции могут отставать. Параметр "Максимально допустимое нарушение порядка во входном источнике" определяет действия задания потоковой передачи в отношении событий, которые нарушают установленный порядок, в соответствии со значением параметра "Действие с поздними событиями", удаляя события или изменяя их метки времени.

### Дополнительные ресурсы
Дополнительные сведения о создании входных источников см. в [руководстве по программированию концентраторов событий](azure.event.hubs.developer.guide) и статье [Использование хранилища BLOB-объектов из .NET](azure.blob.storage.use).



## Query
Логика фильтрации входящих данных, их обработки и управления ими определяется в запросе заданий Stream Analytics. Запросы создаются на языке запросов Stream Analytics, SQL-подобном языке, во многом представляющем собой подмножество стандартного синтаксиса T-SQL с некоторыми специфическими расширениями для временных запросов.

### Оконное расширение
Оконные расширения позволяют выполнять агрегирование и вычисление над подмножествами событий, попадающими в определенный период времени. Функции оконного расширения вызываются с помощью инструкции **GROUP BY**. Например, следующий запрос подсчитывает количество событий, полученное в секунду:

	SELECT Count(*) 
	FROM Input1 
	GROUP BY TumblingWindow(second, 1) 

### Шаги выполнения
При составлении более сложных запросов можно использовать стандартное SQL-предложение **WITH**, чтобы определить временное именованный результирующий набор. Например, предложение **WITH** используется в этом запросе, чтобы выполнить преобразование в два этапа:
 
	WITH step1 AS ( 
		SELECT Avg(Reading) as avr 
		FROM temperatureInput1 
		GROUP BY Building, TumblingWindow(hour, 1) 
	) 

	SELECT Avg(avr) AS campus_Avg 
	FROM step1 
	GROUP BY TumblingWindow (day, 1) 

Дополнительные сведения о языке запросов см. в [справочнике по языку запросов Azure Stream Analytics](stream.analytics.query.language.reference).

## Выходные данные
В назначение выходных данных записываются результаты выполнения задания Stream Analytics. Результаты постоянно записываются в назначение выходных данных по мере обработки заданием входных событий. Поддерживаются следующие назначения выходных данных.

- Концентраторы событий Azure. Если нужно объединить несколько потоковых конвейеров (например, для ответной выдачи команд устройствам), выберите концентратор событий в качестве назначения выходных данных.
- Хранилище больших двоичных объектов Azure. Используйте хранилище больших двоичных объектов для долгосрочного архивирования выходных данных или хранения данных для последующей обработки.
- Хранилище таблиц Azure. Оно представляет собой хранилище структурированных данных с меньшим числом ограничений для схемы. В одной таблице Azure можно хранить сущности разных типов, использующие различные схемы. Хранилище таблиц Azure можно использовать для постоянного хранения данных и эффективного их извлечения. Дополнительные сведения см. в статьях [Введение в хранилище Azure](storage.introduction.md) и [Проектирование стратегии масштабируемого секционирования для хранилища таблиц Azure](https://msdn.microsoft.com/library/azure/hh508997.aspx).
- База данных SQL Azure. Это назначение выходных данных подходит для реляционных данных или приложений, которые зависят от содержимого, размещенного в базе данных.


## Задания масштабирования

Задание Stream Analytics можно масштабировать с помощью настроек единиц потоковой передачи, которые определяют объем вычислительной мощности, получаемый заданием. Каждая единица потоковой передачи соответствует пропускной способности около 1 МБ/с. Каждая подписка предоставляет квоту в 12 единиц потоковой передачи на регион, распределяемую между заданиями в этом регионе.

Дополнительные сведения см. в статье [Масштабирование заданий в службе Azure Stream Analytics](stream.analytics.scale.jobs).


## Мониторинг заданий и устранение неполадок в них

### Учетная запись хранения регионального мониторинга

Чтобы отслеживать задания в службе Stream Analytics, вам необходимо назначить учетную запись хранения Azure для мониторинга данных в каждом регионе, содержащем задания Stream Analytics. Этот процесс производится при создании задания.

### Метрики
Для мониторинга использования и производительности заданий Stream Analytics доступны следующие метрики.

- Ошибки. Количество сообщений об ошибках, вызванных заданием Stream Analytics.
- Входные события. Объем данных, полученных заданием Stream Analytics, измеренный счетчиком событий.
- Выходные события. Объем данных, отправляемых заданием Stream Analytics в выходной источник, измеренный счетчиком событий.
- Неупорядоченные события. Количество событий, полученных в неактуальное время, которые были удалены или получили откорректированную метку времени в соответствии с политикой в отношении неупорядоченных событий.
- Ошибки преобразования данных. Количество ошибок преобразования данных, вызванных заданием Stream Analytics.

### Журналы операций
Отладку и диагностику заданий Stream Analytics лучше всего осуществлять с помощью журналов операций Azure. Журналы операций можно открыть в разделе **Службы управления** портала. Чтобы просмотреть журналы своего задания, задайте для параметра **Тип службы** значение **Stream Analytics**, а для параметра **Имя службы** — имя задания.


## Управление заданиями 

### Запуск и остановка заданий
При запуске задания вам будет предложено указать значение параметра **Начало вывода**, который определяет, когда это задание начнет выдавать результирующие выходные данные. Если связанный запрос включает в себя окно, задание начнет получать входные данные из входных источников ввода в начале требуемого периода окна, чтобы подготовить первое выходное событие к указанному времени. Доступны три параметра: **Время запуска задания**, **Пользовательский** и **Время последней остановки**. По умолчанию задается параметр **Время запуска задания**. Если задание временно остановлено, для параметра **Время последней остановки** рекомендуется установить значение **Начало вывода**, чтобы возобновить выполнение задания с момента последнего вывода и избежать потери данных. При выборе параметра **Пользовательский** вам необходимо указать дату и время. Этот параметр полезен для указания объема используемых исторических данных из входных источников и для приема данных в конкретное время.

### Настройка заданий
Вы сможете настроить следующие параметры верхнего уровня для задания Stream Analytics.

- **Начало вывода**. Используйте этот параметр, чтобы указать, когда это задание начнет выдавать результирующие выходные данные. Если связанный запрос включает в себя окно, задание начнет получать входные данные из входных источников в начале требуемого периода, чтобы подготовить первое выходное событие к указанному времени. Доступны два параметра: **Время запуска задания** и **Пользовательский**. По умолчанию задается параметр **Время запуска задания**. При выборе параметра **Пользовательский** вам необходимо указать дату и время. Этот параметр полезен для указания объема используемых данных из входных источников и для приема данных из конкретного времени: например, с момента последней остановки задания. 
- **Неупорядоченная политика**: параметры для обработки событий, которые не поступают в задание Stream Analytics последовательно. Вы можете указать временной порог, в пределах которого события будут переупорядочены, задав диапазон отклонений, а также указать действие, применяемое к событиям за пределами этого диапазона: **Удалить** или **Изменить**. Вариант **Удалить** приведет к удалению всех событий, полученных в неактуальное время, а вариант **Изменить** изменит значение System.Timestamp для таких событий на метку времени упорядоченного события, полученного в последнюю очередь. 
- **Политика в отношении позднего прибытия**. Если данные считываются из входных источников, включающих несколько секций, и при этом в одной или нескольких секциях наблюдается задержка или нет данных, заданию потоковой передачи необходимо решить, как устранить эту проблему для дальнейшей поддержки потока событий в системе. Входной параметр "Максимально допустимая задержка прибытия" управляет этими действиями и по умолчанию ожидает поступления данных в течение неопределенного времени. Это значит, что метки времени событий не будут изменены, однако при этом события будут проходить с учетом скорости потока в самой медленной входной секции, останавливаясь в случае отсутствия данных в одной или нескольких входных секциях. Это полезно, если данные равномерно распределяются между входными секциями и крайне важно обеспечить согласованность событий по времени. Пользователь также может решить ожидать данные только в течение ограниченного времени. В этом случае параметр "Максимально допустимая задержка прибытия" определяет задержку, после которого задание будет продолжено. При этом запаздывающие входные секции пропускаются, а задание реагирует на события в соответствии со значением параметра "Действие с поздними событиями", удаляя события или изменяя их метки времени, если данные поступают позже. Это полезно, если задержка имеет первостепенное значение и при этом допускается смещение меток времени, однако входные данные могут распределяться неравномерно.
- **Локаль**. Этот параметр используется для указания приоритета интернационализации для задания Stream Analytics. Хотя метки времени данных не зависят от локали, данные параметры влияют на анализ, сравнение и сортировку данных заданием. В предварительной версии поддерживается только локаль **en-US**.

### Состояние

Состояние заданий Stream Analytics можно проверить на портале Azure. Выполняющиеся задания могут находиться в одном из трех состояний: **Неактивно**, **Обработка** или **Деградация**. Ниже приведены определения всех этих состояний.

- **Неактивно**. С момента создания задания или в течение 2 последних минут им не получено никаких входных данных. Если задание находится в состоянии **Неактивно** в течение длительного периода времени, скорее всего, входные данные есть, но при этом нет необработанных сведений для обработки.
- **Обработка**. Задание Stream Analytics успешно обработало ненулевое количество отфильтрованных событий ввода. Если задание находится в состоянии **Обработка**, не выдавая выходные данные, скорее всего, это обусловлено большим окном времени обработки или сложностью логики запроса.
- **Деградация**. Это состояние указывает, что при выполнении задания Stream Analytics возникла одна из следующих ошибок: ошибка связи с входными или выходными данными, ошибка запроса или ошибка во время выполнения повторной попытки. Чтобы определить тип ошибок, которые возникли при выполнении задания, просмотрите журналы операций.


## Получение поддержки
За дополнительной помощью обращайтесь на наш [форум Azure Stream Analytics](https://social.msdn.microsoft.com/Forums/en-US/home?forum=AzureStreamAnalytics).


## Дальнейшие действия

- [Введение в Azure Stream Analytics](stream-analytics-introduction.md)
- [Приступая к работе с Azure Stream Analytics](stream-analytics-get-started.md)
- [Масштабирование заданий в службе Azure Stream Analytics](stream-analytics-scale-jobs.md)
- [Справочник по языку запросов Azure Stream Analytics](https://msdn.microsoft.com/library/azure/dn834998.aspx)
- [Справочник по API-интерфейсу REST управления Stream Analytics](https://msdn.microsoft.com/library/azure/dn835031.aspx)



<!--Image references-->
[5]: ./media/markdown-template-for-new-articles/octocats.png
[6]: ./media/markdown-template-for-new-articles/pretty49.png
[7]: ./media/markdown-template-for-new-articles/channel-9.png


<!--Link references-->
[azure.blob.storage]: http://azure.microsoft.com/documentation/services/storage/
[azure.blob.storage.use]: http://azure.microsoft.com/documentation/articles/storage-dotnet-how-to-use-blobs/

[azure.event.hubs]: http://azure.microsoft.com/services/event-hubs/
[azure.event.hubs.developer.guide]: http://msdn.microsoft.com/library/azure/dn789972.aspx

[stream.analytics.query.language.reference]: http://go.microsoft.com/fwlink/?LinkID=513299
[stream.analytics.forum]: http://go.microsoft.com/fwlink/?LinkId=512151

[stream.analytics.introduction]: stream-analytics-introduction.md
[stream.analytics.get.started]: stream-analytics-get-started.md
[stream.analytics.developer.guide]: stream-analytics-developer-guide.md
[stream.analytics.scale.jobs]: stream-analytics-scale-jobs.md
[stream.analytics.limitations]: stream-analytics-limitations.md
[stream.analytics.query.language.reference]: http://go.microsoft.com/fwlink/?LinkID=513299
[stream.analytics.rest.api.reference]: http://go.microsoft.com/fwlink/?LinkId=517301

<!--HONumber=54-->