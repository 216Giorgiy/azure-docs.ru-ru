---
title: "Руководство по загрузке данных. Хранилище данных SQL Azure | Документация Майкрософт"
description: "Рекомендации по загрузке данных и выполнению извлечения, загрузки и преобразования в хранилище данных SQL Azure."
services: sql-data-warehouse
documentationcenter: NA
author: barbkess
manager: jenniehubbard
editor: 
ms.assetid: 7b698cad-b152-4d33-97f5-5155dfa60f79
ms.service: sql-data-warehouse
ms.devlang: NA
ms.topic: get-started-article
ms.tgt_pltfrm: NA
ms.workload: data-services
ms.custom: performance
ms.date: 12/13/2017
ms.author: barbkess
ms.openlocfilehash: 8903be1361d1574a5d81b69223f608ccb7a698ea
ms.sourcegitcommit: fa28ca091317eba4e55cef17766e72475bdd4c96
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 12/14/2017
---
# <a name="guidance-for-loading-data-into-azure-sql-data-warehouse"></a>Руководство по загрузке данных в хранилище данных SQL Azure
Рекомендации и оптимизация производительности загрузки данных в хранилище данных SQL Azure: 

- Дополнительные сведения о PolyBase и проектировании процесса извлечения, загрузки и преобразования (ELT) см. в статье о [проектировании ELT для хранилища данных SQL](design-elt-data-loading.md).
- Руководство по загрузке см. в статье [Загрузка данных из хранилища BLOB-объектов Azure в хранилище данных SQL Azure с помощью PolyBase](load-data-from-azure-blob-storage-using-polybase.md).


## <a name="extract-source-data"></a>Извлечение исходных данных

При экспорте данных в формат файлов ORC из SQL Server или хранилища данных SQL Azure число текстовых столбцов большого объема может быть ограничено до 50 из-за ошибок нехватки памяти в Java. Чтобы решить эту проблему, экспортируйте только подмножество столбцов.


## <a name="land-data-to-azure"></a>Размещение данных в Azure
PolyBase не может загружать строки, содержащие более миллиона байтов данных. Объем данных, загружаемых в текстовые файлы в хранилище BLOB-объектов Azure или Azure Data Lake Store, не должен превышать один миллион байтов. Это требование не зависит от определенной схемы таблицы.

Совместите уровень хранения и хранилище данных, чтобы свести задержки к минимуму.

## <a name="data-preparation"></a>Подготовка данных

Все форматы файлов имеют разные характеристики производительности. Чтобы максимально ускорить загрузку, используйте сжатые текстовые файлы с разделителями. Разница между производительностью UTF-8 и UTF-16 будет минимальной.

Разбейте большие сжатые файлы на сжатые файлы меньшего размера.

## <a name="create-designated-loading-users"></a>Создание назначенных для загрузки пользователей
Чтобы запускать загрузки в соответствующих вычислительных ресурсах, создайте пользователей, назначенных для выполнения загрузок. Назначьте каждому пользователю, выполняющему загрузку, конкретный ресурс класса. Войдите от имени одного из таких пользователей и запустите загрузку. Загрузка выполняется с помощью класса ресурсов пользователя.  Использовать этот метод проще, чем пытаться изменить класс ресурсов пользователя в соответствии с текущими потребностями.

При помощи этого кода создается пользователь, выполняющий загрузку, для класса ресурсов staticrc20. Пользователь получает разрешение управлять базой данных и добавляется как члена роли для базы данных staticrc20. Чтобы выполнить загрузку с помощью ресурсов класса staticRC20, войдите в систему как пользователь LoaderRC20 и запустите загрузку. 

    ```sql
    CREATE LOGIN LoaderRC20 WITH PASSWORD = 'a123STRONGpassword!';
    CREATE USER LoaderRC20 FOR LOGIN LoaderRC20;
    GRANT CONTROL ON DATABASE::[mySampleDataWarehouse] to LoaderRC20;
    EXEC sp_addrolemember 'staticrc20', 'LoaderRC20';
    ```

Запускайте загрузку в статических (не динамических) классах ресурсов. Статические классы ресурсов гарантируют, что вы используете одни и те же ресурсы, независимо от [уровня обслуживания](performance-tiers.md#service-levels). Если вы используете динамический класс ресурсов, ресурсы будут разными на разных уровнях обслуживания. При использовании динамических классов более низкий уровень обслуживания означает, что, возможно, для пользователя, выполняющего загрузку, потребуется более высокий класс ресурса.

### <a name="example-for-isolating-loading-users"></a>Пример для изоляции пользователей, выполняющих загрузку

Зачастую необходимо иметь несколько пользователей, которые могут загрузить данные в хранилище данных SQL. Так как для использования инструкции [CREATE TABLE AS SELECT (Transact-SQL)][CREATE TABLE AS SELECT (Transact-SQL)] требуются разрешения на управление базой данных, вы получите несколько пользователей с контролем доступа во всех схемах. Чтобы ограничить это, можно использовать инструкцию DENY CONTROL.

Пример. Рассмотрим схемы базы данных schema_A для отдела A и schema_B для отдела B. Задайте пользователей базы данных user_A и user_B в качестве пользователей для загрузки PolyBase в отделе A и B соответственно. Им обоим предоставлены разрешения CONTROL для базы данных. Теперь создатели схем A и B блокируют свои схемы, используя инструкцию DENY:

```sql
   DENY CONTROL ON SCHEMA :: schema_A TO user_B;
   DENY CONTROL ON SCHEMA :: schema_B TO user_A;
```   

Теперь для пользователей user_A и user_B доступ к схеме другого отдела заблокирован.


## <a name="load-to-a-staging-table"></a>Загрузка в промежуточную таблицу

Чтобы максимально ускорить загрузку, выполняйте ее в round_robin — промежуточную таблицу кучи. Это наиболее эффективный способ перемещения данных с уровня службы хранилища Azure в хранилище данных SQL.

Увеличьте масштаб хранилища данных, если планируется выполнение большого задания загрузки.

Чтобы оптимизировать производительность загрузки, запускайте одновременно только одно задание загрузки.

### <a name="optimize-columnstore-index-loads"></a>Оптимизация загрузок в индекс columnstore

Индексы columnstore требуют много памяти для сжатия данных в группы строк высокого качества. Чтобы обеспечить лучшее сжатие и эффективность индексов, индекс columnstore должен сжать в каждую группу строк 1 048 576 строк. Это максимальное количество для группы строк. При нехватке памяти индекс columnstore не сможет добиться максимального сжатия. Это влияет на производительность запросов. Дополнительные сведения см. в статье [Максимальное повышение качества группы строк для индекса columnstore](sql-data-warehouse-memory-optimizations-for-columnstore-compression.md).

- Чтобы пользователям, выполняющим загрузку, хватало памяти для максимального сжатия, они должны быть членами средних и крупных классов ресурсов. 
- Загрузите достаточно строк, чтобы полностью заполнить новые группы строк. При массовой загрузке каждые 1 048 576 строк загружаются непосредственно в columnstore. Загрузки, которые содержат менее 102 400 строк, отправляют строки в deltastore, который содержит строки в кластеризованном индексе, пока не наберется достаточно строк для сжатия. Если загрузить слишком мало строк, они все могут перейти в deltastore и не будут сжаты непосредственно в формат columnstore.


### <a name="handling-loading-failures"></a>Обработка ошибок загрузки

Загрузка с помощью внешней таблицы может завершиться ошибкой *Запрос прерван — достигнуто максимальное пороговое значение отклонения при чтении из внешнего источника*. Это означает, что внешние данные содержат *«грязные»* записи. Запись данных считается "грязной", если фактические типы данных и количество столбцов не соответствуют определениям столбцов из внешней таблицы или если данные не соответствуют указанному формату внешнего файла. 

Чтобы устранить эту проблему, убедитесь в правильности определений внешней таблицы и формата внешнего файла, а также в том, что внешние данные соответствуют этим определениям. Если подмножество записей внешних данных "грязные", можно отклонить эти записи для запросов с помощью параметров отклонения в CREATE EXTERNAL TABLE DDL.



## <a name="insert-data-into-production-table"></a>Вставка данных в рабочую таблицу
Здесь приведены рекомендации по вставке строк в рабочие таблицы.


### <a name="batch-insert-statements"></a>Пакетные инструкции INSERT
Одноразовую загрузку в небольшую таблицу или даже периодическую перезагрузку результатов поиска рекомендуется выполнять, используя такой [синтаксис инструкции INSERT](/sql/t-sql/statements/insert-transact-sql.md): `INSERT INTO MyLookup VALUES (1, 'Type 1')`.  Единоразовые множественные вставки не так эффективны, как массовая загрузка. 

Если вы выполняете тысячи или более вставок в течение дня, объедините вставки в пакет, чтобы выполнить массовую загрузку.  Разработайте процессы для добавления единоразовых вставок в файл, а затем создайте другой процесс, который периодически загружает файл.

### <a name="create-statistics-after-the-load"></a>Создание статистики после загрузки

Чтобы повысить производительность запросов, важно сформировать статистические данные для всех столбцов всех таблиц после первой загрузки или после значительных изменений в данных.  Подробное описание статистики см. в статье [Статистика][Statistics]. В следующем примере создается статистика для пяти столбцов таблицы Customer_Speed.

```sql
create statistics [SensorKey] on [Customer_Speed] ([SensorKey]);
create statistics [CustomerKey] on [Customer_Speed] ([CustomerKey]);
create statistics [GeographyKey] on [Customer_Speed] ([GeographyKey]);
create statistics [Speed] on [Customer_Speed] ([Speed]);
create statistics [YearMeasured] on [Customer_Speed] ([YearMeasured]);
```

## <a name="rotate-storage-keys"></a>Смена ключей к хранилищу данных
В целях безопасности рекомендуем регулярно менять ключ доступа к хранилищу BLOB-объектов. Для учетной записи хранения больших двоичных объектов есть два ключа хранения. Это необходимо для перемещения ключей.

Для смены ключей учетной записи службы хранилища Azure выполните следующие действия.

1. Создайте второй набор учетных данных для базы данных, используя вторичный ключ доступа к хранилищу.
2. Создайте второй внешний источник данных, используя эти учетные данные.
3. Удалите и создайте внешнюю таблицу или таблицы, указывающие на только что созданный внешний источник данных. 

После переноса внешних таблиц в новый источник данных выполните задачи очистки.

1. Удалите первый внешний источник данных.
2. Удалите первые учетные данные, собранные в базе данных, основанные на первичном ключе доступа к хранилищу.
3. Войдите в Azure и повторно создайте первичный ключ доступа, который будет использоваться до следующей смены ключей.


## <a name="next-steps"></a>Дальнейшие действия
Инструкции по мониторингу процесса загрузки см. в статье [Мониторинг рабочей нагрузки с помощью динамических административных представлений](sql-data-warehouse-manage-monitor.md).



