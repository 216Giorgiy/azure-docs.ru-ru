<properties 
	pageTitle="Ознакомление с основными понятиями Stream Analytics | Microsoft Azure" 
	description="Ознакомьтесь с основными понятиями Azure Stream Analytics — компонентами задания Stream Analytics, в том числе со сведениями о поддерживаемых входных и выходных данных, конфигурации задания и метриках." 
	keywords="event processing,data stream,key concepts,serialization"	
	services="stream-analytics" 
	documentationCenter="" 
	authors="jeffstokes72" 
	manager="paulettm" 
	editor="cgronlun" />

<tags 
	ms.service="stream-analytics" 
	ms.devlang="na" 
	ms.topic="article" 
	ms.tgt_pltfrm="na" 
	ms.workload="data-services" 
	ms.date="04/28/2015" 
	ms.author="jeffstok" />


# Основные понятия Stream Analytics: руководство по основам работы с заданиями Stream Analytics 

Azure Stream Analytics — это полностью управляемая служба, обеспечивающая малые задержки, высокий уровень доступности и масштабируемости, а также возможности сложной обработки событий для потока данных в облаке. Stream Analytics дает клиентам возможность настраивать задания потоковой передачи данных для анализа потоков данных и выполнять его в режиме, близком к режиму реального времени. В этой статье описываются основные понятия, связанные с заданиями Stream Analytics.

## Возможности Stream Analytics
С помощью Stream Analytics можно выполнять указанные ниже операции.

- Сложная обработка событий в потоках больших объемов данных, передающихся с большой скоростью.    
- Сбор данных о событиях с глобально распределенных ресурсов или оборудования, например с подключенных к сети автомобилей или из сетей коммунальных служб. 
- Обработка данных телеметрии для мониторинга и диагностики в режиме, близком к режиму реального времени. 
- Захват и архивирование событий в реальном времени для последующей обработки

Дополнительные сведения см. в статье [Введение в Azure Stream Analytics](stream.analytics.introduction).

Задание Stream Analytics включает следующие компоненты: * один или несколько источников входных данных; * запрос через поток входных данных; * целевые выходные данные.


## Входные данные

### Поток данных

В каждом определении задания Stream Analytics должен содержаться как минимум один входной источник потока данных, потребляемый и изменяемый заданием. [Хранилище больших двоичных объектов Azure](azure.blob.storage) и [концентраторы событий Azure](azure.event.hubs) поддерживаются как источники входных потоковых данных. Входные источники концентраторов событий используются для сбора потоков событий из различных устройств и служб, в то время как хранилище больших двоичных объектов может быть входным источником для приема данных большого объема. Так как большие двоичные объекты не выполняют потоковую передачу данных, задания Stream Analytics, работающие с такими объектами, не будут по своей природе временными, если только в большом двоичном объекте нет меток времени.

### Ссылочные данные
Служба Stream Analytics также поддерживает входные источники второго типа: ссылочные данные. Это вспомогательные данные, используемые для проверки взаимосвязи и подстановки. Данные здесь, как правило, не меняются или меняются редко. [Хранилище больших двоичных объектов](azure.blob.storage) — единственный поддерживаемый входной источник для ссылочных данных. Максимальный размер больших двоичных объектов в источнике ссылочных данных — 50 МБ.

Чтобы включить поддержку обновления ссылочных данных, необходимо указать список больших двоичных объектов во входной конфигурации, вставив маркеры {date} и {time} в шаблон пути. Задание обеспечит загрузку соответствующего большого двоичного объекта на основе даты и времени, закодированных в именах таких объектов с использованием часового пояса в формате UTC.

Например, если задание включает ссылочные входные данные, настроенные на портале с помощью шаблона пути (например, /sample/{date}/{time}/products.csv с форматом даты "ГГГГ-ММ-ДД" и форматом времени "ЧЧ: мм"), задание выберет файл /sample/2015-04-16/17:30/products.csv в 17:30, 16 апреля 2015 г. по часовому поясу в формате UTC (что эквивалентно 10:30, 16 апреля 2015 г. по часовому поясу в формате PST).


### Сериализация
Чтобы обеспечить правильное поведение запросов, Stream Analytics должен учитывать формат сериализации, применяемый к входящим потокам данных. В настоящее время поддерживаются следующие форматы сериализации: JSON, CSV и Avro для потоков данных и CSV или JSON для ссылочных данных.

### Создаваемые свойства
В зависимости от используемого в задании входного типа будет создано несколько дополнительных полей с метаданными событий. Эти поля можно использовать в запросах так же, как и другие входные столбцы. Если в существующем событии есть поле с таким же именем, как одно из указанных ниже свойств, оно будет перезаписано входными метаданными.

<table border="1">
	<tr>
		<th></th>
		<th>Свойство</th>
		<th>Описание</th>
	</tr>
	<tr>
		<td rowspan="4" valign="top"><strong>Большой двоичный объект</strong></td>
		<td>BlobName</td>
		<td>Имя входного большого двоичного объекта, от которого поступило событие.</td>
	</tr>
	<tr>
		<td>EventProcessedUtcTime</td>
		<td>Дата и время обработки записи большого двоичного объекта.</td>
	</tr>
	<tr>
		<td>BlobLastModifiedUtcTime</td>
		<td>Дата и время последнего изменения большого двоичного объекта.</td>
	</tr>
	<tr>
		<td>PartitionId</td>
		<td>Идентификатор секции для входного адаптера (нумерация идет от нуля).</td>
	</tr>
	<tr>
		<td rowspan="3" valign="top"><b>Концентратор событий</b></td>
		<td>EventProcessedUtcTime</td>
		<td>Дата и время обработки события.</td>
	</tr>
	<tr>
		<td>EventEnqueuedUtcTime</td>
		<td>Дата и время получения события концентраторами событий.</td>
	</tr>
	<tr>
		<td>PartitionId</td>
		<td>Идентификатор секции для входного адаптера (нумерация идет от нуля).</td>
	</tr>
</table>

###Секции с медленно записываемыми входными данными или без входных данных.
Если данные считываются из входных источников, включающих несколько секций, и при этом в одной или нескольких секциях наблюдается задержка или нет данных, заданию потоковой передачи необходимо решить, как устранить эту проблему для дальнейшей поддержки потока событий в системе. Входной параметр "Максимально допустимая задержка прибытия" управляет этими действиями и по умолчанию ожидает поступления данных в течение неопределенного времени. Это значит, что метки времени событий не будут изменены, однако при этом события будут проходить с учетом скорости потока в самой медленной входной секции, останавливаясь в случае отсутствия данных в одной или нескольких входных секциях. Это полезно, если данные равномерно распределяются между входными секциями и крайне важно обеспечить согласованность событий по времени.

Кроме того, вы можете принять решение ожидать данные только в течение ограниченного периода времени. В этом случае параметр "Максимально допустимая задержка прибытия" определяет задержку, после которой задание будет продолжено. При этом запаздывающие входные секции пропускаются, а задание реагирует на события в соответствии со значением параметра "Действие с поздними событиями", удаляя события или изменяя их метки времени, если данные поступают позже. Это полезно, если задержка имеет первостепенное значение и при этом допускается смещение меток времени, однако входные данные могут распределяться неравномерно.

###Секции с неупорядоченными событиями
Если в запросе задания потоковой передачи используется ключевое слово TIMESTAMP BY, невозможно гарантировать порядок поступления событий во входной источник. Некоторые события в одной входной секции могут отставать. Параметр "Максимально допустимое нарушение порядка во входном источнике" определяет действия задания потоковой передачи в отношении событий, которые нарушают установленный порядок, в соответствии со значением параметра "Действие с поздними событиями", удаляя события или изменяя их метки времени.

### Дополнительные ресурсы
Дополнительные сведения о создании входных источников см. в [руководстве по программированию концентраторов событий](azure.event.hubs.developer.guide) и статье [Использование хранилища BLOB-объектов из .NET](azure.blob.storage.use).



## Запрос
Логические операции фильтрации и обработки входных данных, а также работы с ними, задаются в запросах заданий Stream Analytics. Запросы создаются на языке запросов Stream Analytics, SQL-подобном языке, во многом представляющем собой подмножество стандартного синтаксиса T-SQL с некоторыми специфическими расширениями для временных запросов.

### Оконное расширение
Оконные расширения позволяют выполнять агрегирование и вычисление над подмножествами событий, попадающими в определенный период времени. Функции оконного расширения вызываются с помощью инструкции **GROUP BY**. Например, следующий запрос подсчитывает количество событий, полученное в секунду:

	SELECT Count(*) 
	FROM Input1 
	GROUP BY TumblingWindow(second, 1) 

### Шаги выполнения
При составлении более сложных запросов можно использовать стандартное SQL-предложение **WITH**, чтобы определить временное именованный результирующий набор. Например, предложение **WITH** используется в этом запросе, чтобы выполнить преобразование в два этапа:
 
	WITH step1 AS ( 
		SELECT Avg(Reading) as avr 
		FROM temperatureInput1 
		GROUP BY Building, TumblingWindow(hour, 1) 
	) 

	SELECT Avg(avr) AS campus_Avg 
	FROM step1 
	GROUP BY TumblingWindow (day, 1) 

Дополнительные сведения о языке запросов см. в [справочнике по языку запросов Azure Stream Analytics](stream.analytics.query.language.reference).

## Выходные данные
В назначение выходных данных записываются результаты выполнения задания Stream Analytics. Результаты постоянно записываются в назначение выходных данных по мере обработки заданием входных событий. Поддерживаются следующие назначения выходных данных.

- Концентраторы событий Azure. Если нужно объединить несколько потоковых конвейеров (например, для ответной выдачи команд устройствам), выберите концентратор событий в качестве назначения выходных данных.
- Хранилище больших двоичных объектов Azure. Используйте хранилище больших двоичных объектов для долгосрочного архивирования выходных данных или хранения данных для последующей обработки.
- Хранилище таблиц Azure. Оно представляет собой хранилище структурированных данных с меньшим числом ограничений для схемы. В одной таблице Azure можно хранить сущности разных типов, использующие различные схемы. Хранилище таблиц Azure можно использовать для постоянного хранения данных и эффективного их извлечения. Дополнительные сведения см. в статьях [Введение в хранилище Azure](storage.introduction.md) и [Проектирование стратегии масштабируемого секционирования для хранилища таблиц Azure](https://msdn.microsoft.com/library/azure/hh508997.aspx).
- База данных SQL Azure. Это назначение выходных данных подходит для реляционных данных или приложений, которые зависят от содержимого, размещенного в базе данных.


## Задания масштабирования

Задание Stream Analytics можно масштабировать, настраивая единицы потоковой передачи, определяющие объем вычислительной мощности, получаемый заданием. Каждая единица потоковой передачи соответствует пропускной способности около 1 МБ/с. Каждая подписка предоставляет квоту в 12 единиц потоковой передачи на регион, распределяемую между заданиями в этом регионе.

Дополнительные сведения см. в статье [Масштабирование заданий в службе Azure Stream Analytics](stream.analytics.scale.jobs).


## Мониторинг заданий и устранение неполадок в них

### Учетная запись хранения регионального мониторинга

Чтобы отслеживать задания в службе Stream Analytics, вам необходимо назначить учетную запись хранения Azure для мониторинга данных в каждом регионе, содержащем задания Stream Analytics. Этот процесс производится при создании задания.

### Метрики
Для мониторинга использования и производительности заданий Stream Analytics доступны следующие метрики.

- Ошибки. Количество сообщений об ошибках, вызванных заданием Stream Analytics.
- Входные события. Объем данных, полученных заданием Stream Analytics, измеренный 
- счетчиком событий.
- Выходные события. Объем данных, отправляемых заданием Stream Analytics в выходной источник, измеренный счетчиком событий.
- Неупорядоченные события. Количество событий, полученных в неактуальное время, которые были удалены или получили откорректированную метку времени в соответствии с политикой в отношении неупорядоченных событий.
- Ошибки преобразования данных. Количество ошибок преобразования данных, вызванных заданием Stream Analytics.

### Журналы операций
Отладку и диагностику заданий Stream Analytics лучше всего осуществлять с помощью журналов операций Azure. Журналы операций можно открыть в разделе **Службы управления** портала. Чтобы просмотреть журналы своего задания, задайте для параметра **Тип службы** значение **Stream Analytics**, а для параметра **Имя службы** — имя задания.


## Управление заданиями 

### Запуск и остановка заданий
При запуске задания вам будет предложено указать значение параметра **Начало вывода**, который определяет, когда это задание начнет выдавать результирующие выходные данные. Если связанный запрос включает в себя окно, задание начнет получать входные данные из входных источников ввода в начале требуемого периода окна, чтобы подготовить первое выходное событие к указанному времени. Доступны три параметра: **Время запуска задания**, **Пользовательский** и **Время последней остановки**. По умолчанию задается параметр **Время запуска задания**. Если задание временно остановлено, для параметра **Время последней остановки** рекомендуется установить значение **Начало вывода**, чтобы возобновить выполнение задания с момента последнего вывода и избежать потери данных. При выборе параметра **Пользовательский** вам необходимо указать дату и время. Этот параметр полезен для указания объема используемых исторических данных из входных источников и для приема данных в конкретное время.

### Настройка заданий
Вы сможете настроить следующие параметры верхнего уровня для задания Stream Analytics.

- **Начало вывода**. Используйте этот параметр, чтобы указать, когда это задание начнет выдавать результирующие выходные данные. Если связанный запрос включает в себя окно, задание начнет получать входные данные из входных источников в начале требуемого периода, чтобы подготовить первое выходное событие к указанному времени. Доступны два параметра: **Время запуска задания** и **Пользовательский**. По умолчанию задается параметр **Время запуска задания**. При выборе параметра **Пользовательский** вам необходимо указать дату и время. Этот параметр полезен для указания объема используемых данных из входных источников и для приема данных из конкретного времени: например, с момента последней остановки задания. 
- **Неупорядоченная политика**: параметры для обработки событий, которые не поступают в задание Stream Analytics последовательно. Вы можете указать временной порог, в пределах которого события будут переупорядочены, задав диапазон отклонений, а также указать действие, применяемое к событиям за пределами этого диапазона: **Удалить** или **Изменить**. Если выбрано действие **Удалить**, будут удалены все события, полученные в неверном порядке. Если выбрано действие **Изменить**, будут внесены изменения в систему. Отношение метки времени неупорядоченных событий к метке времени последнего полученного упорядоченного события. 
- **Политика в отношении позднего прибытия**. Если данные считываются из входных источников, включающих несколько секций, и при этом в одной или нескольких секциях наблюдается задержка или нет данных, заданию потоковой передачи необходимо решить, как устранить эту проблему для дальнейшей поддержки потока событий в системе. Входной параметр "Максимально допустимая задержка прибытия" управляет этими действиями и по умолчанию ожидает поступления данных в течение неопределенного времени. Это значит, что метки времени событий не будут изменены, однако при этом события будут проходить с учетом скорости потока в самой медленной входной секции, останавливаясь в случае отсутствия данных в одной или нескольких входных секциях. Это полезно, если данные равномерно распределяются между входными секциями и крайне важно обеспечить согласованность событий по времени. Пользователь также может решить ожидать данные только в течение ограниченного времени. В этом случае параметр "Максимально допустимая задержка прибытия" определяет задержку, после которого задание будет продолжено. При этом запаздывающие входные секции пропускаются, а задание реагирует на события в соответствии со значением параметра "Действие с поздними событиями", удаляя события или изменяя их метки времени, если данные поступают позже. Это полезно, если задержка имеет первостепенное значение и при этом допускается смещение меток времени, однако входные данные могут распределяться неравномерно.
- **Локаль**. Этот параметр используется для указания приоритета интернационализации для задания Stream Analytics. Хотя метки времени данных не зависят от локали, данные параметры влияют на анализ, сравнение и сортировку данных заданием. В предварительной версии поддерживается только локаль **en-US**.

### Состояние

Состояние заданий Stream Analytics можно проверить на портале Azure. Выполняющиеся задания могут находиться в одном из трех состояний: **Неактивно**, **Обработка** или **Деградация**. Ниже приведены определения всех этих состояний.

- **Неактивно**. С момента создания задания или в течение 2 последних минут им не получено никаких входных данных. Если задание находится в состоянии **Неактивно** в течение длительного периода времени, скорее всего, входные данные есть, но при этом нет необработанных сведений для обработки.
- **Обработка**. Задание Stream Analytics успешно обработало ненулевое количество отфильтрованных событий ввода. Если задание находится в состоянии **Обработка** и не выдает выходные данные, то, скорее всего, это обусловлено большим временным окном, выделенным на обработку данных, или сложностью логических операций в запросе.
- **Деградация**. Это состояние указывает, что при выполнении задания Stream Analytics возникла одна из следующих ошибок: ошибка связи с входными или выходными данными, ошибка запроса или ошибка во время выполнения повторной попытки. Чтобы определить тип ошибок, которые возникли при выполнении задания, просмотрите журналы операций.


## Получение поддержки
За дополнительной помощью обращайтесь на наш [форум Azure Stream Analytics](https://social.msdn.microsoft.com/Forums/en-US/home?forum=AzureStreamAnalytics).


## Дальнейшие действия

Теперь, когда вы ознакомились с основными понятиями Stream Analytics, прочитайте указанные ниже статьи.

- [Введение в Azure Stream Analytics](stream-analytics-introduction.md)
- [Приступая к работе с Azure Stream Analytics](stream-analytics-get-started.md)
- [Масштабирование заданий в службе Azure Stream Analytics](stream-analytics-scale-jobs.md)
- [Справочник по языку запросов Azure Stream Analytics](https://msdn.microsoft.com/library/azure/dn834998.aspx)
- [Справочник по API-интерфейсу REST управления Stream Analytics](https://msdn.microsoft.com/library/azure/dn835031.aspx)





<!--Link references-->
[azure.blob.storage]: http://azure.microsoft.com/documentation/services/storage/
[azure.blob.storage.use]: http://azure.microsoft.com/documentation/articles/storage-dotnet-how-to-use-blobs/

[azure.event.hubs]: http://azure.microsoft.com/services/event-hubs/
[azure.event.hubs.developer.guide]: http://msdn.microsoft.com/library/azure/dn789972.aspx

[stream.analytics.query.language.reference]: http://go.microsoft.com/fwlink/?LinkID=513299
[stream.analytics.forum]: http://go.microsoft.com/fwlink/?LinkId=512151

[stream.analytics.introduction]: stream-analytics-introduction.md
[stream.analytics.get.started]: stream-analytics-get-started.md
[stream.analytics.developer.guide]: stream-analytics-developer-guide.md
[stream.analytics.scale.jobs]: stream-analytics-scale-jobs.md
[stream.analytics.limitations]: stream-analytics-limitations.md
[stream.analytics.query.language.reference]: http://go.microsoft.com/fwlink/?LinkID=513299
[stream.analytics.rest.api.reference]: http://go.microsoft.com/fwlink/?LinkId=517301

<!---HONumber=58-->