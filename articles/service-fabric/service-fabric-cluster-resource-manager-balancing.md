<properties
   pageTitle="Балансирование нагрузки в кластере с помощью диспетчера кластерных ресурсов Azure Service Fabric | Microsoft Azure"
   description="Общие сведения о распределении нагрузки в кластере с помощью диспетчера кластерных ресурсов Azure Service Fabric."
   services="service-fabric"
   documentationCenter=".net"
   authors="masnider"
   manager="timlt"
   editor=""/>

.<tags
   ms.service="Service-Fabric"
   ms.devlang="dotnet"
   ms.topic="article"
   ms.tgt_pltfrm="NA"
   ms.workload="NA"
   ms.date="05/20/2016"
   ms.author="masnider"/>

# Балансировка кластера Service Fabric
Диспетчер кластерных ресурсов Service Fabric позволяет формировать отчеты о динамической нагрузке, реагировать на изменения в кластере и создавать планы балансировки, но когда именно он выполняет все эти действия? Из-за чего запускается перераспределение нагрузки в кластере, если размещение служб происходит в момент их создания с учетом нагрузки по умолчанию? С этим связано несколько элементов управления.

Первый набор элементов управления контролирует балансировку таймеров, которые определяют, как часто диспетчер кластерных ресурсов должен проверять состояние кластера и предпринимать необходимые меры. Такие таймеры относятся к различным этапам уже выполняемой работы, а именно:

1.	Размещение: на этом этапе происходит размещение отсутствующих метрик с отслеживанием состояния или без. Сюда входят как новые службы, так и работа с репликами или экземплярами, которые завершились сбоем и нуждаются в восстановлении. Тут же выполняется удаление реплик и экземпляров.
2.	Проверка ограничений: на этом этапе проверяются и устраняются нарушения различных ограничений (правил) размещения в системе. Примеры правил — это контроль за тем, чтобы не превышалась емкость узлов и соблюдались ограничения на размещение службы (подробнее этот момент мы обсудим позднее).
3.	Балансировка: на этом этапе проверяется необходимость упреждающего перераспределения нагрузки с учетом требуемого уровня нагрузки для различных метрик, а также поиск оптимально сбалансированной структуры кластера.

## Настройка этапов и таймеров диспетчера кластерных ресурсов
Периодичность выполнения каждого из этих этапов регулируется отдельным таймером. Например, размещение новой рабочей нагрузки служб в кластере может выполняться раз в час (что позволит группировать такие задачи), а регулярная проверка балансировки — через определенное количество секунд. Когда какой-либо таймер срабатывает, устанавливается флаг, сообщающий о необходимости выполнения соответствующих обязанностей диспетчера ресурсов. Необходимые действия выполняются во время следующей общей проверки конечного автомата (вот почему такие конфигурации определяются как "минимальные интервалы"). По умолчанию диспетчер ресурсов сканирует состояние кластера и применяет обновления каждую 1/10 секунды, устанавливает флаги размещения и проверки ограничений каждую секунду, а флаг балансировки — каждые 5 секунд.

ClusterManifest.xml:

``` xml
        <Section Name="PlacementAndLoadBalancing">
            <Parameter Name="PLBRefreshGap" Value="0.1" />
            <Parameter Name="MinPlacementInterval" Value="1.0" />
            <Parameter Name="MinConstraintCheckInterval" Value="1.0" />
            <Parameter Name="MinLoadBalancingInterval" Value="5.0" />
        </Section>
```

В настоящее время все эти действия выполняются по очереди. В результате прежде чем, например, перейти к балансировке кластера, мы выполняем запросы о создании новых реплик. Как видно по интервалам времени, заданным по умолчанию, сканирование и проверку необходимости каких-либо мер можно выполнять очень часто. В этом случае набор изменений в конце цикла будет меньше: нам не нужно несколько часов сканировать изменения в кластере и пытаться исправить все одновременно; мы делаем это по мере необходимости, иногда прибегая к пакетной обработке, если изменения происходят одновременно. Таким образом, диспетчер ресурсов Service Fabric чутко реагирует на все, что происходит в кластере.

Кроме того, диспетчеру кластерных ресурсов необходимо знать, в каком случае кластер следует считать несбалансированным и какие реплики нужно переместить, чтобы исправить ситуацию. Для этого используются два основных элемента конфигурации: *пороговые значения балансировки* и *пороговые значения активности*.

## Пороговые значения балансировки
Пороговое значение балансировки — это основной элемент управления для запуска упреждающего перераспределения нагрузки. Пороговое значение балансировки определяет, насколько несбалансированным должен быть кластер с точки зрения определенной метрики для того, чтобы платформа Resource Manager признала его таковым и запустила балансировку. Пороговые значения балансировки определяются в определении кластера на основе метрик:

ClusterManifest.xml

``` xml
    <Section Name="MetricBalancingThresholds">
      <Parameter Name="MetricName1" Value="2"/>
      <Parameter Name="MetricName2" Value="3.5"/>
    </Section>
```

Пороговое значение балансировки для метрики представляет собой коэффициент. Если объем нагрузки на наиболее загруженном узле, деленный на объем нагрузки на наименее загруженном узле, превышает это значение, кластер считается несбалансированным и при следующем запуске узла состояния диспетчер ресурсов инициирует балансировку.

![Пример порогового значения балансировки][Image1]

В этом простом примере каждая служба использует только одну единицу определенной метрики. В верхнем примере максимальная нагрузка на узле составляет 5, а минимальная — 2. Предположим, что пороговое значение балансировки для метрики — 3. Таким образом, в приведенном примере кластер считается сбалансированным, и балансировка запущена не будет (так как коэффициент для кластера 5/2 = 2,5, что меньше указанного порогового значения балансировки, равного 3).

В примере ниже максимальная нагрузка на узле составляет 10, а минимальная — 2 (значит, коэффициент будет равен 5), а значит, желаемое пороговое значение балансировки, равное 3, превышено. В итоге при следующем срабатывании таймера произойдет полная балансировка, и почти наверняка нагрузка будет распределена на узел Node3. Обратите внимание: поскольку мы не используем "жадный" подход, часть нагрузки может осесть на узле Node2, что позволит минимизировать разницу между узлами, однако основная часть нагрузки ляжет на узел Node3.

![Действия в примере порогового значения балансировки][Image2]

Обратите внимание, что выход за нижнее пороговое значение балансировки не является основной задачей. Пороговые значения балансировки выступают обычными *триггерами*, которые сообщают Service Fabric Cluster Resource Manager о том, что нужно проверить кластер на предмет возможных улучшений.

## Пороговые значения активности
Иногда узлы могут быть относительно несбалансированными, даже если общий объем нагрузки в кластере небольшой. Это может быть связано с временем суток или с тем, что кластер только что создан и проходит начальную загрузку. В любом случае, никто не захочет тратить ценное время на балансировку, поскольку выгода будет минимальной — сетевые и вычислительные ресурсы будут потрачены на простую перетасовку. В диспетчере Resource Manager существует еще один элемент управления, который называется пороговым значением активности и который позволяет указать абсолютную нижнюю границу активности — когда на узле выполняется хотя бы этот объем нагрузки, балансировка не запускается, даже если пороговое значение балансировки превышено. В качестве примера предположим, что у нас есть отчеты с указанным ниже объемом использования ресурсов на узлах. Допустим, пороговое значение балансировки по-прежнему равно 3, но теперь у нас есть пороговое значение активности, которое составляет 1536. В первом случае согласно пороговому значению балансировки кластер не сбалансирован, однако ни один узел не превышает пороговое значение активности, поэтому все остается как есть. В нижнем примере узел Node1 намного превышает пороговое значение активности, поэтому балансировка выполняется.

![Пример порогового значения активности][Image3]

Как и пороговые значения балансировки, пороговые значения активности определяются в определении кластера на основе метрик:

ClusterManifest.xml

``` xml
    <Section Name="MetricActivityThresholds">
      <Parameter Name="Memory" Value="1536"/>
    </Section>
```

Обратите внимание, что пороговые значения балансировки и активности привязаны к метрике. Балансировка запускается только тогда, когда оба пороговых значения превышены по отношению к метрике. То есть если превышено пороговое значение балансировки для памяти и пороговое значение активности для ЦП, балансировка не запускается, если не превышены другие пороговые значения (пороговое значение балансировки для ЦП и пороговое значение активности для памяти).

## Одновременная балансировка служб
Следует отметить, что несбалансированность кластера определяется по общему состоянию кластера, а устраняется путем перемещения реплик или экземпляров отдельных служб. Звучит разумно, не правда ли? Если память накапливается в одном узле, в этом могут участвовать сразу несколько реплик или экземпляров, а значит, перемещать нужно все реплики или экземпляры, которые пользуются несбалансированной метрикой.

Время от времени клиенты по телефону или через форму связи сообщают нам о перемещении службы, которая не была несбалансированной. Почему это происходит, если все метрики службы сбалансированы, в то время как в системе существует другой дисбаланс? Давайте разберемся.

Возьмем для примера четыре службы: S1, S2, S3 и S4. Служба S1 отчитывается о метриках M1 и M2, S2 — о метриках M2 и M3, S3 — о метриках M3 и M4, а S4 — о некоторой метрике M99. Понимаете, к чему все это ведет? Возникает цепочка. С точки зрения диспетчера ресурсов мы имеем дело не с четырьмя независимыми службами, а с рядом связанных служб (S1, S2 и S3) и одной отдельной.

.![Одновременная балансировка служб][Image4]

Таким образом, дисбаланс в метрике M1 может вызвать перемещение реплик или экземпляров, относящихся к службе S3. Обычно эти перемещения довольно ограничены, но могут быть и более масштабны в зависимости от того, насколько несбалансирована метрика M1 и какие изменения потребуются в кластере для того, чтобы ее устранить. Кроме того, можно с уверенностью сказать, что дисбаланс в метриках M1, M2 и M3 не вызовет перемещения службы S4 — оно не имеет смысла, так как перемещение реплик или экземпляров службы S4 никак не повлияет на баланс метрик M1, M2 и M3.

При каждом запуске диспетчер ресурсов автоматически определяет связанные службы, поскольку службы могут добавляться и удаляться, а метрики — изменяться. Например, если между запусками балансировки конфигурацию службы S2 изменят, удалив из нее реплику S2, связь между службами S1 и S2 будет разорвана. В этом случае две группы служб превратятся в три:

![Одновременная балансировка служб][Image5]

## Дальнейшие действия
- Метрики показывают, как диспетчер кластерных ресурсов Service Fabric управляет потреблением и емкостью в кластере. Дополнительные сведения о метриках и их настройке см. в [этой статье](service-fabric-cluster-resource-manager-metrics.md).
- Стоимость перемещения — один из способов сообщить диспетчеру кластерных ресурсов, что некоторые службы перемещать затратнее, чем остальные. Чтобы больше узнать о стоимости перемещения, см. [эту статью](service-fabric-cluster-resource-manager-movement-cost.md).
- В диспетчере кластерных ресурсов имеется несколько регулировок, которые можно настроить, чтобы замедлить отток в кластере. Обычно они не требуется, но при необходимости узнать о регулировках можно [здесь](service-fabric-cluster-resource-manager-advanced-throttling.md).


[Image1]: ./media/service-fabric-cluster-resource-manager-balancing/cluster-resrouce-manager-balancing-thresholds.png
[Image2]: ./media/service-fabric-cluster-resource-manager-balancing/cluster-resource-manager-balancing-threshold-triggered-results.png
[Image3]: ./media/service-fabric-cluster-resource-manager-balancing/cluster-resource-manager-activity-thresholds.png
[Image4]: ./media/service-fabric-cluster-resource-manager-balancing/cluster-resource-manager-balancing-services-together1.png
[Image5]: ./media/service-fabric-cluster-resource-manager-balancing/cluster-resource-manager-balancing-services-together2.png

<!---HONumber=AcomDC_0810_2016-->