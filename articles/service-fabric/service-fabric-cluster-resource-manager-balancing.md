---
title: Балансирование нагрузки в кластере с помощью диспетчера кластерных ресурсов Azure Service Fabric | Microsoft Docs
description: Общие сведения о распределении нагрузки в кластере с помощью диспетчера кластерных ресурсов Azure Service Fabric.
services: service-fabric
documentationcenter: .net
author: masnider
manager: timlt
editor: ''

ms.service: Service-Fabric
ms.devlang: dotnet
ms.topic: article
ms.tgt_pltfrm: NA
ms.workload: NA
ms.date: 08/19/2016
ms.author: masnider

---
# Балансировка кластера Service Fabric
Диспетчер кластерных ресурсов Service Fabric позволяет формировать отчеты о динамической нагрузке, реагировать на изменения в кластере, корректировать нарушения ограничений и перераспределять кластер, если это необходимо. Но как часто выполняются эти действия и что их активирует? С этим связано несколько элементов управления.

Первый набор элементов управления балансировкой — это набор таймеров. Они определяют, как часто диспетчер кластерных ресурсов должен проверять состояние кластера и принимать необходимые меры. Существует три разных категории работ, каждая из которых имеет собственные таймеры. К ним относятся:

1. Размещение: на этом этапе происходит размещение отсутствующих метрик с отслеживанием состояния или без. Сюда входят как новые службы, так и работа с репликами с отслеживанием состояния или экземплярами без отслеживания состояния, которые породили сбой и нуждаются в повторном создании. Тут же выполняется удаление реплик и экземпляров.
2. Проверка ограничений: на этом этапе проверяются и устраняются нарушения различных ограничений (правил) размещения в системе. Примеры правил — это контроль за тем, чтобы не превышалась емкость узлов и соблюдались ограничения на размещение службы (подробнее этот момент мы обсудим позднее).
3. Балансировка: на этом этапе проверяется необходимость упреждающего перераспределения нагрузки с учетом требуемого уровня нагрузки для различных метрик, а также поиск оптимально сбалансированной структуры кластера.

## Настройка этапов и таймеров диспетчера кластерных ресурсов
Каждым из типов исправлений, которые может внести диспетчер кластерных ресурсов, управляет соответствующий таймер, который определяет частоту исправлений. Например, размещение новой рабочей нагрузки служб в кластере может выполняться только раз в час (что позволит группировать такие задачи), а регулярная проверка балансировки — через определенное количество секунд. При каждом срабатывании таймера задача добавляется в расписание. По умолчанию диспетчер кластерных ресурсов сканирует состояние кластера и применяет обновления (объединяя все изменения, произошедшие с момента последнего сканирования; например, он может выявить, что узел стал неработоспособен) каждую 1/10 секунды, устанавливает флаги проверки размещения и ограничений каждую секунду, а флаг балансировки — каждые 5 секунд.

ClusterManifest.xml:

``` xml
        <Section Name="PlacementAndLoadBalancing">
            <Parameter Name="PLBRefreshGap" Value="0.1" />
            <Parameter Name="MinPlacementInterval" Value="1.0" />
            <Parameter Name="MinConstraintCheckInterval" Value="1.0" />
            <Parameter Name="MinLoadBalancingInterval" Value="5.0" />
        </Section>
```

Сегодня мы выполняем только одно из этих действий одновременно, т. е. выполняем их последовательно (вот почему мы называем эти конфигурации "минимальными интервалами"). В результате прежде чем, например, перейти к балансировке кластера, мы выполняем ожидающие запросы на создание новых реплик. Как видно по интервалам времени, заданным по умолчанию, сканирование и проверку необходимости каких-либо мер можно выполнять очень часто. В этом случае набор изменений в конце цикла будет меньше: нам не нужно несколько часов сканировать изменения в кластере и пытаться исправить их все одновременно. Мы делаем это по мере необходимости, иногда прибегая к пакетной обработке, если изменения происходят одновременно. Таким образом, диспетчер ресурсов Service Fabric чутко реагирует на все, что происходит в кластере.

Хотя большинство этих задач просты (если нарушены ограничения, то их следует исправить, если должны быть созданы службы, то их следует создать), диспетчеру кластерных ресурсов нужны некоторые дополнительные сведения, чтобы выявить дисбаланс кластера. Для этого используются два других элемента конфигурации: *пороговые значения балансировки* и *пороговые значения активности*.

## Пороговые значения балансировки
Пороговое значение балансировки является основным элементом управления для активации упреждающего перераспределения (помните, что таймер просто задает периодичность проверок, выполняемых диспетчером кластерных ресурсов, т. е. его наличие не значит, что что-то произойдет). Пороговое значение балансировки определяет, насколько несбалансированным должен быть кластер по определенной метрике, чтобы диспетчер кластерных ресурсов признал его таковым и активировал балансировку.

Пороговые значения балансировки задаются в определении кластера на основе метрик. Дополнительные сведения о метриках см. в [этой статье](service-fabric-cluster-resource-manager-metrics.md).

ClusterManifest.xml

``` xml
    <Section Name="MetricBalancingThresholds">
      <Parameter Name="MetricName1" Value="2"/>
      <Parameter Name="MetricName2" Value="3.5"/>
    </Section>
```

Пороговое значение балансировки для метрики представляет собой коэффициент. Если результат деления объема нагрузки на наиболее загруженном узле на объем нагрузки на наименее загруженном узле превышает это число, то кластер считается несбалансированным, и при следующей проверке диспетчер кластерных ресурсов активирует балансировку (по умолчанию она выполняется каждые 5 минут; частотой управляет параметр MinLoadBalancingInterval, как показано выше).

![Пример порогового значения балансировки][Image1]

В этом простом примере каждая служба использует одну единицу определенной метрики. В верхнем примере максимальная нагрузка на узле составляет 5, а минимальная — 2. Предположим, что пороговое значение балансировки для метрики — 3. Поэтому в приведенном примере кластер считается сбалансированным, и при проверке диспетчером кластерных ресурсов балансировка активирована не будет (так как коэффициент в кластере 5 / 2 = 2,5, что меньше указанного порогового значения балансировки, равного 3).

В примере ниже максимальная нагрузка на узле составляет 10, а минимальная — 2 (значит, коэффициент будет равен 5). Это значит, что желаемое пороговое значение балансировки для этой метрики, равное 3, превышено. В результате при следующем срабатывании таймера будет запланирован запуск глобального перераспределения кластера. Обратите внимание, что активация поиска балансировки вовсе не означает, что что-либо будет перемещено. Иногда кластер несбалансирован, но улучшить ситуацию невозможно. Однако в нашей ситуации (по крайней мере по умолчанию) часть нагрузки почти наверняка будет распределена на Node3. Обратите внимание: так как мы не используем "жадный" подход, часть нагрузки также может быть распределена на Node2, что позволит минимизировать общую разницу между узлами, однако основная часть нагрузки перейдет на Node3.

![Действия в примере порогового значения балансировки][Image2]

Обратите внимание, что выход за нижнее пороговое значение балансировки не является основной целью. Пороговые значения балансировки — это просто *триггеры*, которые сообщают диспетчеру кластерных ресурсов о том, что нужно проверить кластер на предмет возможных улучшений.

## Пороговые значения активности
Иногда узлы могут быть относительно несбалансированными, даже если *общий* объем нагрузки в кластере небольшой. Это может быть связано с временем суток или с тем, что кластер только что создан и проходит начальную загрузку. В любом случае, вы можете не захотеть тратить время на балансировку кластера, так как польза будет минимальной — сетевые и вычислительные ресурсы будут потрачены на простую перетасовку нагрузки, не влияющую на результат. Так как этого нужно избегать, в диспетчере кластерных ресурсов существует еще один элемент управления, который называется пороговым значением активности. Он позволяет указать абсолютную нижнюю границу активности — когда на узле выполняется хотя бы этот объем нагрузки, балансировка не запускается, даже если пороговое значение балансировки превышено.

В качестве примера предположим, что у нас есть отчеты с указанным ниже объемом использования ресурсов на узлах. Допустим, пороговое значение балансировки для этой метрики по-прежнему равно 3, но теперь у нас есть пороговое значение активности, которое составляет 1536. В первом случае согласно пороговому значению балансировки кластер несбалансирован, однако ни один узел не превышает пороговое значение активности, поэтому все остается как есть. В примере ниже Node1 заметно превысил пороговое значения активности, поэтому будет запущена балансировка (так как превышены пороговые значения балансировки и активности для метрики).

![Пример порогового значения активности][Image3]

Как и пороговые значения балансировки, пороговые значения активности определяются в определении кластера на основе метрик:

ClusterManifest.xml

``` xml
    <Section Name="MetricActivityThresholds">
      <Parameter Name="Memory" Value="1536"/>
    </Section>
```

Обратите внимание, что пороговые значения балансировки и активности привязаны к метрике. Балансировка запускается только тогда, когда оба пороговых значения превышены по отношению к метрике. То есть если превышено пороговое значение балансировки для памяти и пороговое значение активности для ЦП, балансировка не запускается, если не превышены другие пороговые значения (пороговое значение балансировки для ЦП и пороговое значение активности для памяти).

## Одновременная балансировка служб
Следует отметить, что несбалансированность кластера определяется по общему состоянию кластера, а устраняется путем перемещения реплик или экземпляров отдельных служб. Звучит разумно, не правда ли? Если память накапливается в одном узле, в этом могут участвовать сразу несколько реплик или экземпляров. Это значит, что может потребоваться переместить все реплики с отслеживанием состояния или экземпляры без отслеживания состояния, которые пользуются несбалансированной метрикой.

Время от времени клиенты по телефону или через форму связи сообщают нам о перемещении службы, которая не была несбалансированной. Почему служба может перемещаться между узлами, даже если все метрики этой службы отлично сбалансированы, в то время как существует другой дисбаланс? Давайте разберемся.

Возьмем для примера четыре службы: S1, S2, S3 и S4. S1 использует метрики M1 и M2, S2 — метрики M2 и M3, S3 — метрики M3 и M4, а S4 — какую-то метрику M99. Понимаете, к чему все это ведет? Возникает цепочка. С точки зрения диспетчера кластерных ресурсов мы имеем дело не с четырьмя независимыми службами, а с набором связанных служб (S1, S2 и S3) и одной отдельной службой.

![Одновременная балансировка служб][Image4]

Таким образом, дисбаланс в метрике M1 может вызвать перемещение реплик или экземпляров, относящихся к службе S3. Обычно эти перемещения довольно ограничены, но могут быть и более масштабны в зависимости от того, насколько несбалансирована метрика M1 и какие изменения потребуются в кластере для того, чтобы ее устранить. Кроме того, можно с уверенностью сказать, что дисбаланс в метриках M1, M2 и M3 не вызовет перемещения службы S4 — оно не имеет смысла, так как перемещение реплик или экземпляров службы S4 никак не повлияет на баланс метрик M1, M2 и M3.

При каждом запуске диспетчер кластерных ресурсов автоматически определяет, какие службы связаны, так как службы могут добавляться и удаляться, а конфигурации их метрик — изменяться. Например, между запусками балансировки конфигурация службы S2 может быть изменена для удаления из нее метрики M2. Тогда связь между службами S1 и S2 будет разорвана. В этом случае две группы служб превратятся в три:

![Одновременная балансировка служб][Image5]

## Дальнейшие действия
* Метрики показывают, как диспетчер кластерных ресурсов Service Fabric управляет потреблением и емкостью в кластере. Дополнительные сведения о метриках и их настройке см. в [этой статье](service-fabric-cluster-resource-manager-metrics.md).
* Стоимость перемещения — один из способов сообщить диспетчеру кластерных ресурсов, что некоторые службы перемещать затратнее, чем остальные. Чтобы больше узнать о стоимости перемещения, см. [эту статью](service-fabric-cluster-resource-manager-movement-cost.md).
* В диспетчере кластерных ресурсов имеется несколько регулировок, которые можно настроить, чтобы замедлить отток в кластере. Обычно они не требуется, но при необходимости узнать о регулировках можно [здесь](service-fabric-cluster-resource-manager-advanced-throttling.md).

[Image1]: ./media/service-fabric-cluster-resource-manager-balancing/cluster-resrouce-manager-balancing-thresholds.png
[Image2]: ./media/service-fabric-cluster-resource-manager-balancing/cluster-resource-manager-balancing-threshold-triggered-results.png
[Image3]: ./media/service-fabric-cluster-resource-manager-balancing/cluster-resource-manager-activity-thresholds.png
[Image4]: ./media/service-fabric-cluster-resource-manager-balancing/cluster-resource-manager-balancing-services-together1.png
[Image5]: ./media/service-fabric-cluster-resource-manager-balancing/cluster-resource-manager-balancing-services-together2.png

<!---HONumber=AcomDC_0824_2016-->