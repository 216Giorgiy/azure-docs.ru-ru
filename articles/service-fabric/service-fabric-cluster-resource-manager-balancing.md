<properties
   pageTitle="Распределение нагрузки в кластере с помощью диспетчера кластерных ресурсов Azure Service Fabric"
   description="Общие сведения о распределении нагрузки в кластере с помощью диспетчера кластерных ресурсов Azure Service Fabric."
   services="service-fabric"
   documentationCenter=".net"
   authors="masnider"
   manager="timlt"
   editor=""/>

<tags
   ms.service="Service-Fabric"
   ms.devlang="dotnet"
   ms.topic="article"
   ms.tgt_pltfrm="NA"
   ms.workload="NA"
   ms.date="03/03/2016"
   ms.author="masnider"/>

# Балансировка кластера Service Fabric
Диспетчер кластерных ресурсов Service Fabric позволяет формировать отчеты о динамической нагрузке, реагировать на изменения в кластере и создавать планы балансировки, но когда именно он выполняет все эти действия? Из-за чего запускается перераспределение нагрузки в кластере, если размещение служб происходит в момент их создания с учетом нагрузки по умолчанию? С этим связано несколько элементов управления.

Первый набор элементов управления контролирует балансировку таймеров, которые определяют, как часто диспетчер кластерных ресурсов должен проверять состояние кластера и предпринимать необходимые меры. Такие таймеры относятся к различным этапам уже выполняемой работы, а именно:

1.	Размещение: на этом этапе происходит размещение отсутствующих метрик с отслеживанием состояния или без. Сюда входят как новые службы, так и работа с репликами или экземплярами, которые завершились сбоем и нуждаются в восстановлении. Тут же выполняется удаление реплик и экземпляров.
2.	Проверка ограничений: на этом этапе проверяются и устраняются нарушения различных ограничений (правил) размещения в системе. Примеры правил — это контроль за тем, чтобы не превышалась емкость узлов и соблюдались ограничения на размещение службы (подробнее этот момент мы обсудим позднее).
3.	Балансировка: на этом этапе проверяется необходимость упреждающего перераспределения нагрузки с учетом требуемого уровня нагрузки для различных метрик, а также поиск оптимально сбалансированной структуры кластера.

Периодичность выполнения каждого из этих этапов регулируется отдельным таймером. Например, размещение новой рабочей нагрузки служб в кластере может выполняться раз в час (что позволит группировать такие задачи), а регулярная проверка балансировки — через определенное количество секунд. Когда какой-либо таймер срабатывает, устанавливается флаг, сообщающий о необходимости выполнения соответствующих обязанностей диспетчера ресурсов. Необходимые действия выполняются во время следующей общей проверки конечного автомата (вот почему такие конфигурации определяются как "минимальные интервалы"). По умолчанию диспетчер ресурсов сканирует состояние кластера и применяет обновления каждую 1/10 секунды, устанавливает флаги размещения и проверки ограничений каждую секунду, а флаг балансировки — каждые 5 секунд.

ClusterManifest.xml:

``` xml
        <Section Name="PlacementAndLoadBalancing">
            <Parameter Name="PLBRefreshGap" Value="0.1" />
            <Parameter Name="MinPlacementInterval" Value="1.0" />
            <Parameter Name="MinConstraintCheckInterval" Value="1.0" />
            <Parameter Name="MinLoadBalancingInterval" Value="5.0" />
        </Section>
```

В настоящее время все эти действия выполняются по очереди. В результате прежде чем, например, перейти к балансировке кластера, мы выполняем запросы о создании новых реплик. Как видно по интервалам времени, заданным по умолчанию, сканирование и проверку необходимости каких-либо мер можно выполнять очень часто — в этом случае набор изменений будет меньше (нам не приходится сканировать часы изменений в кластере и пытаться исправить все одновременно, мы делаем это по мере необходимости). Таким образом, диспетчер ресурсов Service Fabric чутко реагирует на все, что происходит в кластере.

Кроме того, диспетчеру кластерных ресурсов необходимо знать, в каком случае кластер следует считать несбалансированным и какие реплики нужно переместить, чтобы исправить ситуацию. Для этого используются два основных элемента конфигурации: пороговые значения балансировки и пороговые значения активности.

## Пороговые значения балансировки
Пороговое значение балансировки — это основной элемент управления для запуска упреждающего перераспределения нагрузки. Пороговое значение балансировки определяет, насколько несбалансированным должен быть кластер с точки зрения определенной метрики для того, чтобы диспетчер ресурсов признал его таковым и запустил балансировку. Пороговые значения балансировки определяются в манифесте кластера на основе метрик:

``` xml
    <Section Name="MetricBalancingThresholds">
      <Parameter Name="MetricName1" Value="2"/>
      <Parameter Name="MetricName2" Value="3.5"/>
    </Section>
```

Пороговое значение балансировки для метрики представляет собой коэффициент. Если объем нагрузки на наиболее загруженном узле, деленный на объем нагрузки на наименее загруженном узле, превышает это значение, кластер считается несбалансированным и при следующем запуске узла состояния диспетчер ресурсов инициирует балансировку.

![Пример порогового значения балансировки][Image1]

В этом простом примере каждая служба использует только одну единицу определенной метрики. В верхнем примере максимальная нагрузка на узле составляет 5, а минимальная — 2. Предположим, что пороговое значение балансировки для метрики — 3. Таким образом, в верхнем примере кластер считается сбалансированным, и балансировка не запускается. В нижнем примере максимальная нагрузка на узле составляет 10, а минимальная — 2, а значит, желаемое пороговое значение балансировки, равное 3, превышено. При следующем запуске диспетчер нагрузки, скорее всего, перебросит нагрузку на узел Node3. Обратите внимание на то, что поскольку мы не используем "жадный" подход, часть нагрузки может осесть на узле Node2, что позволит минимизировать разницу между узлами.

![Действия в примере порогового значения балансировки][Image2]

Помните, что пороговые значения балансировки служат только для запуска балансировки и уменьшение разницы в нагрузке ниже заданного ими предела не составляет конечную цель. Пороговые значения активности. Узлы могут быть относительно несбалансированными, даже если общий объем нагрузки в кластере небольшой. Это может быть связано с временем суток или с тем, что кластер только что создан и проходит начальную загрузку. В любом случае, никто не захочет тратить ценное время на балансировку, поскольку выгода будет минимальной — сетевые и вычислительные ресурсы будут потрачены на простую перетасовку. В диспетчере ресурсов существует еще один элемент управления, который называется пороговым значением активности и позволяет указать абсолютную нижнюю границу активности — когда на узле выполняется хотя бы этот объем нагрузки, балансировка не запускается, даже если пороговое значение балансировки превышено. В качестве примера предположим, что у нас есть отчеты с указанным ниже объемом использования ресурсов на узлах. Допустим, пороговое значение балансировки по-прежнему равно 3, но теперь у нас есть пороговое значение активности, которое составляет 1536. В первом случае согласно пороговому значению балансировки кластер не сбалансирован, однако ни один узел не превышает пороговое значение активности, поэтому все остается как есть. В нижнем примере узел Node1 намного превышает пороговое значение активности, поэтому балансировка выполняется.

![Пример порогового значения активности][Image3]

Как и пороговые значения балансировки, пороговые значения активности определяются в манифесте кластера на основе метрик:

``` xml
      <Section Name="MetricActivityThresholds">
        <Parameter Name="Memory" Value="1536"/>
      </Section>
```

## Одновременная балансировка служб
Следует отметить, что несбалансированность кластера определяется по общему состоянию кластера, а устраняется путем перемещения реплик или экземпляров отдельных служб. Звучит разумно, не правда ли? Если память накапливается в одном узле, в этом могут участвовать сразу несколько реплик или экземпляров, а значит, перемещать нужно все реплики или экземпляры, которые пользуются несбалансированной метрикой.

Время от времени клиенты по телефону или через форму связи сообщают нам о перемещении службы, которая не была несбалансированной. Почему это происходит, если все метрики службы сбалансированы, в то время как в системе существует другой дисбаланс? Давайте разберемся.

Возьмем для примера четыре службы: S1, S2, S3 и S4. Служба S1 отчитывается о метриках M1 и M2, S2 — о метриках M2 и M3, S3 — о метриках M3 и M4, а S4 — о некоторой метрике M99. Понимаете, к чему все это ведет? Возникает цепочка. С точки зрения диспетчера ресурсов мы имеем дело не с четырьмя независимыми службами, а с рядом связанных служб (S1, S2 и S3) и одной отдельной.

![Одновременная балансировка служб][Image4]

Таким образом, дисбаланс в метрике M1 может вызвать перемещение реплик или экземпляров, относящихся к службе S3. Обычно эти перемещения довольно ограничены, но могут быть и более масштабны в зависимости от того, насколько несбалансирована метрика M1 и какие изменения потребуются в кластере для того, чтобы ее устранить. Кроме того, можно с уверенностью сказать, что дисбаланс в метриках M1, M2 и M3 не вызовет перемещения службы S4 — оно не имеет смысла, так как перемещение реплик или экземпляров службы S4 никак не повлияет на баланс метрик M1, M2 и M3.

При каждом запуске диспетчер ресурсов автоматически определяет связанные службы, поскольку службы могут добавляться и удаляться, а метрики — изменяться. Например, если между запусками балансировки конфигурацию службы S2 изменят, удалив из нее реплику S2, связь между службами S1 и S2 будет разорвана. В этом случае две группы служб превратятся в три:

![Одновременная балансировка служб][Image5]

<!--Every topic should have next steps and links to the next logical set of content to keep the customer engaged-->
## Дальнейшие действия
- [Дополнительные сведения о метриках](service-fabric-cluster-resource-manager-metrics.md)
- [Дополнительные сведения о регулировании диспетчера ресурсов](service-fabric-cluster-resource-manager-advanced-throttling.md)
- [Дополнительные сведения о стоимости перемещения служб](service-fabric-cluster-resource-manager-movement-cost.md)


[Image1]: ./media/service-fabric-cluster-resource-manager-balancing/cluster-resrouce-manager-balancing-thresholds.png
[Image2]: ./media/service-fabric-cluster-resource-manager-balancing/cluster-resource-manager-balancing-threshold-triggered-results.png
[Image3]: ./media/service-fabric-cluster-resource-manager-balancing/cluster-resource-manager-activity-thresholds.png
[Image4]: ./media/service-fabric-cluster-resource-manager-balancing/cluster-resource-manager-balancing-services-together1.png
[Image5]: ./media/service-fabric-cluster-resource-manager-balancing/cluster-resource-manager-balancing-services-together2.png

<!---HONumber=AcomDC_0309_2016-->