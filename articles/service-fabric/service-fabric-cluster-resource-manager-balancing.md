---
title: "Балансирование нагрузки кластера Azure Service Fabric | Документация Майкрософт"
description: "Общие сведения о распределении нагрузки в кластере с помощью диспетчера кластерных ресурсов Azure Service Fabric."
services: service-fabric
documentationcenter: .net
author: masnider
manager: timlt
editor: 
ms.assetid: 030b1465-6616-4c0b-8bc7-24ed47d054c0
ms.service: Service-Fabric
ms.devlang: dotnet
ms.topic: article
ms.tgt_pltfrm: NA
ms.workload: NA
ms.date: 01/05/2017
ms.author: masnider
translationtype: Human Translation
ms.sourcegitcommit: bb27d279396aa7b670187560cebe2ed074576bad
ms.openlocfilehash: ee77a01b2bb17ab70099e891e00255d1cec676f6


---
# <a name="balancing-your-service-fabric-cluster"></a>Балансировка кластера Service Fabric
Служба Cluster Resource Manager в Service Fabric поддерживает динамическое изменение нагрузки, реагируя на добавление или удаление узлов или служб, корректируя нарушения ограничений и перераспределяя нагрузку в кластере. Но как часто выполняются эти действия и что их активирует?

Первый набор элементов управления балансировкой — это набор таймеров. Они определяют, как часто диспетчер кластерных ресурсов должен проверять состояние кластера и принимать необходимые меры. Существует три разных категории работ, каждая из которых имеет собственные таймеры. К ним относятся:

1. Размещение: на этом этапе происходит размещение отсутствующих метрик с отслеживанием состояния или без. Сюда входят как новые службы, так и работа с репликами с отслеживанием состояния или экземплярами без отслеживания состояния, которые вызвали сбой. Тут выполняется удаление реплик и экземпляров.
2. Проверка ограничений: на этом этапе проверяются и устраняются нарушения различных ограничений (правил) размещения в системе. Примеры правил — это средства, предотвращающие избыточное использование ресурсов узлов, обеспечивающие соблюдение ограничений на размещение службы и т. п.
3. Балансировка: на этом этапе проверяется необходимость упреждающего перераспределения нагрузки с учетом требуемого уровня нагрузки для различных метрик. Если это необходимо, предпринимается попытка найти более сбалансированную схему упорядочения в кластере.

## <a name="configuring-cluster-resource-manager-steps-and-timers"></a>Настройка этапов и таймеров диспетчера кластерных ресурсов
Каждым из типов исправлений, которые может внести Cluster Resource Manager, управляет соответствующий таймер, который определяет частоту их применения. При каждом срабатывании таймера задача добавляется в расписание. По умолчанию диспетчер ресурсов:

* проверяет состояние и применяет обновления (например, записывает, что узел не работает) каждые 1/10 секунды;
* устанавливает флаги проверки расположения и ограничений каждую секунду;
* устанавливает флаг балансировки каждые пять секунд.

Это отражается в следующих сведениях о конфигурации:

ClusterManifest.xml:

``` xml
        <Section Name="PlacementAndLoadBalancing">
            <Parameter Name="PLBRefreshGap" Value="0.1" />
            <Parameter Name="MinPlacementInterval" Value="1.0" />
            <Parameter Name="MinConstraintCheckInterval" Value="1.0" />
            <Parameter Name="MinLoadBalancingInterval" Value="5.0" />
        </Section>
```

Для автономных развернутых служб используется ClusterConfig.json, а для размещенных в Azure кластеров — Template.json:

```json
"fabricSettings": [
  {
    "name": "PlacementAndLoadBalancing",
    "parameters": [
      {
          "name": "PLBRefreshGap",
          "value": "0.10"
      },
      {
          "name": "MinPlacementInterval",
          "value": "1.0"
      },
      {
          "name": "MinLoadBalancingInterval",
          "value": "5.0"
      }
    ]
  }
]
```

На сегодняшний день Cluster Resource Manager выполняет эти действия по очереди (вот почему мы называем эти таймеры "минимальными интервалами"). Например, Cluster Resource Manager обрабатывает ожидающие запросы, чтобы создать службы перед балансировкой кластера. Заданный по умолчанию интервал времени свидетельствует о том, что Cluster Resource Manager часто ищет и проверяет все необходимые элементы, поэтому набор изменений, внесенных в конце каждого этапа, обычно незначителен. Благодаря такой высокой частоте применения незначительных изменений Cluster Resource Manager чутко реагирует на то, что происходит в кластере. Таймеры по умолчанию обеспечивают своего рода пакетную обработку, поскольку обычно множество событий одного типа происходят одновременно. По умолчанию Cluster Resource Manager ищет изменения в кластере, внесенные на протяжении нескольких часов, и не пытается обработать все изменения за один раз. Такой подход может приводить к резкому увеличению оттока.

Чтобы выявить дисбаланс кластера, Cluster Resource Manager необходимы также некоторые дополнительные сведения. Для этого используются два других элемента конфигурации: *пороговые значения балансировки* и *пороговые значения активности*.

## <a name="balancing-thresholds"></a>Пороговые значения балансировки
Пороговое значение балансировки — это основной элемент управления для запуска упреждающего перераспределения нагрузки. Таймер MinLoadBalancingInterval просто задает периодичность проверок, выполняемых Cluster Resource Manager. Его наличие не означает, что что-то произойдет. Пороговое значение балансировки определяет, насколько несбалансированным должен быть кластер по определенной метрике, чтобы диспетчер кластерных ресурсов признал его таковым и активировал балансировку.

Пороговые значения балансировки задаются в определении кластера на основе метрик. Дополнительные сведения о метриках см. в [этой статье](service-fabric-cluster-resource-manager-metrics.md).

ClusterManifest.xml

```xml
    <Section Name="MetricBalancingThresholds">
      <Parameter Name="MetricName1" Value="2"/>
      <Parameter Name="MetricName2" Value="3.5"/>
    </Section>
```

Для автономных развернутых служб используется ClusterConfig.json, а для размещенных в Azure кластеров — Template.json:

```json
"fabricSettings": [
  {
    "name": "MetricBalancingThresholds",
    "parameters": [
      {
          "name": "MetricName1",
          "value": "2"
      },
      {
          "name": "MetricName2",
          "value": "3.5"
      }
    ]
  }
]
```

Пороговое значение балансировки для метрики представляет собой коэффициент. Если объем нагрузки на наиболее загруженном узле, деленный на объем нагрузки на наименее загруженном узле, превышает это число, кластер считается несбалансированным. В результате балансировка запускается при очередной проверке службой Cluster Resource Manager.

<center>
![Пример порогового значения балансировки][Image1]
</center>

В этом примере каждая служба использует одну единицу определенной метрики. В верхнем примере максимальная нагрузка на узле составляет пять, а минимальная — два. Предположим, что пороговое значение балансировки для метрики — три. Так как коэффициент для кластера 5/2 = 2,5, что меньше указанного порогового значения балансировки, равного трем, кластер сбалансирован. При очередной проверке службой Cluster Resource Manager балансировка не запускается.

В примере ниже максимальная нагрузка на узле составляет десять, а минимальная — два (значит, коэффициент будет равен пяти). Пять больше установленного порогового значения балансировки для этой метрики, равного трем. В результате при следующем срабатывании таймера будет запланирован запуск перераспределения кластера. В подобной ситуации часть нагрузки почти наверняка будет распределена на узел Node3. Поскольку Cluster Resource Manager в Service Fabric не использует каскадный подход, часть нагрузки может также распределяться на узел Node2. Это позволит свести к минимуму общие различия между узлами, что и является одной из задач Cluster Resource Manager.

<center>
![Действия в примере порогового значения балансировки][Image2]
</center>

Уменьшение разницы в нагрузке ниже заданного предела — это не главная цель. Пороговые значения балансировки — это просто *триггеры*, которые сообщают Cluster Resource Manager о том, что нужно проверить кластер на предмет возможных улучшений. Безусловно, запуск поиска возможности балансировки вовсе не означает, что что-либо будет перемещено. Иногда кластер несбалансирован, но улучшить ситуацию невозможно.

## <a name="activity-thresholds"></a>пороговые значения активности
Иногда узлы могут быть относительно несбалансированными, даже если *общий* объем нагрузки в кластере небольшой. Отсутствие нагрузки может быть связано с ее временным снижением или с тем, что кластер только что создан и проходит начальную загрузку. В любом случае в такой ситуации можно не тратить время на балансировку кластера, так как это вряд ли принесет ощутимые результаты. Если кластер прошел балансировку, вы только потратите сетевые и вычислительные ресурсы на перемещение объектов, а в результате не будет *абсолютно* никакой разницы. Чтобы этого избежать, можно воспользоваться еще одним элементом управления под названием "Пороговые значения активности". С его помощью можно задать абсолютное значение нижней границы активности. Если это пороговое значение не превышено ни для одного узла, балансировка не запускается даже при достижении ее порогового значения.

Например, посмотрите на схему ниже. Допустим, пороговое значение балансировки для этой метрики по-прежнему равно трем, но теперь у нас есть пороговое значение активности, которое составляет 1536. В первом случае согласно пороговому значению балансировки кластер несбалансирован, однако ни один из узлов не превышает пороговое значение активности, поэтому ничего не происходит. В нижнем примере узел Node1 намного превышает пороговое значение активности. Так как для метрики превышены пороговые значения и балансировки, и активности, планируется балансировка.

<center>
![Пример порогового значения активности][Image3]
</center>

Как и пороговые значения балансировки, пороговые значения активности определяются в определении кластера на основе метрик:

ClusterManifest.xml

``` xml
    <Section Name="MetricActivityThresholds">
      <Parameter Name="Memory" Value="1536"/>
    </Section>
```

Для автономных развернутых служб используется ClusterConfig.json, а для размещенных в Azure кластеров — Template.json:

```json
"fabricSettings": [
  {
    "name": "MetricActivityThresholds",
    "parameters": [
      {
          "name": "Memory",
          "value": "1536"
      }
    ]
  }
]
```

Пороговые значения балансировки и активности привязаны к определенной метрике. Балансировка запускается только тогда, когда оба эти пороговые значения превышены для одной и той же метрики.

## <a name="balancing-services-together"></a>Одновременная балансировка служб
Следует отметить, что несбалансированность кластера определяется по общему состоянию кластера. Тем не менее она устраняется путем перемещения реплик или экземпляров отдельных служб. Звучит разумно, не правда ли? Если память накапливается в одном узле, это может быть вызвано сразу несколькими репликами или экземплярами. Для устранения дисбаланса может потребоваться перемещение всех реплик с отслеживанием состояния или экземпляров без отслеживания состояния, для которых используется несбалансированная метрика.

Тем не менее в таком случае иногда перемещается и служба, в которой не было дисбаланса. Почему служба может перемещаться между узлами, даже если все метрики этой службы отлично сбалансированы, в то время как существует другой дисбаланс? Давайте разберемся.

Возьмем для примера четыре службы: S1, S2, S3 и S4. S1 использует метрики M1 и M2, S2 — метрики M2 и M3, S3 — метрики M3 и M4, а S4 — какую-то метрику M99. Понимаете, к чему все это ведет? Возникает цепочка. Мы имеем дело не с четырьмя независимыми службами, а с набором связанных служб (S1, S2 и S3) и одной отдельной службой.

<center>
![Одновременная балансировка служб][Image4]
</center>

Таким образом, дисбаланс по метрике M1 может вызвать перемещение реплик или экземпляров, относящихся к службе S3 (которая не сообщает значение метрики M1). Обычно эти перемещения ограничены, но могут быть и более масштабными в зависимости от того, насколько велик дисбаланс по метрике M1 и какие изменения потребуются в кластере, чтобы его устранить. Кроме того, можно с уверенностью сказать, что дисбаланс в метриках M1, M2 и M3 не вызовет перемещения службы S4. Оно не имеет смысла, так как перемещение реплик или экземпляров службы S4 никак не повлияет на баланс метрик M1, M2 и M3.

При каждом запуске Cluster Resource Manager автоматически определяет, какие службы связаны, так как службы могут добавляться и удаляться, а конфигурации их метрик — изменяться. Например, между двумя запусками балансировки конфигурация службы S2 может быть изменена для удаления из нее метрики M2. В результате такого изменения связь между службами S1 и S2 будет разорвана. В этом случае две группы связанных служб превратятся в три:

<center>
![Одновременная балансировка служб][Image5]
</center>

## <a name="next-steps"></a>Дальнейшие действия
* Метрики показывают, как диспетчер кластерных ресурсов Service Fabric управляет потреблением и емкостью в кластере. Дополнительные сведения о метриках и их настройке см. в [этой статье](service-fabric-cluster-resource-manager-metrics.md).
* Стоимость перемещения — один из способов сообщить диспетчеру кластерных ресурсов, что некоторые службы перемещать затратнее, чем остальные. Дополнительные сведения о стоимости перемещения см. в [этой статье](service-fabric-cluster-resource-manager-movement-cost.md).
* В диспетчере кластерных ресурсов имеется несколько регулировок, которые можно настроить, чтобы замедлить отток в кластере. Обычно они не требуется, но при необходимости узнать о регулировках можно [здесь](service-fabric-cluster-resource-manager-advanced-throttling.md)

[Image1]:./media/service-fabric-cluster-resource-manager-balancing/cluster-resrouce-manager-balancing-thresholds.png
[Image2]:./media/service-fabric-cluster-resource-manager-balancing/cluster-resource-manager-balancing-threshold-triggered-results.png
[Image3]:./media/service-fabric-cluster-resource-manager-balancing/cluster-resource-manager-activity-thresholds.png
[Image4]:./media/service-fabric-cluster-resource-manager-balancing/cluster-resource-manager-balancing-services-together1.png
[Image5]:./media/service-fabric-cluster-resource-manager-balancing/cluster-resource-manager-balancing-services-together2.png



<!--HONumber=Jan17_HO4-->


