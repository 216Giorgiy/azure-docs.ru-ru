<properties
   pageTitle="Диспетчер ресурсов кластера Service Fabric — интеграция управления"
   description="Обзор точек интеграции между диспетчером ресурсов кластера и управлением Service Fabric."
   services="service-fabric"
   documentationCenter=".net"
   authors="masnider"
   manager="timlt"
   editor=""/>

<tags
   ms.service="Service-Fabric"
   ms.devlang="dotnet"
   ms.topic="article"
   ms.tgt_pltfrm="NA"
   ms.workload="NA"
   ms.date="03/03/2016"
   ms.author="masnider"/>


# Интеграция управления
Диспетчер ресурсов не является основным компонентом Service Fabric, обрабатывающим операции управления (например обновления приложения), но он участвует в этом процессе. Первый способ, используемый диспетчером ресурсов при управлении, заключается в отслеживании требуемого состояния кластера и служб внутри него с точки зрения выделения ресурсов и формирования баланса, а также внесения изменений через подсистему работоспособности Service Fabric в случае возникновения проблем. Другой вариант связан с принципами действия обновлений. В частности, во время обновлений происходит изменение некоторых функциональных возможностей диспетчера ресурсов и вступает в силу ряд специальных процессов. Мы рассмотрим эти вопросы далее.

## Интеграция работоспособности
Диспетчер ресурсов постоянно отслеживает правила, определенные для служб, и если он не может их выполнить, то выдает ошибки и предупреждения о работоспособности. Например, если узел перегружен и диспетчер ресурсов не может исправить ситуацию, он выведет предупреждение о работоспособности, указывающее, какой узел перегружен и какие показатели превышены.

Можно привести еще один пример: диспетчер ресурсов выводит предупреждения о работоспособности, если вы определили ограничение размещения (например "NodeColor == Blue") и диспетчер ресурсов обнаруживает его нарушение. Это делается для настраиваемых ограничений, а также для ограничений по умолчанию (например, распространение домена сбоя и обновления), которые диспетчер ресурсов применяет автоматически. Ниже приведен пример такого отчета о работоспособности. В этом случае отчет о работоспособности создан для одного из разделов системной службы, поскольку реплики этого раздела временно сгруппированы в слишком незначительное количество доменов сбоя, что может произойти из-за строки ошибок:

```posh
PS C:\Users\User > Get-WindowsFabricPartitionHealth -PartitionId '00000000-0000-0000-0000-000000000001'


PartitionId           : 00000000-0000-0000-0000-000000000001
AggregatedHealthState : Warning
UnhealthyEvaluations  :
                        Unhealthy event: SourceId='System.PLB', Property='ReplicaConstraintViolation_FaultDomain', HealthState='Warning', ConsiderWarningAsError=false.

ReplicaHealthStates   :
                        ReplicaId             : 130766528804733380
                        AggregatedHealthState : Ok

                        ReplicaId             : 130766528804577821
                        AggregatedHealthState : Ok

                        ReplicaId             : 130766528854889931
                        AggregatedHealthState : Ok

                        ReplicaId             : 130766528804577822
                        AggregatedHealthState : Ok

                        ReplicaId             : 130837073190680024
                        AggregatedHealthState : Ok

HealthEvents          :
                        SourceId              : System.PLB
                        Property              : ReplicaConstraintViolation_FaultDomain
                        HealthState           : Warning
                        SequenceNumber        : 130837100116930204
                        SentAt                : 8/10/2015 7:53:31 PM
                        ReceivedAt            : 8/10/2015 7:53:33 PM
                        TTL                   : 00:01:05
                        Description           : The Load Balancer has detected a Constraint Violation for this Replica: fabric:/System/FailoverManagerService Secondary Partition 00000000-0000-0000-0000-000000000001 is
                        violating the Constraint: FaultDomain Details: Node -- 3d1a4a68b2592f55125328cd0f8ed477  Policy -- Packing
                        RemoveWhenExpired     : True
                        IsExpired             : False
                        Transitions           : Ok->Warning = 8/10/2015 7:13:02 PM, LastError = 1/1/0001 12:00:00 AM
```

В сообщении о работоспособности содержатся следующие сведения:

1.	все реплики работоспособны (это первоочередной приоритет Service Fabric);
2.	в настоящее время нарушается ограничение распространения домена сбоя (это значит, что количество реплик для этого раздела в определенном домене сбоя превышает допустимое значение);
3.	узел, который содержит реплику, вызвавшую нарушение (узел с ИД 3d1a4a68b2592f55125328cd0f8ed477);
4.	время возникновения этой ситуаций (10.08.2015 19:13:02). Это полезные данные для предупреждения, выводимого в рабочей среде и информирующего о нарушении, на которое вы, возможно, захотите взглянуть. В нашем случае, к примеру, мы хотели бы узнать, почему диспетчер ресурсов выбрал именно вариант с группировкой реплик в домен сбоя. Это может быть связано с тем, что была нарушена работа всех узлов в других доменах сбоя и не оказалось достаточного количества запасных доменов. Если же имелось достаточное количество работающих доменов, возможно, существовала какая-либо другая причина выхода из строя узлов в других доменах сбоя (например политика InvalidDomain в службе).

Предположим, что вы хотите создать службу или диспетчер ресурсов пытается найти место для размещения некоторых служб, но, похоже, приемлемые решения отсутствуют. Может существовать целый ряд причин, но, как правило, такая ситуация возникает при одном из двух указанных далее условий.

1.	Некое переходное условие сделало невозможным правильное размещение этого экземпляра службы или реплики.
2.	Требования к службе настроены неправильно, в результате чего их невозможно выполнить.

При наличии каждого из этих условий диспетчер ресурсов создаст отчет о работоспособности со сведениями, которые помогут определить, что происходит и почему нельзя разместить службу. Этот процесс мы будем называть последовательностью устранения ограничений. В его ходе рассмотрим настроенные ограничения, влияющие на службу, и узнаем, что именно они устраняют. Таким образом, когда не удается разместить какие-либо компоненты, можно определить, какие узлы были устранены и почему. Давайте поговорим о каждом из ограничений, которые можно увидеть в отчетах о работоспособности, и о том, что они проверяют. Обратите внимание, что большую часть времени вы не будете видеть эти ограничения, устраняющие узлы, поскольку по умолчанию они находятся на программном или оптимальном уровне (как было указано ранее). Их можно увидеть, если они отражаются или считаются жесткими ограничениями. Ниже они указываются для полноты информации.
-	ReplicaExclusionStatic и ReplicaExclusionDynamic. Это внутреннее ограничение, которое указывает, что во время поиска было установлено, что на узле будут размещены две реплики (а это запрещено). ReplicaExclusionStatic и ReplicaExclusionDynamic являются практически аналогичным правилом. Ограничение ReplicaExclusionDynamic означает, что эту реплику не удалось разместить в данном месте, поскольку единственное предлагаемое решение уже разместило здесь реплику. Оно отличается от исключения ReplicaExclusionStatic, указывающего на не предлагаемый, фактический конфликт — на узле уже есть реплика. Это запутанная ситуация? Да. Она имеет большое значение? Нет. Достаточно сказать, что если вы видите последовательность устранения ограничений, содержащую ограничение ReplicaExclusionStatic или ReplicaExclusionDynamic, диспетчер ресурсов считает, что узлов недостаточно для размещения всех реплик. Следующие ограничения обычно информируют, почему у нас остается слишком мало узлов.
-	PlacementConstraint: это сообщение означает, что некоторые узлы были устранены из-за несоответствия ограничениям на размещение службы. Мы рассматриваем текущие настроенные ограничения на размещения как часть этого сообщения.
-	NodeCapacity: это ограничение означает, что нельзя размещать реплики на указанных узлах, поскольку в противном случае это приведет к превышению емкости узлов.
-	Affinity: это ограничение означает, что нельзя размещать реплики на затронутых узлах, поскольку в противном случае это приведет к нарушению ограничения сходства.
-	FaultDomain и UpgradeDomain: это ограничение исключает узлы, если размещение реплики на указанных узлах привело к упаковке в конкретный домен сбоя или обновления.
-	PreferredLocation: обычно это ограничение не отображается. Оно приводит к удалению узлов из решения, так как оно является оптимальным только по умолчанию. Кроме того, ограничение на предпочтительное расположение обычно существует только во время обновлений (если оно используется для возврата реплик в то место, где они находились при запуске обновления). Тем не менее, оно допускается.

## Обновления
Во время обновлений приложения и кластера диспетчер ресурсов помогает обеспечить плавное выполнение процесса, соблюдение правил и сохранение производительности кластера на должном уровне.

### Соблюдение правил
В первую очередь следует иметь в виду, что правила — строгие принципы контроля таких действий, как ограничения на размещение, — по-прежнему применяются во время обновлений. Вам может показаться, что это подразумевается само собой, однако мы все равно отметим этот факт. Положительной стороной является то, что если нужно, чтобы определенные рабочие нагрузки не выполнялись на конкретных узлах, это будет (или не будет) по-прежнему происходить даже во время обновления без каких-либо действий из операций. Если ваша среда характеризуется высоким уровнем ограничений, выполнение обновлений может занять много времени, так как существует совсем немного вариантов относительно мест размещения службы, если ее потребуется приостановить для применения исправлений.

### Интеллектуальные замены
При запуске обновления диспетчер ресурсов создает моментальный снимок текущего размещения кластера и по завершении обновления пытается вернуть элементы в это состояние. Причина проста — во-первых, это гарантирует, что в процессе обновления для каждой службы будет выполнено лишь несколько переходов (выход из затронутого узла и последующее возвращение обратно). Во-вторых, это гарантирует, что само обновление не оказывает значительное влияние на структуру кластера. Если кластер был хорошо организован до обновления, он останется в этом состоянии и после обновления.

### Сокращение оттока
Во время обновлений возникает еще одна ситуация, когда диспетчер ресурсов отключает балансировку для обновляемой сущности. Поэтому если у вас есть два разных экземпляра приложения и затем на одном из них запускается обновление, происходит приостановка балансировки для этого, а не другого экземпляра приложения. Предотвращение упреждающей балансировки не позволяет диспетчеру ресурсов реагировать на обновление ("О, нет! Пустой узел! Лучше заполнить его чем- нибудь!") и создавать множество дополнительных действий для служб в кластере, которые нужно просто отменить, когда службам требуется вернуться на узлы после завершения обновления. Если рассматриваемое обновление является обновлением кластера, то весь кластер приостанавливается для балансировки на время обновления (проверки ограничений, обеспечивающие соблюдение правил, остаются активными).

### Упрощенные правила
Во время обновлений обычно возникает необходимость завершения процесса внесения изменений, даже если кластер ограничен или полон. По сути, мы уже говорили о том, как это сделать, но во время обновления этот момент становится особенно важным, поскольку обычно от 5 до 20 процентов кластера находится в нерабочем состоянии при развертывании обновления в кластере, а эта рабочая нагрузка должна где-нибудь находиться. Именно здесь вступает в игру понятие буферизованных емкостей, о котором упоминалось ранее. Несмотря на то, что буферизованная емкость поддерживается во время обычной работы, во время обновлений диспетчер ресурсов будет проводить заполнение до общей емкости.


<!--Every topic should have next steps and links to the next logical set of content to keep the customer engaged-->
## Дальнейшие действия
- [Знакомство с диспетчером ресурсов кластера Service Fabric](service-fabric-cluster-resource-manager-introduction.md)

<!---HONumber=AcomDC_0309_2016-->