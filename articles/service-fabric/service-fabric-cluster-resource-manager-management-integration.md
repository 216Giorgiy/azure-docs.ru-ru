<properties
   pageTitle="Диспетчер кластерных ресурсов Service Fabric — интеграция управления | Microsoft Azure"
   description="Обзор точек интеграции между диспетчером ресурсов кластера и управлением Service Fabric."
   services="service-fabric"
   documentationCenter=".net"
   authors="masnider"
   manager="timlt"
   editor=""/>

<tags
   ms.service="Service-Fabric"
   ms.devlang="dotnet"
   ms.topic="article"
   ms.tgt_pltfrm="NA"
   ms.workload="NA"
   ms.date="08/19/2016"
   ms.author="masnider"/>


# Интеграция диспетчера кластерных ресурсов с управлением кластерами Service Fabric
Диспетчер кластерных ресурсов Service Fabric не является основным компонентом Service Fabric, обрабатывающим операции управления (например, обновления приложения), но он участвует в этом процессе. Во-первых, диспетчер кластерных ресурсов отслеживает требуемое состояние кластера и служб внутри него с точки зрения распределения ресурсов и баланса. Он отправляет отчеты о работоспособности, если не удается перевести кластер в нужное состояние (например, если недостаточно свободных ресурсов или возникает конфликт между правилами размещения служб). Второе направление интеграции связано с обновлениями: диспетчер кластерных ресурсов во время обновления изменяет свое поведение. Мы рассмотрим эти вопросы далее.

## Интеграция функций работоспособности
Диспетчер кластерных ресурсов постоянно отслеживает правила, которые вы определили для своих служб, и контролирует свободные ресурсы узлов и кластера в целом. Кроме того, он передает сообщения о работоспособности и ошибках, если не может выполнить эти правила или выделить достаточно ресурсов. Например, если узел перегружен и диспетчер кластерных ресурсов не может исправить ситуацию, то он выведет предупреждение о работоспособности, указывающее, какой узел перегружен и по каким метрикам.

Можно привести еще один пример: диспетчер ресурсов выводит предупреждения о работоспособности, если вы определили ограничение размещения (например "NodeColor == Blue") и диспетчер ресурсов обнаруживает его нарушение. Это делается для настраиваемых ограничений, а также для ограничений по умолчанию (например, ограничений для доменов сбоя и обновления), которые применяются автоматически.

Ниже приведен пример такого отчета о работоспособности. В этом случае отчет о работоспособности создан для одного из разделов системной службы. Он сообщает, что реплики этого раздела временно сгруппированы в слишком малое количество доменов обновления. Такое может происходить в случае серии сбоев.

```posh
PS C:\Users\User > Get-WindowsFabricPartitionHealth -PartitionId '00000000-0000-0000-0000-000000000001'


PartitionId           : 00000000-0000-0000-0000-000000000001
AggregatedHealthState : Warning
UnhealthyEvaluations  :
                        Unhealthy event: SourceId='System.PLB', Property='ReplicaConstraintViolation_UpgradeDomain', HealthState='Warning', ConsiderWarningAsError=false.

ReplicaHealthStates   :
                        ReplicaId             : 130766528804733380
                        AggregatedHealthState : Ok

                        ReplicaId             : 130766528804577821
                        AggregatedHealthState : Ok

                        ReplicaId             : 130766528854889931
                        AggregatedHealthState : Ok

                        ReplicaId             : 130766528804577822
                        AggregatedHealthState : Ok

                        ReplicaId             : 130837073190680024
                        AggregatedHealthState : Ok

HealthEvents          :
                        SourceId              : System.PLB
                        Property              : ReplicaConstraintViolation_UpgradeDomain
                        HealthState           : Warning
                        SequenceNumber        : 130837100116930204
                        SentAt                : 8/10/2015 7:53:31 PM
                        ReceivedAt            : 8/10/2015 7:53:33 PM
                        TTL                   : 00:01:05
                        Description           : The Load Balancer has detected a Constraint Violation for this Replica: fabric:/System/FailoverManagerService Secondary Partition 00000000-0000-0000-0000-000000000001 is
                        violating the Constraint: UpgradeDomain Details: Node -- 3d1a4a68b2592f55125328cd0f8ed477  Policy -- Packing
                        RemoveWhenExpired     : True
                        IsExpired             : False
                        Transitions           : Ok->Warning = 8/10/2015 7:13:02 PM, LastError = 1/1/0001 12:00:00 AM
```

В сообщении о работоспособности содержатся следующие сведения:

1.	все реплики работоспособны (это первоочередной приоритет Service Fabric);
2.	В настоящее время нарушается ограничение по распределению в доменах обновления (то есть количество реплик в определенном домене обновления для этого раздела превышает допустимое значение).
3.	узел, который содержит реплику, вызвавшую нарушение (узел с ИД 3d1a4a68b2592f55125328cd0f8ed477);
4.	Время обнаружения этой проблемы (08.10.2015, 19:13:02).

Это полезные данные для предупреждения, выводимого в рабочей среде и информирующего о нарушении, на которое вы, возможно, захотите взглянуть. В нашем случае, к примеру, мы хотели бы узнать, почему диспетчер ресурсов выбрал именно вариант с группировкой реплик в домен обновления. Это может быть связано с тем, что была нарушена работа всех узлов в других доменах обновления и не оказалось достаточного количества других свободных доменов. Если же имелось достаточное количество работающих доменов, то, возможно, узлы в других доменах сбоя были недоступны по другим причинам (например, в соответствии с политикой размещения для службы или при отсутствии достаточных ресурсов).

Предположим, что вы хотите создать службу или диспетчер ресурсов пытается найти место для размещения некоторых служб, но, похоже, приемлемые решения отсутствуют. Может существовать целый ряд причин, но, как правило, такая ситуация возникает при одном из двух указанных далее условий.

1.	Некое переходное условие сделало невозможным правильное размещение этого экземпляра службы или реплики.
2.	Требования к службе настроены неправильно, в результате чего их невозможно выполнить.

При наличии каждого из этих условий диспетчер кластерных ресурсов создаст отчет о работоспособности со сведениями, которые помогут определить, что происходит и почему нельзя разместить службу. Этот процесс мы будем называть последовательностью устранения ограничений. В его ходе система анализирует настроенные ограничения, влияющие на службу, и записывает, что именно они исключают. Таким образом, когда не удается разместить какие-либо службы, можно узнать, какие узлы были исключены и почему.

## Типы ограничений
Давайте поговорим о каждом из ограничений, которые можно увидеть в отчетах о работоспособности, и о том, что они проверяют. Обратите внимание, что большую часть времени вы не будете видеть эти ограничения, исключающие узлы, так как по умолчанию они нежесткие или используются для оптимизации. Приоритеты ограничений более подробна рассматриваются далее в этой статье. Однако вы можете видеть сообщения о работоспособности, относящиеся к этим ограничениям, если это жесткие ограничения или ограничения, которые приводят к исключению узлов (что случается реже). Поэтому мы добавили их для полноты информации.

-	ReplicaExclusionStatic и ReplicaExclusionDynamic. Это внутреннее ограничение, которое указывает, что во время поиска было установлено, что на узле должны быть размещены две реплики с отслеживанием состояния или два экземпляра без отслеживания состояний из одной секции (а это запрещено). ReplicaExclusionStatic и ReplicaExclusionDynamic являются практически аналогичным правилом. Ограничение ReplicaExclusionDynamic означает, что эту реплику не удалось разместить в данном месте, поскольку единственное предлагаемое решение уже разместило здесь реплику. Оно отличается от исключения ReplicaExclusionStatic, указывающего на не предлагаемый, фактический конфликт — на узле уже есть реплика. Это запутанная ситуация? Да. Она имеет большое значение? Нет. Достаточно сказать, что если вы видите последовательность устранения ограничения, содержащую ограничение ReplicaExclusionStatic или ReplicaExclusionDynamic, то диспетчер кластерных ресурсов считает, что узлов недостаточно для размещения всех реплик. Следующие ограничения обычно информируют, почему у нас остается слишком мало узлов.
-	PlacementConstraint: это сообщение означает, что некоторые узлы были устранены из-за несоответствия ограничениям на размещение службы. Мы рассматриваем текущие настроенные ограничения на размещения как часть этого сообщения. Обычно это нормально, если указаны какие-либо ограничения на размещение. Однако если в ограничении на размещение имеется ошибка, которая приводит к исключению слишком большого числа узлов, то именно здесь вы увидите результат этого.
-	NodeCapacity: это ограничение означает, что нельзя размещать реплики на указанных узлах, поскольку в противном случае это приведет к превышению емкости узлов.
-	Affinity: это ограничение означает, что нельзя размещать реплики на затронутых узлах, поскольку в противном случае это приведет к нарушению ограничения сходства.
-	FaultDomain и UpgradeDomain. Это ограничение исключает узлы, если размещение реплики на указанных узлах привело к упаковке в конкретный домен сбоя или обновления. Несколько примеров, посвященных этому ограничению, представлены в разделе об [ограничениях доменов сбоя и обновления и результирующем поведении](service-fabric-cluster-resource-manager-cluster-description.md).
-	PreferredLocation: обычно это ограничение не отображается. Оно приводит к удалению узлов из решения, так как оно является оптимальным только по умолчанию. Кроме того, ограничение на предпочтительное расположение обычно существует только во время обновлений (если оно используется для возврата реплик в то место, где они находились при запуске обновления). Тем не менее, оно допускается.

### Приоритеты ограничений
Изучив все эти ограничения, вы можете прийти к выводу, что ограничения на размещение являются самым важным аспектом в вашей системе. Вы готовы нарушать все остальные ограничения, даже ограничения сходства и емкости, лишь бы гарантировать, что ограничения размещения никогда не будут нарушены.

И мы рады сообщить, что это возможно! Для ограничений можно указать несколько разных уровней применения, которые подразделяются на "жесткие" (0), "мягкие" (1), "оптимизационные" (2) и "отключено" (-1). Большинство ограничений мы по умолчанию определили как "жесткие" (в связи с тем, что большинство пользователей не готовы смягчать требования к емкости ресурсов), и практически все относятся к категориям "жестких" или "мягких". Но в ситуациях, когда требуется точная настройка, это можно изменить. Например, если вы готовы к нарушению правил сходства ради решения проблем с емкостью ресурсов узла, то вы можете задать для ограничений сходства "нежесткий" приоритет (1), а для ограничений емкости оставить "жесткий" приоритет (0). Разные приоритеты ограничений являются причиной того, что некоторые предупреждения о нарушении ограничений будут появляться чаще других. Именно эти ограничения мы готовы временно смягчить (нарушить). Обратите внимание, что эти уровни не гарантируют, что определенное ограничение всегда будет нарушаться или никогда не будет нарушаться. Приоритет лишь устанавливает порядок, в котором предпочтительно применять эти правила, и этот порядок мы используем для поиска компромиссов, когда невозможно выполнить сразу все условия.

Ниже перечислены конфигурации и значения приоритета по умолчанию для различных ограничений.

ClusterManifest.xml

```xml
        <Section Name="PlacementAndLoadBalancing">
            <Parameter Name="PlacementConstraintPriority" Value="0" />
            <Parameter Name="CapacityConstraintPriority" Value="0" />
            <Parameter Name="AffinityConstraintPriority" Value="0" />
            <Parameter Name="FaultDomainConstraintPriority" Value="0" />
            <Parameter Name="UpgradeDomainConstraintPriority" Value="1" />
            <Parameter Name="PreferredLocationConstraintPriority" Value="2" />
        </Section>
```

Можно заметить, что здесь установлены ограничения для доменов обновления и сбоя, при этом ограничение для доменов обновления является нежестким. Также вы видите непонятное на первый взгляд ограничение PreferredLocation с некоторым приоритетом. Что же все это значит?

Здесь мы сформулировали правила, соответствующие нашему намерению распределять службы по доменам сбоя и обновления в соответствии с ограничениями внутри диспетчера ресурсов. В нашей работе мы несколько раз сталкивались с необходимостью очень строго соблюдать размещение в доменах сбоя или обновления. Также несколько раз возникали такие проблемы, при которых мы вынуждены были полностью (но ненадолго!) игнорировать эти домены. Поэтому мы очень рады, что выбрали такую конфигурацию, и довольны гибкостью инфраструктуры ограничений. В большинстве случаев ограничение для домена обновления является нежестким. Это значит, что Resource Manager позволяется временно сгруппировать пару реплик в домен обновления, например, в процессе выполнения обновления, или в результате нескольких одновременных сбоев или нарушений других ограничений (жестких). Обычно этого не требуется, пока не случится множество сбоев или других сложностей в системе, мешающих правильному размещению. А если среда настроена правильно, состояние всегда остается стабильным, и правила для доменов обновления полностью соблюдаются.

Ограничение PreferredLocation отличается от остальных, и оно здесь единственное с "оптимизационным" приоритетом. Мы используем это ограничение в период обновлений, чтобы службы по возможности восстанавливались там же, где они работали перед обновлением. На практике это может оказаться невозможным из-за множества причин, но это хорошее правило для оптимизации, и мы его сохраняем. Мы подробнее поговорим о нем, когда будем рассматривать помощь диспетчера кластерных ресурсов в процессе обновления.

## Обновления
Во время обновлений приложения и кластера диспетчер кластерных ресурсов помогает обеспечить плавное выполнение процесса, соблюдение правил и сохранение производительности кластера на должном уровне.

### Применение правил
В первую очередь следует иметь в виду, что правила — строгие принципы контроля таких действий, как ограничения на размещение, — по-прежнему применяются во время обновлений. Вам может показаться, что это подразумевается само собой, однако мы все равно отметим этот факт. С одной стороны, это хорошо. Если вам нужно, чтобы определенные рабочие нагрузки не выполнялись на конкретных узлах, такие правила будут автоматически применяться даже во время обновления. С другой стороны, большое количество ограничений в среде может привести к тому, что обновления будут выполняться довольно долго. У системы будет очень мало возможностей правильно разместить службу, если для обновления ее (или узел, на котором она размещена) нужно отключить.

### Интеллектуальные замены
При запуске обновления диспетчер ресурсов создает моментальный снимок текущего размещения кластера и по завершении обновления пытается вернуть элементы в это состояние. Причина проста — во-первых, это гарантирует, что в процессе обновления для каждой реплики или экземпляра службы будет выполнено лишь несколько переходов (выход из затронутого узла и последующее возвращение обратно). Во-вторых, это гарантирует, что само обновление не окажет значительного влияния на структуру кластера. Если кластер был хорошо организован до обновления, то и после обновления он будет организован хорошо или по крайней мере не хуже прежнего.

### Сокращение оттока
Во время обновлений действует еще одно дополнительное правило — диспетчер кластерных ресурсов отключает балансировку для обновляемой сущности. Поэтому если у вас есть два разных экземпляра приложения и затем на одном из них запускается обновление, происходит приостановка балансировки для этого, а не другого экземпляра приложения. Отключение реактивной балансировки позволяет избежать реакции системы на само обновление ("О нет! Пустой узел! Нужно заполнить его чем-нибудь!"), чтобы не выполнялось множество лишних действий для служб в кластере. Такие действия все равно придется отменять, когда службы вернутся на свои узлы после завершения обновления. Если рассматривается обновление кластера, то на время обновления приостанавливается балансировка во всем кластере (проверки ограничений, обеспечивающие соблюдение правил, остаются активными; отключается только упреждающее перераспределение).

### Упрощенные правила
Во время обновлений обычно возникает необходимость завершения процесса внесения изменений, даже если кластер ограничен или полон. Иметь возможность управлять емкостью кластера во время обновления даже важнее, чем обычно, так как при развертывании обновления в кластере обычно отключается 5–20 % его емкости, и соответствующую рабочую нагрузку, как правило, можно перенаправить в другое место. Именно здесь вступает в игру понятие [буферизованных емкостей](service-fabric-cluster-resource-manager-cluster-description.md#buffered-capacity). Буферизованная емкость поддерживается во время обычной работы (что оставляет некоторый запас), но во время обновлений диспетчер кластерных ресурсов будет использовать все доступные емкости (использовать созданный буфер).

## Дальнейшие действия
- Начните с самого начала, [изучив общие сведения о диспетчере кластерных ресурсов Service Fabric](service-fabric-cluster-resource-manager-introduction.md).

<!---HONumber=AcomDC_0824_2016-->