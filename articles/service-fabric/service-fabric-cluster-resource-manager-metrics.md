---
title: Управление метриками с помощью диспетчера кластерных ресурсов Azure Service Fabric | Microsoft Docs
description: Узнайте, как настраивать и использовать метрики в Service Fabric.
services: service-fabric
documentationcenter: .net
author: masnider
manager: timlt
editor: ''

ms.service: Service-Fabric
ms.devlang: dotnet
ms.topic: article
ms.tgt_pltfrm: NA
ms.workload: NA
ms.date: 08/19/2016
ms.author: masnider

---
# Управление потреблением ресурсов и нагрузкой в Service Fabric с помощью метрик
Метрики — общий термин в Service Fabric, относящийся к ресурсам, которые нужны вашим службам и которые предоставляются узлами в кластере. Вообще метрика — это все, чем требуется управлять, чтобы контролировать производительность своих служб.

Примерами метрик являются ресурсы памяти, дисков и ЦП. Это физические метрики, т. е. ресурсы, соответствующие физическим ресурсам на узле, которыми нужно управлять. Метрики могут также быть (и обычно бывают) логическими. Например, такие параметры, как MyWorkQueueDepth, определяются в приложении и соответствуют определенному уровню потребления ресурсов (само приложение не знает, в каком объеме оно потребляет ресурсы, или же не знает, как это потребление измерить). Большинство встречающихся пользователям метрик являются логическими. Этому есть несколько причин, среди которых можно выделить две самые распространенные. Первая — многие наши клиенты пишут свои службы в управляемом коде. Вторая — в рамках конкретного экземпляра службы без сохранения состояния или объекта реплики службы с отслеживанием состояния трудно измерить и представить показатели потребления фактических физических ресурсов. Именно из-за сложности создания отчетов о собственных метриках мы предоставляем готовые к использованию метрики.

## Метрики по умолчанию
Предположим, вы хотите приступить к работе и не знаете, какие ресурсы вы будете использовать, или даже не знаете, какие ресурсы вообще вам нужны. Поэтому вы переходите к реализации и создаете службы без указания метрик. Все в порядке. Мы выберем метрики для вас. Если вы не указали собственные метрики, мы по умолчанию используем для вас сегодня следующие метрики: PrimaryCount (первичное количество), ReplicaCount (количество реплик) и (звучит немного туманно, мы понимаем) Count (количество). В следующей таблице показана связь нагрузки для каждой метрики с соответствующими объектами службы.

| Метрика | Нагрузка экземпляра без отслеживания состояния | Вторичная нагрузка с отслеживанием состояния | Первичная нагрузка с отслеживанием состояния |
| --- | --- | --- | --- |
| PrimaryCount |0 |0 |1 |
| ReplicaCount |0 |1 |1 |
| Count |1 |1 |1 |

Итак, что же вы получаете с помощью этих используемых по умолчанию метрик? Оказывается, что работа правильно распределяется между основными рабочими нагрузками. На примере ниже вы можете увидеть, что произойдет, если мы создадим одну службу с отслеживанием состояния с тремя разделами, укажем для размера набора целевых реплик значение "3", а также создадим единую службу без отслеживания состояния с количеством экземпляров "3". Получится что-то вроде этого.

![Макет кластера с метриками по умолчанию][Image1]

В этом примере мы видим следующие результаты.

* Первичные реплики для службы с отслеживанием состояния не расположены на одном узле.
* Реплики одного раздела не находятся на одном и том же узле.
* Общее число первичных и вторичных реплик правильно распределено в кластере.
* Общее количество объектов службы (без отслеживания состояния и с отслеживанием состояния) равномерно выделено на каждом узле.

Неплохо!

Все будет работать отлично, пока у вас не возникнет вопрос: какова вероятность того, что выбранная схема секционирования со временем приведет к идеально равномерному распределению всех узлов? И еще один вопрос: какова вероятность того, что данная служба испытывает одинаковую нагрузку все время или даже прямо сейчас? Оказывается, что для любой серьезной нагрузки вероятность того, что все реплики будут эквивалентны, достаточно мала. Поэтому если вы стремитесь максимально эффективно использовать кластер, возможно, стоит рассмотреть пользовательские метрики.

На самом деле вы можете спокойно работать с только метриками по умолчанию. Но обычно это приводит к тому, что кластер используется меньше, чем хотелось бы (так как создание отчетов не является адаптивным и рассматривает все как эквивалентное). В худшем случае это может привести к неправильному планированию узлов, что, в свою очередь, может вызвать проблемы с производительностью. Лучше использовать пользовательские метрики с отчетами о динамической нагрузке. Их мы и рассмотрим ниже.

## Настраиваемые метрики
Мы выяснили, что метрики могут быть физическими и логическими, а пользователи могут определять свои собственные метрики. Отлично! Но как? На самом деле это довольно просто! Достаточно при создании службы настроить метрику и используемую по умолчанию начальную нагрузку. Любой набор метрик и значений по умолчанию, представляющих предполагаемое использование ресурсов службой, можно настроить отдельно для каждого экземпляра именованной службы во время ее создания.

Обратите внимание, что если вы начали определять пользовательские метрики, нужно явно добавить их к метрикам по умолчанию, если вы хотите, чтобы они (пользовательские метрики) тоже использовались для балансировки нагрузки в службе. Вот почему нам необходимо иметь четкое представление о связи между метриками по умолчанию и пользовательскими метриками: возможно, для вас важнее больше интересует, как потребляется память или какова длина рабочей очереди, чем распределение нагрузки в первичной реплике.

Предположим, вы хотите настроить службу, которая выдает значение метрики с именем "Память" (в дополнение к метрике по умолчанию). Предположим, вы провели основные измерения и выяснили, что обычно первичная реплика этой службы может занимать 20 МБ памяти, хотя вторичные реплики той же службы занимают 5 МБ. Вы знаете, что "Память" является наиболее важной метрикой с точки зрения управления производительностью данной службы. Но вы все равно хотите, чтобы нагрузка была сбалансирована между основными репликами и чтобы в результате потери узла или домена сбоя не пропадало слишком большое количество первичных реплик. В остальных случаях вы будет использовать значения по умолчанию.

Сделать нужно следующее.

Код:

```csharp
StatefulServiceDescription serviceDescription = new StatefulServiceDescription();
StatefulServiceLoadMetricDescription memoryMetric = new StatefulServiceLoadMetricDescription();
memoryMetric.Name = "MemoryInMb";
memoryMetric.PrimaryDefaultLoad = 20;
memoryMetric.SecondaryDefaultLoad = 5;
memoryMetric.Weight = ServiceLoadMetricWeight.High;

StatefulServiceLoadMetricDescription primaryCountMetric = new StatefulServiceLoadMetricDescription();
primaryCountMetric.Name = "PrimaryCount";
primaryCountMetric.PrimaryDefaultLoad = 1;
primaryCountMetric.SecondaryDefaultLoad = 0;
primaryCountMetric.Weight = ServiceLoadMetricWeight.Medium;

StatefulServiceLoadMetricDescription replicaCountMetric = new StatefulServiceLoadMetricDescription();
replicaCountMetric.Name = "ReplicaCount";
replicaCountMetric.PrimaryDefaultLoad = 1;
replicaCountMetric.SecondaryDefaultLoad = 1;
replicaCountMetric.Weight = ServiceLoadMetricWeight.Low;

StatefulServiceLoadMetricDescription totalCountMetric = new StatefulServiceLoadMetricDescription();
totalCountMetric.Name = "Count";
totalCountMetric.PrimaryDefaultLoad = 1;
totalCountMetric.SecondaryDefaultLoad = 1;
totalCountMetric.Weight = ServiceLoadMetricWeight.Low;

serviceDescription.Metrics.Add(memoryMetric);
serviceDescription.Metrics.Add(primaryCountMetric);
serviceDescription.Metrics.Add(replicaCountMetric);
serviceDescription.Metrics.Add(totalCountMetric);

await fabricClient.ServiceManager.CreateServiceAsync(serviceDescription);
```

PowerShell:

```posh
New-ServiceFabricService -ApplicationName $applicationName -ServiceName $serviceName -ServiceTypeName $serviceTypeName –Stateful -MinReplicaSetSize 2 -TargetReplicaSetSize 3 -PartitionSchemeSingleton –Metric @("Memory,High,20,5”,"PrimaryCount,Medium,1,0”,"ReplicaCount,Low,1,1”,"Count,Low,1,1”)
```

(Напоминаем, что если вы хотите использовать метрики по умолчанию, то вам вообще не нужно ничего менять в коллекции метрик или делать что-то специальное при создании службы.)

Теперь, когда вы знаете, как определить собственные метрики, поговорим о различных свойствах метрик. Мы уже показали их вам, но сейчас нужно выяснить, что они значат. Метрика может иметь четыре разных свойства.

* Metric Name — это имя метрики. Это уникальный идентификатор метрики в кластере с точки зрения диспетчера Resource Manager.
* Загрузка по умолчанию представляется по-разному в зависимости от того, какой является служба: с отслеживанием или без отслеживания состояния.
  * Для служб без отслеживания состояния у каждой метрики есть только одно свойство — Default Load.
  * Для служб с отслеживанием состояния можно определить следующие свойства:
    * PrimaryDefaultLoad — объем нагрузки по умолчанию, которую служба будет производить для этой метрики в качестве первичной реплики.
    * SecondaryDefaultLoad — объем нагрузки по умолчанию, которую служба будет производить для этой метрики в качестве вторичной реплики.
* Weight — это вес метрики по сравнению с другими настроенными метриками для этой службы.

## загрузить
Нагрузка — это общее понятие, которое обозначает количество этой метрики, используемое экземпляром или репликой службы на заданном узле.

## Нагрузка по умолчанию
Нагрузка по умолчанию — это объем нагрузки, который по предположению Cluster Resource Manager будет потреблять каждый экземпляр или каждая реплика этой службы до получения обновлений от фактических экземпляров или реплик службы. Для простых служб это будет статическое определение, которое никогда не обновляется динамически и поэтому используется в течение всего времени существования службы. Это отлично работает для простого планирования ресурсов, так как именно это мы обычно делаем — выделяем конкретные ресурсы для конкретных рабочих нагрузок. Но преимуществом является то, что теперь мы работаем с категорией микрослужб, в которой ресурсы на самом деле не назначаются статически для конкретных рабочих нагрузок, а человеческий фактор вообще не входит в цикл принятия решений.

Мы позволяем службам с отслеживанием состояния указывать нагрузки по умолчанию для метрик "Первичные реплики" и "Вторичные реплики". В действительности для многих служб эти числа не совпадают, так как первичные и вторичные реплики выполняют различные рабочие нагрузки. А так как первичные реплики обычно служат и для чтения, и для записи, а также берут на себя большую часть вычислительной нагрузки, нагрузка по умолчанию для первичных реплик превышает нагрузку по умолчанию для вторичных реплик.

Но предположим, что вы запустили свою службу и через какое-то время заметили, что некоторые экземпляры или реплики службы используют больше ресурсов, чем другие, или что со временем их потребление изменяется. Возможно, они связаны с определенным клиентом. Возможно, эти изменения соответствуют рабочим нагрузкам, которые изменяются в течение всего дня, как, например, трафик обмена сообщениями, телефонные звонки или торги на бирже. В любом случае вы заметите, что не существует "единого числа", которое можно использовать для загрузки и при этом сильно не ошибиться, по крайней мере, на какое-то время. Вы заметите также, что ошибка в первоначальной оценке приведет к тому, что диспетчер кластерных ресурсов выделит больше или меньше ресурсов для вашей службы. Следовательно, использование некоторых узлов будет чрезмерным или недостаточным.

Что делать? Можно сделать так, чтобы ваша служба сообщала о нагрузке в режиме реального времени!

## Динамическая нагрузка
Отчеты о динамической нагрузке позволяют репликам или экземплярам во время своего существования регулировать заявленное или реальное использование метрик в кластере. Реплики или экземпляры службы, которые простаивали и не выполняли никакой работы, обычно сообщают, что они использовали меньше заданной метрики. В то же время загруженные реплики или экземпляры сообщают, что они используют больше ресурсов. Этот общий уровень оттока нагрузки в кластере позволяет нам реорганизовать реплики и экземпляры службы в кластере "на лету", чтобы они получали необходимые им ресурсы. То есть мы можем сделать так, чтобы загруженные службы отзывали ресурсы у других реплик и экземпляров, которые простаивают или выполняют меньше работы. Отчеты о нагрузке можно создавать "на лету" с помощью метода ReportLoad, который доступен для свойства ServicePartition базового класса StatefulService или StatelessService посредством модели программирования Reliable Services. В коде вашей службы это будет выглядеть так:

Код:

```csharp
this.ServicePartition.ReportLoad(new List<LoadMetric> { new LoadMetric("Memory", 1234), new LoadMetric("metric1", 42) });
```

Реплики или экземпляры службы могут сообщать о нагрузке только тех метрик, которые настроены для использования. Список метрик задается во время создания каждой службы. Позднее его можно будет изменить. Если реплика или экземпляр пытается сообщить о нагрузке метрики, которая не настроена для использования, Service Fabric заносит отчет в журнал, но игнорирует его. Это означает, что метрика не будет использоваться во время вычисления или создания отчетов о состоянии кластера. Это чудесно, так как такое поведение позволяет больше экспериментировать. Код может измерять любимые метрики и сообщать обо всех метриках, которые он умеет измерять и о которых он умеет сообщать. А оператор может настраивать, поправлять и обновлять ресурс, изменяя правила для службы в режиме реального времени без изменения кода. Эти изменения могут включать, например, отключение метрик с отчетом с ошибками, перенастройку весов метрик на основе поведения или включение новой метрики только после кода развертывания и его проверки с помощью других механизмов.

## Совместное использование значений нагрузки по умолчанию и отчетов о динамической загрузке
Имеет ли смысл указывать нагрузку по умолчанию для службы, которая будет сообщать о нагрузке динамически? Конечно. В этом случае заданная по умолчанию нагрузка используется в качестве приблизительной оценки до тех пор, пока не начинают отображаться реальные отчеты из фактической реплики или экземпляра службы. Это замечательно, так как диспетчер кластерных ресурсов уже может с чем-то работать. Оценка нагрузки по умолчанию позволяет с самого начала оптимально разместить экземпляры или реплики службы. Без сведений о загрузке по умолчанию размещение создаваемых служб фактически осуществляется произвольно, и если позже загрузки изменятся, то диспетчеру кластерных ресурсов почти наверняка придется перемещать ресурсы.

Возьмем наш предыдущий пример и посмотрим, что произойдет, если мы добавим пользовательскую нагрузку, которая после создания службы будет динамически обновляться. В этом примере мы будем использовать в качестве примера метрику "Память". Предположим, мы создали службу с отслеживанием состояния с помощью следующей команды.

PowerShell:

```posh
New-ServiceFabricService -ApplicationName $applicationName -ServiceName $serviceName -ServiceTypeName $serviceTypeName –Stateful -MinReplicaSetSize 2 -TargetReplicaSetSize 3 -PartitionSchemeSingleton –Metric @("Memory,High,21,11”,"PrimaryCount,Medium,1,0”,"ReplicaCount,Low,1,1”,"Count,Low,1,1”)
```

Мы говорили об этом синтаксисе ранее (MetricName, MetricWeight, PrimaryDefaultLoad, SecondaryDefaultLoad). Но конкретное значение метрики Weight подробнее обсудим позже.

Один из возможных макетов кластера может выглядеть как:

![Сбалансированный кластер с метриками по умолчанию и пользовательскими метриками][Image2]

Обратите внимание на следующие факторы.

* Так как реплики и экземпляры используют нагрузку службы по умолчанию, пока они не сообщат о своей собственной нагрузке, мы знаем, что реплики внутри раздела 1 службы с отслеживанием состояния не сообщали о нагрузках сами.
* Вторичные реплики в разделе могут иметь собственную нагрузку.
* В целом метрики считаются хорошими, если разница между максимальной и минимальной нагрузкой на узле (речь идет о пользовательских метриках, которые нас больше всего интересуют) составляет соотношение 1,75 (узел с максимальной нагрузкой для памяти N3, с минимальной — N2, а 28/16 = 1,75). Это означает, что нагрузка в узле сбалансирована правильно.

Но некоторые факторы все же необходимо объяснить.

* Как определить, является ли соотношение 1,75 разумным? Как узнать, достаточно ли этого соотношения или нужно еще что-то делать?
* Когда происходит балансировка нагрузки?
* Что означает, если вес метрики "Память" определен как высокий?

## Вес метрик
Вес метрик — это свойство, которое позволяет двум службам сообщать одну и ту же метрику и одновременно по-разному трактовать важность балансировки нагрузки этой метрики. Например, рассмотрим механизм выполняющейся в памяти аналитики и постоянную базу данных. Вероятно, для обеих служб важна метрика "Память", но для выполняющейся в памяти службы, скорее всего, не имеет значения метрика "Диск". Такая служба может использовать данную метрику, но она не влияет на производительность службы, и служба может даже не создавать отчеты об этой метрике. Возможность отслеживать одни и те же метрики в разных службах очень удобна, так как она позволяет диспетчеру кластерных ресурсов отслеживать реальное потребление ресурсов в кластере, следить за тем, чтобы узлы не использовали больше ресурсов, чем запланировано, и т. д.

Веса метрик позволяют также диспетчеру кластерных ресурсов принимать решение о способе балансировки кластера, когда нет идеального решения (а такое случается очень часто). Метрики могут иметь четыре разных уровня веса: нулевой, низкий, средний и высокий. Метрики с нулевым весом не играют роли для балансировки нагрузки, но их нагрузка имеет значение для измерения производительности.

Так как в кластере есть метрики с разными весами, то для служб действуют разные схемы, потому что диспетчеру кластерных ресурсов было указано, что в совокупности одни метрики важнее, чем другие. Так как ему это известно, то когда метрики с разными весами конфликтуют между собой, диспетчер кластерных ресурсов может выбрать решения, которые лучше балансируют метрики с большими весами.

Рассмотрим простой пример нескольких отчетов нагрузки и влияния разных уровней веса метрик на выделение ресурсов в кластере. В этом примере мы видим, что переключение относительного веса метрики влияет на то, как диспетчер Resource Manager выбирает различные решения путем различного размещения служб.

![Пример веса метрики и его влияния на решения балансировки нагрузки][Image3]

В этом примере есть четыре разные службы. Все они сообщают разные значения для двух метрик, А и Б. В первом случае все службы считают метрику А важной (высокий вес), а метрику Б — относительно незначительной (низкий вес). И мы действительно видим, что диспетчер кластерных ресурсов размещает службы таким образом, чтобы нагрузка на метрику А была лучше сбалансирована (имела меньшие отклонения), чем нагрузка на метрику Б. Во втором случае мы меняем местами веса метрик и видим, что диспетчер кластерных ресурсов, вероятно, поменяет местами службы А и Б, чтобы предложить выделение, в котором для метрики Б нагрузка сбалансирована лучше, чем для метрики А.

### Глобальный вес метрик
Итак, если служба А считает метрику А более важной, а для службы Б эта метрика вообще не имеет значения, то какой вес будет использоваться в конечном итоге?

На самом деле мы принимаем во внимание два веса каждой метрики — вес, который определяет сама служба, и глобальный средний вес метрики во всех службах, для которых эта метрика имеет значение. Мы используем оба эти значения при вычислении результатов решения, которое мы создаем, так как для нас важно обеспечить и балансировку нагрузки в службе согласно ее приоритетам, и правильное выделение ресурсов в кластере в целом.

Что произойдет, если мы не будем учитывать и глобальный, и локальный баланс? Конечно, очень просто создать решения с глобальной балансировкой нагрузки, но это приводит к неправильным балансировке и выделению ресурсов для отдельных служб. В следующем примере рассмотрим метрики по умолчанию, которые настроены для службы с отслеживанием состояния (PrimaryCount, ReplicaCount и Count), и увидим, что происходит, если мы учитываем только глобальную балансировку нагрузки.

![Результаты решения только глобальной балансировки][Image4]

В примере выше, в котором нас интересовала только глобальная балансировка нагрузки, кластер в целом действительно сбалансирован — все узлы имеют одинаковое количество первичных реплик и реплик вообще. Однако, если взглянуть на результаты такого выделения ресурсов, окажется, что оно не совсем правильное. Потеря одного узла непропорционально влияет на конкретную рабочую нагрузку, так как вместе с ним теряются все его первичные реплики. Например, представим, что мы потеряли первый узел. Если это произойдет, будут потеряны одновременно три первичные реплики для трех разных разделов службы "Круг". И, наоборот, другие две службы ("Треугольник" и "Шестиугольник") теряют реплики своих разделов и это не нарушает работу (если не учитывать, что нужно восстановить потерянную реплику).

В примере ниже мы распределили реплики с учетом глобального баланса и баланса каждой службы. При вычислении результата решения мы отдаем большую часть веса глобальному решению, но определенная часть (она может меняться) выделяется на обеспечение оптимальной балансировки нагрузки в самих службах. В результате, если мы потеряем тот же первый узел, мы увидим, что потеря первичных (и вторичных) реплик распределяется на все разделы всех служб и влияет на всех одинаково.

Учитывая веса метрик, глобальный баланс вычисляется на основе среднего веса метрик, настроенных для каждой службы. Мы сбалансировали службу с учетом определенных ею весов метрик.

## Дальнейшие действия
* Дополнительные сведения о других вариантах настройки служб см. в статье [Настройка параметров диспетчера кластерных ресурсов для служб Service Fabric](service-fabric-cluster-resource-manager-configure-services.md).
* Определение метрик дефрагментации — один из способов объединения нагрузки на узлах вместо ее рассредоточения. Сведения о настройке дефрагментации см. в [этой статье](service-fabric-cluster-resource-manager-defragmentation-metrics.md).
* Чтобы узнать, как диспетчер кластерных ресурсов управляет нагрузкой кластера и балансирует ее, ознакомьтесь со статьей о [балансировке нагрузки](service-fabric-cluster-resource-manager-balancing.md).
* Начните с самого начала, [изучив общие сведения о диспетчере кластерных ресурсов Service Fabric](service-fabric-cluster-resource-manager-introduction.md).
* Стоимость перемещения — один из способов сообщить диспетчеру кластерных ресурсов, что некоторые службы перемещать затратнее, чем остальные. Чтобы больше узнать о стоимости перемещения, см. [эту статью](service-fabric-cluster-resource-manager-movement-cost.md).

[Image1]: ./media/service-fabric-cluster-resource-manager-metrics/cluster-resource-manager-cluster-layout-with-default-metrics.png
[Image2]: ./media/service-fabric-cluster-resource-manager-metrics/Service-Fabric-Resource-Manager-Dynamic-Load-Reports.png
[Image3]: ./media/service-fabric-cluster-resource-manager-metrics/cluster-resource-manager-metric-weights-impact.png
[Image4]: ./media/service-fabric-cluster-resource-manager-metrics/cluster-resource-manager-global-vs-local-balancing.png

<!---HONumber=AcomDC_0824_2016-->