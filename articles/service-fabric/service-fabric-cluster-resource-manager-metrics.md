<properties
   pageTitle="Управление метриками с помощью диспетчера кластерных ресурсов Azure Service Fabric | Microsoft Azure"
   description="Узнайте, как настраивать и использовать метрики в Service Fabric."
   services="service-fabric"
   documentationCenter=".net"
   authors="masnider"
   manager="timlt"
   editor=""/>

<tags
   ms.service="Service-Fabric"
   ms.devlang="dotnet"
   ms.topic="article"
   ms.tgt_pltfrm="NA"
   ms.workload="NA"
   ms.date="05/20/2016"
   ms.author="masnider"/>

# Управление потреблением ресурсов и нагрузкой в Service Fabric с помощью метрик
Метрики — общий термин в Service Fabric для ресурсов, которые нужны вашим службам. Вообще метрика — это все параметры ресурса, которые вы хотите контролировать, чтобы управлять производительностью своих служб.

Во всех приведенных выше примерах мы неявно ссылаемся на метрики. Такие параметры, как память, диск, ЦП — все это примеры метрик. Это физические метрики, т. е. ресурсы, соответствующие физическим ресурсам на узле, которыми нужно управлять. Метрики также могут быть логическими. Например, такие параметры, как MyWorkQueueDepth, определяются в приложении и соответствуют определенному уровню потребления ресурсов (само приложение не знает, в каком объеме оно потребляет ресурсы, или же не знает, как это потребление измерить). На самом деле большинство встречающихся нам метрик являются логическими метриками. Этому есть несколько причин, среди которых можно выделить две самые распространенные. Первая — многие наши клиенты пишут свои службы в управляемом коде. Вторая — в рамках конкретного экземпляра службы без сохранения состояния или объекта реплики службы с отслеживанием состояния трудно измерить и представить показатели потребления фактических физических ресурсов. Именно из-за сложности создания отчетов о собственных метриках мы предоставляем готовые к использованию метрики.

## Метрики по умолчанию
Предположим, вы хотите приступить к работе и не знаете, какие ресурсы вы будете использовать, или даже не знаете, какие ресурсы вообще вам нужны. Поэтому вы переходите к реализации и создаете службы без указания метрик. Все в порядке. Мы выберем метрики для вас. Если вы не указали собственные метрики, мы по умолчанию используем для вас сегодня следующие метрики: PrimaryCount (первичное количество), ReplicaCount (количество реплик) и (звучит немного туманно, мы понимаем) Count (количество). В следующей таблице показана связь нагрузки для каждой метрики с соответствующими объектами службы.

| Metric (Метрика) | Нагрузка экземпляра без отслеживания состояния |	Вторичная нагрузка с отслеживанием состояния |	Первичная нагрузка с отслеживанием состояния |
|--------|--------------------------|-------------------------|-----------------------|
| PrimaryCount | 0 |	0 |	1 |
| ReplicaCount | 0 | 1 | 1 |
| Count |	1 |	1 |	1 |

Итак, что же вы получаете с помощью этих используемых по умолчанию метрик? Оказывается, что работа правильно распределяется между основными рабочими нагрузками. На примере ниже вы можете увидеть, что произойдет, если мы создадим одну службу с отслеживанием состояния с тремя разделами, укажем для размера набора целевых реплик значение "3", а также создадим единую службу без отслеживания состояния с количеством экземпляров "3". Получится что-то вроде этого.

![Макет кластера с метриками по умолчанию][Image1]

В этом примере мы видим следующие результаты.
-	Первичные реплики для службы с отслеживанием состояния не расположены на одном узле.
-	Реплики одного раздела не находятся на одном и том же узле.
-	Общее число первичных и вторичных реплик правильно распределено в кластере.
-	Общее количество объектов службы (без отслеживания состояния и с отслеживанием состояния) равномерно выделено на каждом узле.

Неплохо!

Все будет работать отлично, пока у вас не возникнет вопрос: какова вероятность того, что выбранная схема секционирования со временем приведет к идеально равномерному распределению всех узлов? И еще один вопрос: какова вероятность того, что данная служба испытывает одинаковую нагрузку все время или даже прямо сейчас? Оказывается, что для относительно серьезной нагрузки эта вероятность будет мала. Поэтому если вы стремитесь максимально эффективно использовать кластер, возможно, стоит создать пользовательскую метрику.

На самом деле вы можете спокойно работать с метриками по умолчанию или по крайней мере со статическими пользовательскими метриками. Но обычно это приводит к тому, что кластер используется меньше, чем хотелось бы (так как создание отчетов не является адаптивным). В худшем случае это может привести к неправильному планированию узлов, что, в свою очередь, может вызвать проблемы с производительностью. Лучше использовать пользовательские метрики с отчетами о динамической нагрузке.

Теперь перейдем к следующим разделам, посвященным пользовательским метрикам и динамическим нагрузкам.

## Настраиваемые метрики
Мы выяснили, что метрики могут быть физическими и логическими, а пользователи могут определять свои собственные метрики. Но как? На самом деле это довольно просто! Достаточно при создании службы настроить метрику и используемую по умолчанию начальную нагрузку. Любой набор метрик и значения по умолчанию можно настроить отдельно для каждого именованного экземпляра службы во время создания службы.

Обратите внимание, что если вы начали определять пользовательские метрики, нужно явно добавить их к метрикам по умолчанию, если вы хотите, чтобы они (пользовательские метрики) тоже использовались для балансировки нагрузки в службе. Вот почему нам необходимо иметь четкое представление о связи между метриками по умолчанию и пользовательскими метриками: возможно, для вас важнее больше интересует, как потребляется память или какова длина рабочей очереди, чем распределение нагрузки в первичной реплике.

Предположим, вы хотите настроить службу, которая выдает значение метрики с именем "Память" (в дополнение к метрике по умолчанию). Предположим, вы провели основные измерения и выяснили, что обычно первичная реплика этой службы может занимать 20 МБ памяти, хотя вторичные реплики той же службы занимают 5 МБ. Вы знаете, что "Память" является наиболее важной метрикой с точки зрения управления производительностью данной службы. Но вы все равно хотите, чтобы нагрузка была сбалансирована между основными репликами и чтобы в результате потери узла или домена сбоя не пропадало слишком большое количество первичных реплик. В остальных случаях вы будет использовать значения по умолчанию.

Сделать нужно следующее.

Код:

```csharp
StatefulServiceDescription serviceDescription = new StatefulServiceDescription();
StatefulServiceLoadMetricDescription memoryMetric = new StatefulServiceLoadMetricDescription();
memoryMetric.Name = "MemoryInMb";
memoryMetric.PrimaryDefaultLoad = 20;
memoryMetric.SecondaryDefaultLoad = 5;
memoryMetric.Weight = ServiceLoadMetricWeight.High;

StatefulServiceLoadMetricDescription primaryCountMetric = new StatefulServiceLoadMetricDescription();
primaryCountMetric.Name = "PrimaryCount";
primaryCountMetric.PrimaryDefaultLoad = 1;
primaryCountMetric.SecondaryDefaultLoad = 0;
primaryCountMetric.Weight = ServiceLoadMetricWeight.Medium;

StatefulServiceLoadMetricDescription replicaCountMetric = new StatefulServiceLoadMetricDescription();
replicaCountMetric.Name = "ReplicaCount";
replicaCountMetric.PrimaryDefaultLoad = 1;
replicaCountMetric.SecondaryDefaultLoad = 1;
replicaCountMetric.Weight = ServiceLoadMetricWeight.Low;

StatefulServiceLoadMetricDescription totalCountMetric = new StatefulServiceLoadMetricDescription();
totalCountMetric.Name = "Count";
totalCountMetric.PrimaryDefaultLoad = 1;
totalCountMetric.SecondaryDefaultLoad = 1;
totalCountMetric.Weight = ServiceLoadMetricWeight.Low;

serviceDescription.Metrics.Add(memoryMetric);
serviceDescription.Metrics.Add(primaryCountMetric);
serviceDescription.Metrics.Add(replicaCountMetric);
serviceDescription.Metrics.Add(totalCountMetric);

await fabricClient.ServiceManager.CreateServiceAsync(serviceDescription);
```

PowerShell:

```posh
New-ServiceFabricService -ApplicationName $applicationName -ServiceName $serviceName -ServiceTypeName $serviceTypeName –Stateful -MinReplicaSetSize 2 -TargetReplicaSetSize 3 -PartitionSchemeSingleton –Metric @("Memory,High,20,5”,"PrimaryCount,Medium,1,0”,"ReplicaCount,Low,1,1”,"Count,Low,1,1”)
```

(Напоминаем, если вы хотите использовать метрики по умолчанию, то вам вообще не нужно ничего менять в коллекции метрик).

Теперь, когда вы знаете, как определить собственные метрики, поговорим о различных свойствах метрик. Мы уже показали их вам, но сейчас нужно выяснить, что они значат. Метрика может иметь четыре разных свойства.

-	Metric Name — это имя метрики. Это уникальный идентификатор метрики в кластере с точки зрения диспетчера Resource Manager.
-	PrimaryDefaultLoad — объем нагрузки по умолчанию, которую служба будет производить для этой метрики в качестве первичной реплики.
-	SecondaryDefaultLoad — объем нагрузки по умолчанию, которую служба будет производить для этой метрики в качестве вторичной реплики.
-	Weight — это вес метрики по сравнению с другими настроенными метриками для этой службы.

## Загрузка
Нагрузка — это общее понятие, которое обозначает количество этой метрики, используемое экземпляром или репликой службы на заданном узле.

## Нагрузка по умолчанию
Нагрузка по умолчанию — это объем нагрузки, который по предположению Cluster Resource Manager будет потреблять каждый экземпляр или каждая реплика этой службы до получения обновлений от фактических экземпляров или реплик службы. Для простых служб это будет статическое определение, которое никогда не обновляется динамически и поэтому используется в течение всего времени существования службы. Это отлично работает для простого планирования ресурсов, так как именно это мы обычно делаем — выделяем конкретные ресурсы для конкретных рабочих нагрузок. Но преимуществом является то, что теперь мы работаем с категорией микрослужб, в которой ресурсы на самом деле не назначаются статически для конкретных рабочих нагрузок, а человеческий фактор вообще не входит в цикл принятия решений.

Мы позволяем службам с отслеживанием состояния указывать нагрузки по умолчанию для метрик "Первичные реплики" и "Вторичные реплики". В действительности для многих служб эти числа не совпадают, так как первичные и вторичные реплики выполняют различные рабочие нагрузки. А так как первичные реплики обычно служат и для чтения, и для записи, а также берут на себя большую часть вычислительной нагрузки, нагрузка по умолчанию для первичных реплик превышает нагрузку по умолчанию для вторичных реплик.

Но предположим, вы ненадолго запустили свою службу и заметили, что некоторые экземпляры или реплики службы используют больше ресурсов, чем другие, или что со временем их потребление изменяется. Возможно, они связаны с определенным клиентом. Возможно, эти изменения соответствуют рабочим нагрузкам, которые изменяются в течение всего дня, как, например, трафик обмена сообщениями или торги на бирже. В любом случае вы заметите, что не существует "единого числа", которое можно использовать для загрузки и при этом сильно не ошибиться. Вы также заметите, что ошибка в первоначальной оценке приведет к тому, что Service Fabric выделит больше или меньше ресурсов для вашей службы. Следовательно, использование узлов будет чрезмерным или недостаточным. Что делать? Можно сделать так, чтобы ваша служба сообщала о нагрузке в режиме реального времени!

## Динамическая нагрузка
Отчеты о динамической нагрузке позволяют репликам или экземплярам со временем регулировать заявленное или реальное использование метрик в кластере. Реплики или экземпляры службы, которые простаивали и не выполняли никакой работы, обычно сообщают, что они использовали меньше ресурсов. В то же время занятые реплики или экземпляры сообщают, что они используют больше ресурсов. Этот общий уровень обработки в кластере позволяет нам реорганизовать реплики и экземпляры службы в кластере в режиме реального времени, чтобы реплики и экземпляры получали необходимые им ресурсы. Т. е. мы можем сделать так, чтобы занятые службы отзывали ресурсы у других реплик и экземпляров, которые простаивают или выполняют меньше работы. Отчеты о нагрузке в режиме реального времени можно создавать с помощью метода ReportLoad, который доступен для свойства ServicePartition (свойство базового класса StatefulService). В коде вашей службы это будет выглядеть так:

Код:

```csharp
this.ServicePartition.ReportLoad(new List<LoadMetric> { new LoadMetric("Memory", 1234), new LoadMetric("metric1", 42) });
```

Реплики или экземпляры службы могут сообщать о нагрузке только тех метрик, которые настроены для использования. Список метрик задается во время создания каждой службы. Если реплика или экземпляр пытается сообщить о нагрузке метрики, которая не настроена для использования, Service Fabric заносит отчет в журнал, но игнорирует его. Это означает, что метрика не будет использоваться во время вычисления или создания отчетов о состоянии кластера. Это чудесно, так как такое поведение позволяет больше экспериментировать. Код может измерять любимые метрики и сообщать обо всех метриках, которые он умеет измерять и о которых он умеет сообщать. А оператор может настраивать, поправлять и обновлять ресурс, изменяя правила для службы в режиме реального времени без изменения кода. Эти изменения могут включать, например, отключение метрик с отчетом, содержащим ошибки, перенастройку веса метрик на основе поведения или включение новой метрики только после развертывания и проверки кода.

## Совместное использование значений нагрузки по умолчанию и отчетов о динамической загрузке
Имеет ли смысл указывать нагрузку по умолчанию для службы, которая будет сообщать о нагрузке динамически? Конечно. В этом случае заданная по умолчанию нагрузка используется в качестве приблизительной оценки до тех пор, пока не начинают отображаться реальные отчеты из фактической реплики или экземпляра службы. Это очень удобно, так как во время создания диспетчеру Resource Manager есть с чем работать при размещении реплики или экземпляра. Нагрузка по умолчанию заканчивается первоначальной оценкой, что позволяет диспетчеру Resource Manager поместить экземпляры или реплики службы в правильные места с самого начала. Если не указаны никакие сведения, размещение фактически будет случайным и почти наверняка объекты будут перемещаться, когда начнут поступать реальные отчеты о нагрузках.

Возьмем наш предыдущий пример и посмотрим, что произойдет, если мы добавим пользовательскую нагрузку, которая после создания службы будет динамически обновляться. В этом примере мы будем использовать в качестве примера метрику "Память". Предположим, мы создали службу с отслеживанием состояния с помощью следующей команды.

PowerShell:

```posh
New-ServiceFabricService -ApplicationName $applicationName -ServiceName $serviceName -ServiceTypeName $serviceTypeName –Stateful -MinReplicaSetSize 2 -TargetReplicaSetSize 3 -PartitionSchemeSingleton –Metric @("Memory,High,21,11”,"PrimaryCount,Medium,1,0”,"ReplicaCount,Low,1,1”,"Count,Low,1,1”)
```

Мы говорили об этом синтаксисе ранее (MetricName, MetricWeight, PrimaryDefaultLoad, SecondaryDefaultLoad). Но конкретное значение метрики Weight подробнее обсудим позже.

Один из возможных макетов кластера может выглядеть как:

![Сбалансированный кластер с метриками по умолчанию и пользовательскими метриками][Image2]

Обратите внимание на следующие факторы.

-	Так как реплики и экземпляры используют нагрузку службы по умолчанию, пока они не сообщат о своей собственной нагрузке, мы знаем, что реплики внутри раздела 1 службы с отслеживанием состояния не сообщали о нагрузках сами.
-	Вторичные реплики в разделе могут иметь собственную нагрузку.
-	В целом метрики считаются хорошими, если разница между максимальной и минимальной нагрузкой на узле (речь идет о пользовательских метриках, которые нас больше всего интересуют) составляет соотношение 1,75 (узел с максимальной нагрузкой для памяти N3, с минимальной — N2, а 28/16 = 1,75). Это означает, что нагрузка в узле сбалансирована правильно.

Но некоторые факторы все же необходимо объяснить.

-	Как определить, является ли соотношение 1,75 разумным? Как узнать, достаточно ли этого соотношения или нужно еще что-то делать?
-	Когда происходит балансировка нагрузки?
-	Что означает, если вес метрики "Память" определен как высокий?

## Вес метрик
Вес метрик — это свойство, которое позволяет двум службам сообщать одну и ту же метрику и одновременно по-разному трактовать важность балансировки нагрузки этой метрики. Например, рассмотрим механизм выполняющейся в памяти аналитики и постоянную базу данных. Вероятно, для обеих служб важна метрика "Память", но для службы в памяти, вероятно, не имеет значения метрика "Диск" — служба может использовать эту метрику, но она не влияет на производительность службы. Возможность отслеживать одни и те же метрики в разных службах очень удобна, так как она позволяет диспетчеру Resource Manager отслеживать реальное потребление ресурсов в кластере, следить за тем, чтобы узлы не использовали больше ресурсов, чем запланировано, и т. д.

Вес метрик позволяет диспетчеру Resource Manager принимать решение о способе балансировки нагрузки в кластере, когда нет идеального ответа (а такое случается очень часто). Метрики могут иметь четыре разных уровня веса: нулевой, низкий, средний и высокий. Метрики с нулевым весом не играют роли для балансировки нагрузки, но их нагрузка имеет значение для измерения производительности.

Так как в кластере есть метрики с разным весом, службы в кластере физически по-разному размещены, потому что диспетчер Resource Manager "знает", что обеспечение одних метрик важнее, чем других, и, следовательно, их нагрузку нужно балансировать в первую очередь, если они конфликтуют с другой менее приоритетной метрикой.

Рассмотрим простой пример нескольких отчетов нагрузки и влияния разных уровней веса метрик на выделение ресурсов в кластере. В этом примере мы видим, что переключение относительного веса метрики влияет на то, как диспетчер Resource Manager выбирает различные решения путем различного размещения служб.

![Пример веса метрики и его влияния на решения балансировки нагрузки][Image3]

В этом примере есть четыре разные службы. Все они сообщают разные значения для двух разных метрик, А и Б. В первом случае все службы считают метрику А важной (вес = высокий), а метрику Б — относительно незначительной (вес = низкий). И мы действительно видим, что диспетчер Resource Manager кластера Service Fabric размещает службы таким образом, чтобы нагрузка на метрику А была лучше сбалансирована (имела меньшие отклонения), чем нагрузка на метрику Б. Во втором случае мы меняем вес метрик и видим, что диспетчер Resource Manager, вероятно, поменяет местами службы А и Б, чтобы предложить выделение, в котором для метрики Б нагрузка сбалансирована лучше, чем для метрики А.

### Глобальный вес метрик
Итак, если служба А считает метрику А более важной, а для службы Б эта метрика вообще не имеет значения, какой вес будет применять диспетчер Resource Manager?

На самом деле мы принимаем во внимание два веса каждой метрики — вес, который определяет сама служба, и глобальный средний вес метрики во всех службах, для которых эта метрика имеет значение. Мы используем оба эти значения при вычислении результатов решения, которое мы создаем, так как для нас важно обеспечить и балансировку нагрузки в службе согласно ее приоритетам, и правильное выделение ресурсов в кластере в целом.

Что произойдет, если мы не будем учитывать и глобальный, и локальный баланс? Конечно, очень просто создать решения с глобальной балансировкой нагрузки, но это приводит к неправильным балансировке и выделению ресурсов для отдельных служб. В следующем примере рассмотрим метрики по умолчанию, которые настроены для службы с отслеживанием состояния (PrimaryCount, ReplicaCount и TotalCount), и увидим, что происходит, если мы учитываем только глобальную балансировку нагрузки.

![Результаты решения только глобальной балансировки][Image4]

В примере выше, в котором нас интересовала только глобальная балансировка нагрузки, кластер в целом действительно сбалансирован — все узлы имеют одинаковое количество первичных реплик и реплик вообще. Отлично! Все сбалансировано. Однако, если взглянуть на результаты такого выделения ресурсов, окажется, что оно не совсем правильное. Потеря одного узла непропорционально влияет на конкретную рабочую нагрузку, так как вместе с ним теряются все его первичные реплики. Например, представим, что мы потеряли первый узел. Если это произойдет, будут потеряны одновременно три первичные реплики для трех разных разделов службы "Круг". И, наоборот, другие две службы ("Треугольник" и "Шестиугольник") теряют реплики своих разделов и это не нарушает работу (если не учитывать, что нужно восстановить потерянную реплику).

В примере ниже мы распределили реплики с учетом глобального баланса и баланса каждой службы. При вычислении результата решения мы отдаем большую часть веса глобальному решению, но определенная часть (она может меняться) выделяется на обеспечение оптимальной балансировки нагрузки в самих службах. В результате, если мы потеряем тот же первый узел, мы увидим, что потеря первичных (и вторичных) реплик распределяется на все разделы всех служб и влияет на всех одинаково.

Учитывая веса метрик, глобальный баланс вычисляется на основе среднего веса. Мы сбалансировали службу с учетом определенных ею весов метрик.

## Дальнейшие действия
- Дополнительные сведения о других вариантах настройки служб см. в разделе о других конфигурациях Cluster Resource Manager в статье [Настройка параметров диспетчера кластерных ресурсов для служб Service Fabric](service-fabric-cluster-resource-manager-configure-services.md).
- Определение метрик дефрагментации — один из способов объединения нагрузки на узлах вместо ее рассредоточения. Сведения о настройке дефрагментации см. в [этой статье](service-fabric-cluster-resource-manager-defragmentation-metrics.md).
- Чтобы узнать, как диспетчер кластерных ресурсов управляет нагрузкой кластера и балансирует ее, ознакомьтесь со статьей о [балансировке нагрузки](service-fabric-cluster-resource-manager-balancing.md).
- Начните с самого начала, [изучив общие сведения о диспетчере кластерных ресурсов Service Fabric](service-fabric-cluster-resource-manager-introduction.md).
- Стоимость перемещения — один из способов сообщить диспетчеру кластерных ресурсов, что некоторые службы перемещать затратнее, чем остальные. Чтобы больше узнать о стоимости перемещения, см. [эту статью](service-fabric-cluster-resource-manager-movement-cost.md).

[Image1]: ./media/service-fabric-cluster-resource-manager-metrics/cluster-resource-manager-cluster-layout-with-default-metrics.png
[Image2]: ./media/service-fabric-cluster-resource-manager-metrics/Service-Fabric-Resource-Manager-Dynamic-Load-Reports.png
[Image3]: ./media/service-fabric-cluster-resource-manager-metrics/cluster-resource-manager-metric-weights-impact.png
[Image4]: ./media/service-fabric-cluster-resource-manager-metrics/cluster-resource-manager-global-vs-local-balancing.png

<!---HONumber=AcomDC_0525_2016-->