---
title: "Распространенные вопросы о Microsoft Azure Service Fabric | Документация Майкрософт"
description: "Часто задаваемые вопросы о Service Fabric и ответы на них"
services: service-fabric
documentationcenter: .net
author: seanmck
manager: timlt
editor: 
ms.assetid: 5a179703-ff0c-4b8e-98cd-377253295d12
ms.service: service-fabric
ms.devlang: dotnet
ms.topic: article
ms.tgt_pltfrm: na
ms.workload: na
ms.date: 01/19/2017
ms.author: seanmck
translationtype: Human Translation
ms.sourcegitcommit: 102be620e8812cc551aebafe7c8df4e4eac0ae90
ms.openlocfilehash: 2ad3bd7b846693c637fd843383802651a619b128

---


# <a name="commonly-asked-service-fabric-questions"></a>Распространенные вопросы о Service Fabric

Пользователи задают множество схожих вопросов о возможностях и использовании Service Fabric. В этом документе приведены ответы на многие из этих вопросов.

## <a name="cluster-setup-and-management"></a>Настройка кластера и управление им

### <a name="can-i-create-a-cluster-that-spans-multiple-azure-regions"></a>Можно ли создать кластер, располагающийся в нескольких регионах Azure?

Сейчас это невозможно, но это популярный запрос, и мы продолжаем его изучать.

В базовых технологиях кластеризации Service Fabric нет никаких регионов Azure, и эти технологии можно использовать для объединения компьютеров, которые работают в любой точке мира, до тех пор, пока между ними есть сетевое подключение. Тем не менее ресурс кластера Service Fabric в Azure относится к определенному региону, как и масштабируемые наборы виртуальных машин, на основе которых создан кластер. Кроме того, существует врожденная проблема передачи строго согласованных данных репликации между компьютерами, расположенными далеко друг от друга. Прежде чем приступать к поддержке многорегиональных кластеров, мы стремимся обеспечить прогнозируемую приемлемую производительность.

### <a name="do-service-fabric-nodes-automatically-receive-os-updates"></a>Получают ли узлы Service Fabric обновления операционной системы автоматически?

Пока нет, но это популярный запрос, и мы планируем реализовать эту возможность.

Проблема с обновлениями ОС заключается в том, что для них обычно требуется перезагрузка компьютера, что приводит к временной потере доступности. Сама по себе это не проблема, так как Service Fabric выполнит автоматическое перенаправление трафика для этих служб на другие узлы. Тем не менее, если обновления ОС не согласованы между собой по всему кластеру, есть вероятность того, что множество узлов будут выключены одновременно. Такие одновременные перезагрузки могут привести к полной потере доступности службы или как минимум определенного раздела (для службы с отслеживанием состояния).

В будущем мы реализуем поддержку полностью автоматизированной политики обновления ОС, скоординированной между доменами обновления, которая гарантирует, что доступность сохранится несмотря на перезагрузки и другие непредвиденные сбои.

До этого времени можно воспользоваться [предоставленным нами сценарием](https://blogs.msdn.microsoft.com/azureservicefabric/2017/01/09/os-patching-for-vms-running-service-fabric/), с помощью которого администратор кластера может безопасным образом вручную устанавливать исправления на каждом узле.

### <a name="what-is-the-minimum-size-of-a-service-fabric-cluster-why-cant-it-be-smaller"></a>Каков минимальный размер кластера Service Fabric? Почему он не может быть меньше?

Минимальный поддерживаемый размер кластера Service Fabric с рабочими нагрузками — пять узлов. Для сценариев разработки и тестирования мы поддерживаем кластеры из трех узлов.

Эти минимальные значения связаны с тем, что в кластере Service Fabric выполняется ряд системных служб с отслеживанием состояния, включая службы имен и диспетчера отработки отказов. Эти службы, которые отслеживают, какие службы развернуты в кластере и где они в настоящее время размещены, существенно зависят от целостности кластера. Эта целостность, в свою очередь, зависит от возможности получения *кворума* для любого обновления состояния служб, где кворум означает строгое большинство реплик (N/2 +&1;) для данной службы.

С учетом этого давайте рассмотрим некоторые возможные конфигурации кластера.

**Один узел.** Этот вариант не обеспечивает высокой доступности, так как потеря одного узла по какой-либо причине означает потерю всего кластера.

**Два узла.** Кворум для службы, развернутой на двух узлах (N = 2), составляет 2 узла (2/2 + 1 = 2). При потере одной реплики кворум создать невозможно. Так как обновление службы требует временного отключения реплики, эта конфигурация бесполезна.

**Три узла.** В случае трех узлов (N = 3) для создания кворума по-прежнему нужно два узла (3/2 + 1 = 2). Это означает, что можно потерять один узел и по-прежнему сохранить кворум.

Кластер из трех узлов поддерживается для разработки и тестирования, так как в этом случае можно безопасно выполнять обновления и справляться со сбоями отдельных узлов, если они не происходят одновременно. Для рабочих нагрузок в производственной среде кластер должен быть устойчив к таким одновременным сбоям, поэтому пять узлов являются обязательными.

### <a name="can-i-turn-off-my-cluster-at-nightweekends-to-save-costs"></a>Можно ли отключить кластер на ночь или выходные дни для снижения расходов?

Как правило, нет. Service Fabric сохраняет состояние на локальных временных дисках, и это означает, что если виртуальная машина перемещается на другой узел, данные не перемещаются вместе с ней. Во время работы это не проблема, так как состояние нового узла обновляется с помощью других узлов. Однако если остановить все узлы и затем перезагрузить их, существует значительная вероятность, что большинство узлов будут запущены на новых серверах и восстановить систему не удастся.

Если нужно создать кластеры для тестирования приложения перед его развертыванием, мы рекомендуем динамически создавать кластеры в процессе [непрерывной интеграции и непрерывного развертывания](service-fabric-set-up-continuous-integration.md).

## <a name="application-design"></a>Проектирование приложений

### <a name="whats-the-best-way-to-query-data-across-partitions-of-a-reliable-collection"></a>Как лучше запрашивать данные из всех разделов Reliable Collection?

Reliable Collection обычно содержит несколько [разделов](service-fabric-concepts-partitioning.md) для обеспечения развертывания с целью повышения производительности и пропускной способности. Это означает, что состояние данной службы может быть распределено между десятками и сотнями компьютеров. Для выполнения операций над таким полным набором данных доступны такие возможности.

- Создать службу, которая направляет запросы во все разделы другой службы для извлечения необходимых данных.
- Создать службу, которая может получать данные из всех разделов другой службы.
- Периодически передавать данные из каждой службы во внешнее хранилище. Этот подход используется, только если выполняемые запросы не являются частью основной бизнес-логики.


### <a name="whats-the-best-way-to-query-data-across-my-actors"></a>Как лучше запрашивать данные из моих субъектов?

Субъекты должны быть независимыми единицами состояния и вычислений, поэтому не рекомендуется делать широковещательные запросы состояния субъектов во время выполнения. Если требуется направить запрос в полный набор состояния субъекта, необходимо выполнить одно из таких указаний.

- Заменить службы субъектов надежными службами с отслеживанием состояния таким образом, чтобы число сетевых запросов для сбора всех данных из нескольких субъектов выполнялось для нескольких разделов в службе.
- Разрабатывать субъекты таким образом, чтобы периодически передавать их состояние во внешнее хранилище для упрощения запросов. Как и способ выше, этот способ подходит, только если выполняемые запросы не являются обязательными для поведения среды выполнения.

### <a name="how-much-data-can-i-store-in-a-reliable-collection"></a>Сколько данных можно сохранить в Reliable Collection?

Reliable Services обычно состоит из нескольких разделов, поэтому объем данных, которые можно хранить, ограничивается только числом компьютеров в кластере и объемом доступной памяти на этих компьютерах.

Например, предположим, что есть Reliable Collection в службе со 100 разделами и 3 репликами для хранения объектов со средним размером 1 КБ. Теперь предположим, что имеется кластер из 10 компьютеров с 16 ГБ памяти на каждом компьютере. Чтобы обеспечить простоту и умеренность оценки, предположим, что операционная система и системные службы, среда выполнения Service Fabric и службы используют 6 ГБ, оставляя свободными 10 ГБ на каждом компьютере, или 100 ГБ на кластер.

Учитывая, что каждый объект должен быть сохранен трижды (основная копия и две реплики), памяти будет достаточно примерно для 35 миллионов объектов в коллекции при полной загрузке. Тем не менее рекомендуется обеспечить устойчивость к одновременной потере домена сбоя и домена обновления, для чего потребуется примерно треть емкости, и это количество может сократиться примерно до 23 миллионов объектов.

Обратите внимание, что в этой оценке также предполагается следующее.

- Распределение данных по разделам примерно равномерно, или метрики нагрузки сообщаются в диспетчер ресурсов кластера. По умолчанию Service Fabric будет распределять нагрузку, исходя из числа реплик. В приведенном выше примере можно поместить 10 основных реплик и 20 дополнительных реплик на каждом узле в кластере. Этот вариант хорошо подходит для нагрузки, равномерно распределенной по разделам. Если нагрузка распределена неравномерно, необходимо сообщить о нагрузке, чтобы диспетчер ресурсов мог упаковать реплики меньшего размера вместе и разрешить репликам большего размера использовать больше памяти на отдельном узле.

- Рассматриваемая надежная служба является единственной, которая сохраняет состояние в кластере. Так как в кластере можно развернуть несколько служб, необходимо учитывать ресурсы, которые потребуется запускать для каждой из них, и управлять состоянием этих служб.

- Сам кластер не увеличивается и не сжимается. При добавлении дополнительных компьютеров Service Fabric выполнит повторную балансировку реплик для эффективного использования дополнительной емкости, пока количество компьютеров не превысит количество разделов в службе, так как отдельная реплика не может находиться на нескольких компьютерах. Напротив, если уменьшить размер кластера, удалив компьютеры, реплики будут упакованы плотнее и будут обладать меньшей общей емкостью.

### <a name="how-much-data-can-i-store-in-an-actor"></a>Сколько данных можно хранить в субъекте?

Как и в случае с надежными службами, объем данных, которые могут храниться в службе субъекта, ограничивается только общим объемом дискового пространства и доступным объемом памяти на всех узлах в кластере. Тем не менее отдельные субъекты наиболее эффективны, когда они используются для инкапсуляции небольшого объема состояния и связанной бизнес-логики. Как правило, состояние отдельного субъекта должно измеряться в килобайтах.

## <a name="other-questions"></a>Другие вопросы

### <a name="how-does-service-fabric-relate-to-containers"></a>Как Service Fabric связана с контейнерами?

Контейнеры обеспечивают простой способ для упаковки служб и их зависимостей таким образом, чтобы они согласованно запускались во всех средах и могли изолированно работать на одном компьютере. Service Fabric предлагает способ развертывания служб и управления ими, включая [службы, упакованные в контейнер](service-fabric-containers-overview.md).

### <a name="are-you-planning-to-open-source-service-fabric"></a>Планируете создать решение на базе Service Fabric с открытым исходным кодом?

Мы намерены перевести платформы Reliable Services и Reliable Actors на открытый исходный код на GitHub и будем рады участию сообщества в этих проектах. После объявления о начале их реализации с дополнительными сведениями о них можно будет ознакомиться в [блоге Service Fabric](https://blogs.msdn.microsoft.com/azureservicefabric/).

Переводить среду выполнения Service Fabric на открытый исходный код пока не планируется.

## <a name="next-steps"></a>Дальнейшие действия

- [Дополнительные сведения об основных понятиях и рекомендациях Service Fabric](https://mva.microsoft.com/en-us/training-courses/building-microservices-applications-on-azure-service-fabric-16747?l=tbuZM46yC_5206218965)



<!--HONumber=Jan17_HO3-->


