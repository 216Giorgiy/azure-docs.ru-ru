<properties
   pageTitle="Описание кластера для балансировщика ресурсов | Microsoft Azure"
   description="Описание кластера Service Fabric посредством указания доменов сбоя, доменов обновления, свойств узлов и емкости узлов для балансировщика ресурсов."
   services="service-fabric"
   documentationCenter=".net"
   authors="masnider"
   manager="timlt"
   editor=""/>

<tags
   ms.service="Service-Fabric"
   ms.devlang="dotnet"
   ms.topic="article"
   ms.tgt_pltfrm="NA"
   ms.workload="NA"
   ms.date="03/03/2016"
   ms.author="masnider"/>

# Описание кластера Service Fabric

Диспетчер Resource Manager кластера Service Fabric предоставляет несколько механизмов для описания кластера. Во время выполнения диспетчер Resource Manager собирает эту информацию, чтобы обеспечить высокую доступность служб, функционирующих в кластере, и в то же время гарантировать, что ресурсы в кластере используются должным образом.

## Основные понятия
Вот основные функции диспетчера Resource Manager кластера, описывающие кластер:
- Домены сбоя
- Домены обновления
- Свойства узла
- Емкость узла

### Домены сбоя

Домен сбоя — это любая область координированного сбоя. Доменом сбоя является отдельный компьютер (так как его работа может остановиться по многим причинам — от сбоев питания и сбоев дисков до проблем встроенного ПО сетевого адаптера). Несколько компьютеров, подключенных к одному коммутатору Ethernet, находятся в одном домене сбоя, как если бы они были подключены к одному источнику питания.

Если бы вы настраивали собственный кластер, вам нужно было бы подумать об этих различных областях сбоя и убедиться, что домены сбоя настроены правильно и Service Fabric знает, где размещение служб будет безопасным. Под "безопасным" размещением мы имеем в виду продуманное размещение, при котором отключение домена сбоя не приводит к выходу служб из строя. В среде Azure мы используем данные домена сбоя, которые предоставляет диспетчер Resource Manager или контроллер структуры Azure, для правильной настройки узлов в кластере от вашего имени. На следующем рисунке (рис. 7) мы для примера выделили цветом все сущности, которые объективно могут привести к появлению домена сбоя, и привели список всех доменов сбоя, которые появились в результате. В этом примере у нас есть центры обработки данных (DC), стойки (R) и колонки (B). Предположительно, если каждая колонка содержит несколько виртуальных машин, в иерархии домена сбоя может быть еще один уровень.

![Узлы, организованные через домены сбоя][Image1]

 Во время выполнения диспетчер Resource Manager кластера Service Fabric анализирует домены сбоя в кластере и пытается распределить реплики для данной службы таким образом, чтобы все они находились в разных доменах сбоя. Этот процесс помогает сохранить доступность службы в случае отказа какого-либо домена.

 Для диспетчера Resource Manager кластера Service Fabric на самом деле не имеет значения количество уровней в иерархии. Однако так как ему нужно, чтобы потеря какой-либо части иерархии не влияла на кластер или службы, работающие в нем, то обычно рекомендуется, чтобы на каждом уровне глубины в домене сбоя имелось одинаковое количество машин. Вследствие этого в конце дня один сегмент иерархии будет содержать не больше служб, чем другие.

 Из-за настройки кластера, в результате которого "дерево" доменов сбоя остается несбалансированным, диспетчеру Resource Manager сложнее выяснить, где лучше расположить реплики. Это происходит в частности потому, что отказ отдельного домена может повлиять на доступность кластера, и поэтому Resource Manager лавирует между эффективным использованием компьютеров в этом "сложном" домене и размещением служб таким образом, чтобы отказ домена не вызывал проблем.

 На схеме ниже мы подготовили два разных кластера: в одном узлы равномерно распределены между доменами сбоя, а во втором с одним доменом сбоя связано гораздо больше узлов, чем с другими доменами. Обратите внимание, что в Azure выбор узлов, которые будут связаны с доменом сбоя или доменом обновления, осуществляется автоматически, поэтому вы никогда не заметите такого дисбаланса. Однако если вам когда-нибудь придется настраивать собственный кластер локально или в другой среде, то это нужно будет учесть.

 ![Две различные структуры кластеров][Image2]

### Домены обновления

Домены обновления — это еще одна функция, помогающая диспетчеру Resource Manager Service Fabric понять структуру кластера, чтобы заранее прогнозировать сбои. Домены обновления определяют области (наборы узлов), которые одновременно выйдут из строя во время обновления.

Домены обновления очень похожи на домены сбоя, но имеют ряд ключевых отличий. Во-первых, домены обновления обычно определяются политикой, тогда как домены сбоя тщательно определяются областями координированных сбоев (и, следовательно, структурой оборудования среды). В случае доменов обновления вы все-таки сможете решить, сколько их требуется. Еще одно различие заключается в том, что домены обновления (по крайней мере сегодня) не являются иерархическими: они больше похожи на простой тег, чем на иерархию.

На рисунке ниже показана вымышленная структура, где три домена обновления чередуются с тремя доменами сбоя. На нем также представлено одно возможное размещение трех разных реплик службы с отслеживанием состояния. Обратите внимание, что они находятся в разных доменах сбоя и обновления. Это означает, что если во время обновления службы произойдет отказ домена сбоя, то одна копия кода и данных в кластере будет выполняться и далее. В зависимости от потребностей этого может быть достаточно, однако вы, вероятно, заметите, что эта копия является старой (так как Service Fabric использует репликацию, основанную на кворуме). Чтобы действительно избежать простоев в случае двух сбоев, необходимо несколько реплик (как минимум пять).

![Размещение с доменами сбоя и обновления][Image3]

Наличие множества доменов обновления имеет свои преимущества и недостатки. Одно из преимуществ — то, что каждый этап обновления является фрагментированным и поэтому затрагивает меньшее количество узлов или служб. В результате за определенный момент времени необходимо перемещение меньшего количества служб, что обеспечивает меньшую текучесть системы и в целом повышает уровень надежности (так как любая неисправность затрагивает меньше служб). Недостатком при наличии большого количества доменов обновления является то, что Service Fabric проверяет работоспособность каждого домена обновления по мере его обновления и обеспечивает работоспособность соответствующего домена перед переходом к следующему. Цель этой проверки — убедиться, что у служб есть возможность стабилизации работы, а также проверить их работоспособность и выявить любые ошибки перед обновлением. Возникающие при этом негативные последствия допустимы, так как если выбран этот вариант, то неверные изменения не влияют слишком сильно на слишком большую часть службы в определенный момент времени.

Слишком малое количество доменов обновления имеет свои побочные эффекты: во время остановки в работе и обновления каждого отдельного домена большинство общих возможностей недоступны. Например, при наличии только трех доменов обновления вы отключаете примерно треть возможностей службы или кластера в определенный момент времени. Это нежелательно, так как в кластере должно быть достаточно других ресурсов, чтобы они могли справиться с рабочей нагрузкой. Это значит, что в обычных условиях эти узлы загружены меньше, чем должны быть, что увеличивает расходы.

Фактических ограничений на общее количество доменов сбоя или обновления в среде либо же на то, как они перекрываются, не существует. Стандартными структурами являются соотношение 1:1 (каждый отдельный домен сбоя сопоставляется со своим собственным доменом обновления), домен обновления на узел (экземпляр физической или виртуальной ОС) и модель с "чередованием", или "матричная" модель, в которой домены сбоя и домены обновления образуют матрицу с компьютерами, расположенными по диагонали.

![Структуры доменов сбоя и обновления][Image4]

Ответа на вопрос о том, какая структура лучше, не существует, так как каждая из них имеет свои преимущества и недостатки. Например, модель "1 домен сбоя — 1 домен обновления" отличается простотой настройки, а модель "1 домен сбоя на узел" больше всего понравится тем, кто привык управлять небольшими наборами компьютеров, когда каждый из них отключается отдельно от остальных.

Наиболее распространенной моделью (и именно ее мы используем для размещенных кластеров Azure Service Fabric) является матричная модель, в которой домены сбоя и домены обновления формируют таблицу и узлы располагаются по диагонали. То, как располагаются узлы в такой модели — плотно или разреженно, зависит от общего количества узлов по сравнению с количеством доменов сбоя и доменов обновления (иными словами, если кластер достаточно большой, все будет похоже на шаблон плотной матрицы, показанный в правом нижнем углу рис. 10).

## Настройка доменов сбоя и обновления
Определение доменов сбоя и обновления выполняется автоматически в размещенных в Azure развертываниях Service Fabric. Service Fabric просто извлекает сведения о среде из Azure. В свою очередь, пользователь может выбрать необходимое количество доменов. В Azure сведения о домене сбоя и домене обновления выглядят как "один уровень", но на самом деле это инкапсулированная информация из нижних слоев Azure Stack и представление логических доменов сбоя и обновления с точки зрения пользователя.

Если вы настраиваете свой собственный кластер (или просто хотите попробовать запустить определенную топологию на компьютере разработки), необходимо самостоятельно предоставить сведения о доменах сбоя и обновления. В этом примере мы определим кластер из 9 узлов, охватывающий три "центра обработки данных" (каждый с тремя стойками) и три домена обновления, чередующиеся с этими тремя центрами обработки данных. В манифесте кластера это выглядит примерно так:

ClusterManifest.xml

```xml
  <Infrastructure>
    <!-- IsScaleMin indicates that this cluster runs on one-box /one single server -->
    <WindowsServer IsScaleMin="true">
      <NodeList>
        <Node NodeName="Node01" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType01" FaultDomain="fd:/DC01/Rack01" UpgradeDomain="UpgradeDomain1" IsSeedNode="true" />
        <Node NodeName="Node02" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType02" FaultDomain="fd:/DC01/Rack02" UpgradeDomain="UpgradeDomain2" IsSeedNode="true" />
        <Node NodeName="Node03" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType03" FaultDomain="fd:/DC01/Rack03" UpgradeDomain="UpgradeDomain3" IsSeedNode="true" />
        <Node NodeName="Node04" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType04" FaultDomain="fd:/DC02/Rack01" UpgradeDomain="UpgradeDomain1" IsSeedNode="true" />
        <Node NodeName="Node05" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType05" FaultDomain="fd:/DC02/Rack02" UpgradeDomain="UpgradeDomain2" IsSeedNode="true" />
        <Node NodeName="Node06" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType06" FaultDomain="fd:/DC02/Rack03" UpgradeDomain="UpgradeDomain3" IsSeedNode="true" />
        <Node NodeName="Node07" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType07" FaultDomain="fd:/DC03/Rack01" UpgradeDomain="UpgradeDomain1" IsSeedNode="true" />
        <Node NodeName="Node08" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType08" FaultDomain="fd:/DC03/Rack02" UpgradeDomain="UpgradeDomain2" IsSeedNode="true" />
        <Node NodeName="Node09" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType09" FaultDomain="fd:/DC03/Rack03" UpgradeDomain="UpgradeDomain3" IsSeedNode="true" />
      </NodeList>
    </WindowsServer>
  </Infrastructure>
```
> [AZURE.NOTE] В развернутых службах Azure домены сбоя и обновления назначаются автоматически. Поэтому определение ваших узлов и ролей в параметре infrastructure для Azure не содержит данные о доменах сбоя и обновления.

## Ограничения на размещение и свойства узлов
Иногда (фактически большую часть времени) необходимо убеждаться, что некоторые рабочие нагрузки выполняются только на некоторых узлах или некоторых наборах узлов. Например, для одних рабочих нагрузок могут требоваться графические процессоры или накопители SSD, а для других — нет. Хорошим примером этого является практически каждая n-уровневая архитектура, где одни машины используются в качестве фронтальной или интерфейсной части приложения, тогда как остальные (часто с другими аппаратными ресурсами) служат для вычислений и хранения. Service Fabric ожидает, что даже в мире микрослужб существуют случаи, когда определенные рабочие нагрузки должны выполняться на конкретных конфигурациях оборудования, например:

- существующее n-уровневое приложение быстро перемещено в среду Service Fabric;
- рабочая нагрузка должна выполняться на конкретном оборудовании для повышения производительности, масштабирования или изоляции;
-	рабочую нагрузку следует изолировать от других рабочих нагрузок по соображениям политики или потребления ресурсов.

Для поддержки таких типов конфигурации Service Fabric имеет первоклассное понятие того, что называется "ограничениями на размещение". С помощью ограничений можно указать, где должны выполняться определенные службы. Набор ограничений расширяется пользователями, то есть можно добавлять теги к узлам с пользовательскими свойствами, а затем выбирать их.

![Разные рабочие нагрузки структуры кластера][Image5]

Различные теги "ключ-значение" на узлах называются "свойствами размещения узла" (или просто свойствами узла), а выражение в службе называется "ограничением на размещение". В свойстве узла могут быть указаны значения string, bool или signed long. Ограничением может быть любое логическое выражение, которое работает с различными свойствами узла в кластере, и служба может быть размещена только в тех узлах, в которых выражение принимает значение true. Узлы без определенного свойства не совпадают с какими-либо ограничениями на размещение, содержащими это свойство. Кроме того, Service Fabric определяет некоторые свойства по умолчанию, которые могут использоваться автоматически, так что их не нужно задавать. На момент написания этой статьи свойствами по умолчанию, определенными в каждом узле, являются NodeType и NodeName. В целом мы пришли к выводу, что NodeType — одно из самых распространенных свойств, так как обычно оно имеет соотношение 1:1 к типу компьютера, который, в свою очередь, соответствуют типу рабочей нагрузки в традиционной архитектуре n-уровневого приложения.

![Ограничения на размещение и свойства узлов][Image6]

Предположим, что для данного типа узла были определены следующие свойства узла: ClusterManifest.xml

```xml
    <NodeType Name="NodeType01">
      <PlacementProperties>
        <Property Name="HasDisk" Value="true"/>
        <Property Name="Value" Value="5"/>
      </PlacementProperties>
    </NodeType>
```

Вы можете создать следующие ограничения на размещение службы:

C#

```csharp
FabricClient fabricClient = new FabricClient();
StatefulServiceDescription serviceDescription = new StatefulServiceDescription();
serviceDescription.PlacementConstraints = "(HasDisk == true && Value >= 4)";
// add other required servicedescription fields
//...
await fabricClient.ServiceManager.CreateServiceAsync(serviceDescription);
```

PowerShell:

```posh
New-ServiceFabricService -ApplicationName $applicationName -ServiceName $serviceName -ServiceTypeName $serviceType -Stateful -MinReplicaSetSize 2 -TargetReplicaSetSize 3 -PartitionSchemeSingleton -PlacementConstraint "HasDisk == true && Value >= 4"
```

Если вы уверены, что все узлы NodeType01 являются допустимыми, то можно просто выбрать этот тип узла с помощью ограничения на размещение, как показано на рисунках выше.

Одной замечательной особенностью ограничений на размещение службы является то, что они могут обновляться динамически во время выполнения. Поэтому если необходимо, то можно перемещать службу в кластере, добавлять и удалять требования и т. д. Service Fabric обеспечивает работу и доступность службы даже в том случае, если вносятся такие типы изменений.

C#:

```csharp
StatefulServiceUpdateDescription updateDescription = new StatefulServiceUpdateDescription();
updateDescription.PlacementConstraints = "NodeType == NodeType01";
await fabricClient.ServiceManager.UpdateServiceAsync(new Uri("fabric:/app/service"), updateDescription);
```

PowerShell:

```posh
Update-ServiceFabricService -Stateful -ServiceName $serviceName -PlacementConstraints "NodeType == NodeType01"
```

Ограничения на размещение (а также много других свойств, о которых мы еще будем говорить) задаются для каждого нового экземпляра службы. Обновления всегда заменяют (перезаписывают) свойства, заданные ранее.

### Емкость
Одна из важнейших задач любого оркестратора — помощь в управлении потреблением ресурсов в кластере. Если вам нужна эффективная работа служб, самый нежелательный сценарий — это когда есть несколько узлов с высокой нагрузкой (что приводит к конфликтам ресурсов и снижению производительности), а на других узлах нагрузка небольшая (лишние ресурсы). Но давайте подумаем о кое-чем более простом, чем балансировка (которой мы коснемся чуть позже). Как обеспечить достаточное количество ресурсов для узлов?

Оказывается, Service Fabric представляет ресурсы в виде метрик. Метрики — это любые логические или физические ресурсы, которые нужно описать в Service Fabric. Метриками, например, являются атрибуты WorkQueueDepth или MemoryInMb. Метрики отличаются от ограничений и свойств узлов тем, что свойства узла — это обычно статические дескрипторы самих узлов, а метрики касаются физических ресурсов, потребляемых службами в момент, когда они выполняются на узле. Поэтому свойством может быть, например, HasSSD со значением true или false, однако объем дискового пространства, доступного на SSD (и используемого службами), будет метрикой, например DriveSpaceInMb. В зависимости от емкости узла для атрибута DriveSpaceInMb задается объем общего места, свободного на диске, и службы будут сообщать, какая часть этой метрики используется во время выполнения.

При отключении балансировки всех ресурсов диспетчер Resource Manager платформы Service Fabric по-прежнему может гарантировать, что не превышена емкость ни одного узла (если только кластер в целом не слишком заполнен). Емкости представляют собой механизм, с помощью которого Service Fabric определяет, сколько ресурсов имеет узел, из которого мы вычитаем потребление различными службами (подробнее об этом — чуть позже), чтобы узнать, сколько ресурсов остается. На уровне службы и емкость, и потребление выражаются в виде метрик.

Во время выполнения диспетчер Resource Manager отслеживает, какая часть каждого ресурса используется на каждом узле (определяется по объему), а какая — остается (определяется путем вычитания объявленного количества используемых ресурсов из общего количества ресурсов каждой службы). С помощью этой информации диспетчер Resource Manager платформы Service Fabric может определить, где следует разместить или куда переместить реплики так, чтобы не была превышена емкость узлов.

C#:

```csharp
StatefulServiceDescription serviceDescription = new StatefulServiceDescription();
ServiceLoadMetricDescription metric = new ServiceLoadMetricDescription();
metric.Name = "Memory";
metric.PrimaryDefaultLoad = 1024;
metric.SecondaryDefaultLoad = 1024;
metric.Weight = ServiceLoadMetricWeight.High;
serviceDescription.Metrics.Add(metric);
await fabricClient.ServiceManager.CreateServiceAsync(serviceDescription);
```

PowerShell:

```posh
New-ServiceFabricService -ApplicationName $applicationName -ServiceName $serviceName -ServiceTypeName $serviceTypeName –Stateful -MinReplicaSetSize 2 -TargetReplicaSetSize 3 -PartitionSchemeSingleton –Metric @("Memory,High,1024,1024”)
```

![Узлы кластера и емкость][Image7]

Это можно увидеть в манифесте кластера:

ClusterManifest.xml

```xml
    <NodeType Name="NodeType02">
      <Capacities>
        <Capacity Name="Memory" Value="10"/>
        <Capacity Name="Disk" Value="10000"/>
      </Capacities>
    </NodeType>
```

Нагрузка службы может изменяться также динамически. В таком случае место, где в данный момент расположена реплика или экземпляр, станет недопустимым, так как использование всех реплик и экземпляров на этом узле начнет превышать его емкость. Позже мы подробно поговорим о сценарии, в котором нагрузка может изменяться динамически, но, что касается емкости, обработка выполняется всегда одинаково: управление ресурсами Service Fabric включается автоматически и уменьшает загруженность узла, перемещая реплики или экземпляры из этого узла на другие. При этом диспетчер Resource Manager пытается свести к минимуму стоимость всех перемещений (мы вернемся к вопросу стоимости позже).

###Емкость кластера

Так как же предотвратить переполнение кластера в целом? Если используется динамическая нагрузка, сделать мы можем очень мало (так как у служб могут быть всплески нагрузки независимо от действий, предпринятых диспетчером Resource Manager: например, кластер с большим резервом сегодня может оказаться довольно маломощным, если вы станете знамениты завтра). При этом существуют некоторые встроенные элементы управления, которые помогут избежать основных ошибок. Первое, что можно сделать, — это предотвратить создание новых рабочих нагрузок, которые могут привести к переполнению кластера.

Предположим, что вы собираетесь создать простую службу без отслеживания состояния и что некоторые ее нагрузки связаны с кластером (больше о передаче данных о динамической нагрузке и нагрузке по умолчанию поговорим позже). Предположим, что этой службе нужны определенные ресурсы (например, место на диске) и что по умолчанию она будет использовать 5 единиц места на диске для каждого экземпляра службы. Вы хотите создать 3 экземпляра службы. Отлично! Это значит, что в кластере должно быть 15 единиц места на диске только для того, чтобы можно было создать эти экземпляры службы. Service Fabric постоянно подсчитывает общую емкость и потребление для каждой метрики, поэтому мы легко можем определить и отклонить вызов создания службы, если места недостаточно.

Обратите внимание, что, так как единственным требованием является наличие 15 доступных единиц, пространство может быть распределено несколькими различными путями. Например, это может быть одна оставшаяся единица емкости на 15 различных узлах или три оставшиеся единицы емкости на 5 разных узлах и т. д. Если емкости недостаточно на трех разных узлах, Service Fabric реорганизует службы уже в кластере, чтобы освободить место на трех нужных узлах. Такие изменения почти всегда возможны, если только кластер не заполнен окончательно.

Другая особенность, которая поможет пользователям управлять общей емкостью кластера, — добавление к емкости, выделенной на каждом узле, понятия зарезервированного буфера. Этот параметр является необязательным, но позволяет зарезервировать часть общей емкости узла только для размещения служб во время обновлений и отказов — тех случаев, где при иных обстоятельствах емкость кластера сокращается. Сегодня буфер глобально указан для каждой метрики для всех узлов в ClusterManifest. Значение, выбранное для зарезервированной емкости, будет в большей мере влиять на то, насколько ваши службы ограничены в ресурсах, а также на количество доменов сбоя и обновления, имеющихся в кластере. Обычно большее количество доменов сбоя и обновления означает, что можно выбрать меньшее значение буферизованной емкости, так как ожидается, что во время обновлений и сбоев может стать недоступной меньшая часть кластера. Обратите внимание, что указание процента свободного места имеет смысл, только если указана емкость узла для метрики.

ClusterManifest.xml

```xml
        <Section Name=" NodeBufferPercentage">
            <Parameter Name="DiskSpace" Value="10" />
            <Parameter Name="Memory" Value="15" />
            <Parameter Name="SomeOtherMetric" Value="20" />
        </Section>
```
Когда в кластере заканчивается буферизованная емкость, создание вызовов приводит к сбоям новых служб. Вследствие этого в кластере сохраняется достаточно запасного дополнительного места, а обновления и сбои не приводят к тому, что превышается емкость узлов. Resource Manager предоставляет массу сведений с помощью PowerShell и API-интерфейсов запросов, позволяя видеть параметры буферизованной емкости, общую емкость и текущее потребление для каждой заданной метрики. Здесь мы видим пример таких сведений:

```posh
PS C:\Users\user> Get-ServiceFabricClusterLoadInformation
LastBalancingStartTimeUtc : 9/1/2015 12:54:59 AM
LastBalancingEndTimeUtc   : 9/1/2015 12:54:59 AM
LoadMetricInformation     :
                            LoadMetricName        : Metric1
                            IsBalancedBefore      : False
                            IsBalancedAfter       : False
                            DeviationBefore       : 0.192450089729875
                            DeviationAfter        : 0.192450089729875
                            BalancingThreshold    : 1
                            Action                : NoActionNeeded
                            ActivityThreshold     : 0
                            ClusterCapacity       : 189
                            ClusterLoad           : 45
                            ClusterRemainingCapacity : 144
                            NodeBufferPercentage  : 10
                            ClusterBufferedCapacity : 170
                            ClusterRemainingBufferedCapacity : 125
                            ClusterCapacityViolation : False
                            MinNodeLoadValue      : 0
                            MinNodeLoadNodeId     : 3ea71e8e01f4b0999b121abcbf27d74d
                            MaxNodeLoadValue      : 15
                            MaxNodeLoadNodeId     : 2cc648b6770be1bc9824fa995d5b68b1
```

<!--Every topic should have next steps and links to the next logical set of content to keep the customer engaged-->
## Дальнейшие действия
- [Дополнительные сведения об архитектуре диспетчера Resource Manager кластера](service-fabric-cluster-resource-manager-architecture.md)
- [Дополнительные сведения о метриках дефрагментации](service-fabric-cluster-resource-manager-defragmentation-metrics.md)
- [Общие сведения о диспетчере Resource Manager кластера Service Fabric](service-fabric-cluster-resource-manager-introduction.md)

[Image1]: ./media/service-fabric-cluster-resource-manager-cluster-description/cluster-fault-domains.png
[Image2]: ./media/service-fabric-cluster-resource-manager-cluster-description/cluster-uneven-fault-domain-layout.png
[Image3]: ./media/service-fabric-cluster-resource-manager-cluster-description/cluster-fault-and-upgrade-domains-with-placement.png
[Image4]: ./media/service-fabric-cluster-resource-manager-cluster-description/cluster-fault-and-upgrade-domain-layout-strategies.png
[Image5]: ./media/service-fabric-cluster-resource-manager-cluster-description/cluster-layout-different-workloads.png
[Image6]: ./media/service-fabric-cluster-resource-manager-cluster-description/cluster-placement-constraints-node-properties.png
[Image7]: ./media/service-fabric-cluster-resource-manager-cluster-description/cluster-nodes-and-capacity.png

<!---HONumber=AcomDC_0309_2016-->