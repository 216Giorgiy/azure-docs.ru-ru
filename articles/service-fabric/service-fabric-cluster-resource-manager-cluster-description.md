---
title: "Описание кластера для балансировщика ресурсов | Документация Майкрософт"
description: "Описание кластера Service Fabric посредством указания доменов сбоя, доменов обновления, свойств узлов и емкости узлов для диспетчера кластерных ресурсов."
services: service-fabric
documentationcenter: .net
author: masnider
manager: timlt
editor: 
ms.assetid: 55f8ab37-9399-4c9a-9e6c-d2d859de6766
ms.service: Service-Fabric
ms.devlang: dotnet
ms.topic: article
ms.tgt_pltfrm: NA
ms.workload: NA
ms.date: 01/05/2017
ms.author: masnider
ms.translationtype: HT
ms.sourcegitcommit: f76de4efe3d4328a37f86f986287092c808ea537
ms.openlocfilehash: cfc38cecfaf0a0bdaae043fc35dcfed743459823
ms.contentlocale: ru-ru
ms.lasthandoff: 07/11/2017

---

# <a name="describing-a-service-fabric-cluster"></a>Описание кластера Service Fabric
Диспетчер Resource Manager кластера Service Fabric предоставляет несколько механизмов для описания кластера. Во время выполнения диспетчер кластерных ресурсов использует эти сведения, чтобы обеспечить высокую доступность служб, функционирующих в кластере. При применении этих важных правил он также пытается оптимизировать потребление ресурсов кластера.

## <a name="key-concepts"></a>Основные понятия
Диспетчер кластерных ресурсов поддерживает некоторые функции, описывающие кластер:

* Домены сбоя
* Домены обновления
* Свойства узла
* Емкость узла

## <a name="fault-domains"></a>Домены сбоя
Домен сбоя — это любая область координированного сбоя. В качестве домена сбоя используется отдельный компьютер (так как его работа может остановиться по многим причинам — от сбоев питания и сбоев дисков до проблем встроенного ПО сетевой карты). Компьютеры, подключенные к одному коммутатору Ethernet, находятся в одном домене сбоя, как если бы они использовали один источник питания. Так как сбои оборудования естественным образом пересекаются, домены сбоя по своей сути являются иерархическими и представлены в Service Fabric в виде кодов URI.

При настройке собственного кластера вам нужно подумать об этих различных областях сбоя. Важно, чтобы домены сбоя были настроены правильно, так как Service Fabric использует эти сведения для безопасного размещения служб. Service Fabric не хочет размещать службы, в результате чего сбой одного домена сбоя (из-за сбоя отдельного компонента) приводит к выходу служб из строя. В среде Azure Service Fabric использует данные домена сбоя, предоставляемые средой, для правильной настройки узлов в кластере от вашего имени.

На рисунке ниже выделены цветом все сущности, которые могут привести к появлению доменов сбоя, и приведен полный список этих доменов. В этом примере у нас есть центры обработки данных (DC), стойки (R) и колонки (B). Предположительно, если каждая колонка содержит несколько виртуальных машин, в иерархии домена сбоя может быть еще один уровень.

<center>
![Узлы, организованные через домены сбоя][Image1]
</center>

Во время выполнения диспетчер кластерных ресурсов Service Fabric анализирует домены сбоя в кластере и планирует структуру.  Он пытается распределить реплики с отслеживанием состояния или экземпляры без отслеживания состояния для определенной службы таким образом, чтобы все они находились в разных доменах сбоя. Этот процесс помогает сохранить доступность службы в случае отказа какого-либо домена сбоя (на любом уровне иерархии).

Для диспетчера кластерных ресурсов Service Fabric не имеет значения количество уровней в иерархии домена сбоя. Однако он пытается убедиться, что потеря какой-либо части иерархии не повлияет на службы, работающие на ее основе. По этой причине мы советуем настраивать одинаковое количество узлов на каждом уровне глубины в иерархии домена сбоя. Балансировка уровней позволяет избежать того, что один сегмент иерархии будет содержать больше служб, чем другие. В противном случае может возникнуть дисбаланс в нагрузке отдельных узлов, в результате чего сбой определенных доменов будет более опасным, чем других.

Если "дерево" доменов сбоя остается несбалансированным в кластере, диспетчеру кластерных ресурсов сложнее выяснить, где лучше расположить службы. Несбалансированная структура доменов сбоя означает, что потеря определенного домена может повлиять на доступность кластера больше, чем потеря других. Поэтому диспетчер кластерных ресурсов лавирует между использованием компьютеров (размещая на них службы) в нагруженном домене и размещением служб таким образом, чтобы отказ домена не вызывал проблем. Это выглядит следующим образом.

<center>
![Две различные структуры кластеров][Image2]
</center>

На схеме выше показаны примеры двух различных структур кластера. В первом примере узлы равномерно распределены между доменами сбоя, а во втором с одним доменом сбоя связано гораздо больше узлов. Если вам когда-нибудь придется настраивать собственный кластер локально или в другой среде, то это нужно будет учесть.

В Azure выбор домена сбоя, в котором будет содержаться узел, осуществляется автоматически. Тем не менее, в зависимости от количества подготавливаемых узлов, в некоторых доменах сбоя по-прежнему может оказаться большее количество узлов, чем в других. Например, предположим, что у вас есть пять доменов сбоя, но выполняется подготовка семи узлов определенного типа. В этом случае в первых двух доменах сбоя будет размещено больше узлов. Если продолжить развертывание дополнительных типов узлов, используя только несколько экземпляров, проблема усугубится.

## <a name="upgrade-domains"></a>Домены обновления
Домены обновления — это еще одна функция, помогающая диспетчеру кластерных ресурсов Service Fabric понять структуру кластера, чтобы заранее прогнозировать сбои. Домены обновления определяют наборы узлов, обновляемых одновременно.

Домены обновления очень похожи на домены сбоя, но имеют ряд ключевых отличий. Во-первых, домены сбоя тщательно определяются областями координированных сбоев оборудования, тогда как домены обновления определяются политикой. В случае доменов обновления вы сможете решить, сколько их требуется. Их количество не зависит от среды. Еще одно различие заключается в том, что домены обновления (по крайней мере сегодня) не являются иерархическими. Они больше похожи на простой тег.

На схеме ниже показаны три домена обновления, чередующиеся с тремя доменами сбоя. На ней также представлено одно возможное размещение трех разных реплик службы с отслеживанием состояния, где каждая из них находится в разных доменах сбоя и обновления. Такое размещение гарантирует, что если в процессе обновления службы произойдет отказ домена сбоя, то у вас по-прежнему останется одна копия кода и данных.

<center>
![Размещение с доменами сбоя и обновления][Image3]
</center>

Наличие множества доменов обновления имеет свои преимущества и недостатки. Одно из преимуществ — то, что каждый этап обновления становится более детализированным и поэтому затрагивает меньшее количество узлов или служб. В результате за определенный момент времени необходимо переместить меньшее количество служб, что обеспечивает меньшую текучесть системы. Это также повышает уровень надежности (так как любой сбой в процессе обновления затрагивает меньше служб). Большое количество доменов обновления позволяет снизить нагрузку на остальные узлы при обработке последствий обновления. Например, при наличии пяти доменов обновления узлы в каждом из них обрабатывают примерно 20 % вашего трафика. Если в процессе обновления домен обновления необходимо отключить, эту нагрузку необходимо переопределить. За счет большого количества доменов обновления нагрузка на остальные узлы в кластере снижается.

Недостаток наличия большого количества доменов обновления заключается в том, что обновление выполняется дольше. Перед выполнением дальнейших действий Service Fabric ожидает некоторое время после обновления домена обновления. Это необходимо, чтобы система могла обнаружить ошибки, вызванные обновлением. Возникающие при этом негативные последствия допустимы, так как если выбран этот вариант, то неверные изменения не влияют слишком сильно на слишком большую часть службы в определенный момент времени.

Слишком малое количество доменов обновления имеет свои побочные эффекты: во время остановки в работе и обновления каждого отдельного домена большая часть общих ресурсов недоступна. Например, при наличии только трех доменов обновления вы одновременно отключаете примерно треть общих ресурсов службы или кластера. Это нежелательно, так как в кластере должно быть достаточно ресурсов для обработки рабочей нагрузки. Настройка буфера означает, что в обычных условиях эти узлы загружены меньше, чем должны быть, что увеличивает затраты на выполнение службы.

Фактических ограничений на общее количество доменов сбоя или обновления в среде либо же на то, как они перекрываются, не существует. К стандартным структурам относятся:

* соотношение 1:1 (каждый домен сбоя сопоставляется с доменом обновления);
* один домен обновления на узел (экземпляр физической или виртуальной ОС);
* модель с "чередованием", или "матричная" модель, в которой домены сбоя и домены обновления образуют матрицу с компьютерами, обычно выполняющимися по диагонали.

<center>
![Структуры доменов сбоя и обновления][Image4]
</center>

Ответа на вопрос о том, какая структура лучше, не существует, так как каждая из них имеет свои преимущества и недостатки. Например, модель "1 домен сбоя — 1 домен обновления" отличается простотой настройки, а модель "1 домен сбоя на узел" больше всего понравится тем, кто привык управлять небольшими наборами компьютеров, когда каждый из них отключается отдельно от остальных.

Самая распространенная модель (и именно ее мы используем в Azure) — это матричная модель, в которой домены сбоя и домены обновления формируют таблицу и узлы располагаются по диагонали. То, как располагаются узлы в такой модели (плотно или разреженно), зависит от общего количества узлов по сравнению с количеством доменов сбоя и доменов обновления. Иными словами, если кластер достаточно большой, все будет похоже на шаблон плотной матрицы, показанный в правом нижнем углу схемы выше.

## <a name="fault-and-upgrade-domain-constraints-and-resulting-behavior"></a>Ограничения доменов сбоя и обновления и соответствующее поведение
Диспетчер кластерных ресурсов воспринимает попытку сбалансированного распределения службы между доменами сбоя и обновления как ограничение. Дополнительные сведения об ограничениях см. в [этой статье](service-fabric-cluster-resource-manager-management-integration.md). В соответствии с ограничениями доменов сбоя и обновления для определенного раздела службы разница в количестве объектов службы (экземпляры службы без отслеживания состояния или реплики службы с отслеживанием состояния) между двумя доменами никогда не должна составлять *больше 1*.  На практике это означает, что для определенной службы отдельные перемещения или изменения могут быть недопустимыми, так как они нарушают ограничение доменов сбоя или обновления.

Давайте рассмотрим один пример. Предположим, что у нас есть кластер с шестью узлами (У), на котором настроено пять доменов сбоя (ДС) и пять доменов обновления (ДО).

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 |
| --- |:---:|:---:|:---:|:---:|:---:|
| **ДО0** |У1 | | | | |
| **ДО1** |У6 |У2 | | | |
| **ДО2** | | |У3 | | |
| **ДО3** | | | |У4 | |
| **ДО4** | | | | |У5 |

Теперь предположим, что мы создаем службу с параметром TargetReplicaSetSize равным пяти. Реплики размещаются на узлах У1-У5. Узел У6 фактически никогда не будет использован, вне зависимости от количества создаваемых служб. Но почему? Давайте рассмотрим разницу между текущей структурой и тем, что произошло бы, если бы мы выбрали У6.

Ниже показана наша текущая структура и общее число реплик (Р) на домен сбоя и обновления.

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 | Всего ДО |
| --- |:---:|:---:|:---:|:---:|:---:|:---:|
| **ДО0** |Р1 | | | | |1 |
| **ДО1** | |Р2 | | | |1 |
| **ДО2** | | |Р3 | | |1 |
| **ДО3** | | | |Р4 | |1 |
| **ДО4** | | | | |Р5 |1 |
| **Всего ДС** |1 |1 |1 |1 |1 |- |

Эта структура сбалансирована в плане распределения узлов на домен сбоя и домен обновления, а также в плане количества реплик на каждый из этих доменов. На каждый домен приходится одинаковое количество узлов и реплик.

Теперь давайте посмотрим, что произошло бы, если бы вместо У2 мы использовали У6. Как бы тогда распределялись реплики?

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 | Всего ДО |
| --- |:---:|:---:|:---:|:---:|:---:|:---:|
| **ДО0** |Р1 | | | | |1 |
| **ДО1** |Р5 | | | | |1 |
| **ДО2** | | |Р2 | | |1 |
| **ДО3** | | | |Р3 | |1 |
| **ДО4** | | | | |Р4 |1 |
| **Всего ДС** |2 |0 |1 |1 |1 |- |

Заметили что-то? Эта структура нарушает определение ограничения для доменов сбоя. На ДС0 приходится две реплики, а на ДС1 — ноль. То есть разница составляет 2. Диспетчер кластерных ресурсов не разрешит использовать такую расстановку. Аналогично, если бы мы выбрали узлы У2 и У6 (вместо У1 и У2), то получили бы следующее.

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 | Всего ДО |
| --- |:---:|:---:|:---:|:---:|:---:|:---:|
| **ДО0** | | | | | |0 |
| **ДО1** |Р5 |Р1 | | | |2 |
| **ДО2** | | |Р2 | | |1 |
| **ДО3** | | | |Р3 | |1 |
| **ДО4** | | | | |Р4 |1 |
| **Всего ДС** |1 |1 |1 |1 |1 |- |

В плане распределения на домен сбоя такая структура сбалансированная. Однако теперь она нарушает ограничение для доменов обновления (так как на ДО0 реплики не припадают, а на ДО1 приходится две). По этой причине такая структура недопустима.

## <a name="configuring-fault-and-upgrade-domains"></a>Настройка доменов сбоя и обновления
Определение доменов сбоя и обновления выполняется автоматически в размещенных в Azure развертываниях Service Fabric. Service Fabric просто извлекает и использует сведения о среде из Azure.

Если вы создаете собственный кластер (или хотите попробовать запустить определенную топологию в среде разработки), необходимо самостоятельно предоставить сведения о доменах сбоя и обновления. В этом примере мы определяем кластер локальной разработки из девяти узлов, охватывающий три центра обработки данных (каждый с тремя стойками). Этот кластер также имеет три домена обновления, чередующиеся с этими тремя центрами обработки данных. В шаблоне манифеста кластера это выглядит примерно так:

ClusterManifest.xml

```xml
  <Infrastructure>
    <!-- IsScaleMin indicates that this cluster runs on one-box /one single server -->
    <WindowsServer IsScaleMin="true">
      <NodeList>
        <Node NodeName="Node01" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType01" FaultDomain="fd:/DC01/Rack01" UpgradeDomain="UpgradeDomain1" IsSeedNode="true" />
        <Node NodeName="Node02" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType02" FaultDomain="fd:/DC01/Rack02" UpgradeDomain="UpgradeDomain2" IsSeedNode="true" />
        <Node NodeName="Node03" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType03" FaultDomain="fd:/DC01/Rack03" UpgradeDomain="UpgradeDomain3" IsSeedNode="true" />
        <Node NodeName="Node04" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType04" FaultDomain="fd:/DC02/Rack01" UpgradeDomain="UpgradeDomain1" IsSeedNode="true" />
        <Node NodeName="Node05" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType05" FaultDomain="fd:/DC02/Rack02" UpgradeDomain="UpgradeDomain2" IsSeedNode="true" />
        <Node NodeName="Node06" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType06" FaultDomain="fd:/DC02/Rack03" UpgradeDomain="UpgradeDomain3" IsSeedNode="true" />
        <Node NodeName="Node07" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType07" FaultDomain="fd:/DC03/Rack01" UpgradeDomain="UpgradeDomain1" IsSeedNode="true" />
        <Node NodeName="Node08" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType08" FaultDomain="fd:/DC03/Rack02" UpgradeDomain="UpgradeDomain2" IsSeedNode="true" />
        <Node NodeName="Node09" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType09" FaultDomain="fd:/DC03/Rack03" UpgradeDomain="UpgradeDomain3" IsSeedNode="true" />
      </NodeList>
    </WindowsServer>
  </Infrastructure>
```

Использование ClusterConfig.json для автономных развертываний

```json
"nodes": [
  {
    "nodeName": "vm1",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc1/r0",
    "upgradeDomain": "UD1"
  },
  {
    "nodeName": "vm2",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc1/r0",
    "upgradeDomain": "UD2"
  },
  {
    "nodeName": "vm3",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc1/r0",
    "upgradeDomain": "UD3"
  },
  {
    "nodeName": "vm4",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc2/r0",
    "upgradeDomain": "UD1"
  },
  {
    "nodeName": "vm5",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc2/r0",
    "upgradeDomain": "UD2"
  },
  {
    "nodeName": "vm6",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc2/r0",
    "upgradeDomain": "UD3"
  },
  {
    "nodeName": "vm7",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc3/r0",
    "upgradeDomain": "UD1"
  },
  {
    "nodeName": "vm8",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc3/r0",
    "upgradeDomain": "UD2"
  },
  {
    "nodeName": "vm9",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc3/r0",
    "upgradeDomain": "UD3"
  }
],
```

> [!NOTE]
> В развернутых службах Azure домены сбоя и обновления назначаются автоматически. Поэтому определение ваших узлов и ролей в параметре Infrastructure для Azure не содержит данные о доменах сбоя и обновления.
>
>

## <a name="placement-constraints-and-node-properties"></a>Ограничения на размещение и свойства узлов
Иногда (фактически большую часть времени) необходимо убеждаться, что некоторые рабочие нагрузки выполняются только на некоторых узлах или некоторых наборах узлов в кластере. Например, для одних рабочих нагрузок могут требоваться графические процессоры или накопители SSD, а для других — нет. Хорошим примером использования определенного оборудования в зависимости от рабочих нагрузок является практически каждая n-уровневая архитектура. В этих архитектурах одни машины используются в качестве фронтальной или интерфейсной части приложения (и поэтому, очевидно, доступны через Интернет). Остальные наборы машин (часто с другими аппаратными ресурсами) служат для обработки вычислительных уровней и уровней хранилищ (как правило, они не доступны через Интернет). Service Fabric ожидает, что даже в мире микрослужб существуют случаи, когда определенные рабочие нагрузки должны выполняться на конкретных конфигурациях оборудования, например:

* существующее n-уровневое приложение быстро перемещено в среду Service Fabric;
* рабочая нагрузка должна выполняться на конкретном оборудовании для повышения производительности, масштабирования или изоляции;
* рабочая нагрузка должна быть изолирована от других рабочих нагрузок по соображениям политики или потребления ресурсов.

Для поддержки таких типов конфигурации Service Fabric имеет первоклассное понятие тегов, которые можно применить к узлам. Они называются ограничениями на размещение. С помощью ограничений на размещение можно указать, где должны выполняться определенные службы. Набор ограничений можно расширить, то есть можно использовать любые пары "ключ — значение".

<center>
![Разные рабочие нагрузки структуры кластера][Image5]
</center>

Различные теги "ключ — значение" на узлах называются *свойствами* размещения узла (или просто свойствами узла). В свойстве узла могут быть указаны значения string, bool или signed long. Оператор в службе называется *ограничением* на размещение, так как этот оператор применяется там, где может выполняться служба в кластере. Ограничением может быть любой логический оператор, который работает с различными свойствами узла в кластере. Ниже приведены допустимые селекторы в этих логических операторах.

1) Условные проверки для создания определенных операторов:

| Инструкция | Синтаксис |
| --- |:---:|
| "равно" | "==" |
| "не равно" | "!=" |
| "больше" | ">" |
| "больше или равно" | ">=" |
| "меньше" | "<" |
| "меньше или равно" | "<=" |

2) Логические операторы для группирования и логических операций:

| Инструкция | Синтаксис |
| --- |:---:|
| "и" | "&&" |
| "или" | "&#124;&#124;" |
| "не" | "!" |
| "группа как отдельный оператор" | "()" |

Ниже приведено несколько примеров основных операторов ограничения.

  * `"Value >= 5"`
  * `"NodeColor != green"`
  * `"((OneProperty < 100) || ((AnotherProperty == false) && (OneProperty >= 100)))"`

Служба может быть размещена только на тех узлах, где оператор принимает общее значение True. Узлы без определенного свойства не совпадают с какими-либо ограничениями на размещение, содержащими это свойство.

Service Fabric определяет некоторые свойства узла по умолчанию, которые могут использоваться автоматически, так что их не нужно задавать. На момент написания этой статьи по умолчанию в каждом узле определены свойства **NodeType** и **NodeName**. Например, ограничение на размещение можно записать в таком виде: `"(NodeType == NodeType03)"`. В целом мы пришли к выводу, что NodeType — одно из самых распространенных свойств, так как обычно оно имеет соотношение 1:1 к типу компьютера, который, в свою очередь, соответствует типу рабочей нагрузки в традиционной архитектуре n-уровневого приложения.

<center>
![Ограничения на размещение и свойства узлов][Image6]
</center>

Предположим, что для этого типа узла были определены следующие свойства узла:

ClusterManifest.xml

```xml
    <NodeType Name="NodeType01">
      <PlacementProperties>
        <Property Name="HasSSD" Value="true"/>
        <Property Name="NodeColor" Value="green"/>
        <Property Name="SomeProperty" Value="5"/>
      </PlacementProperties>
    </NodeType>
```

Для автономных развертываний используется ClusterConfig.json, а для размещенных в Azure кластеров — Template.json. В шаблоне Azure Resource Manager для кластера такие вещи, как имя типа узла, обычно параметризованные и имеют формат [parameters('vmNodeType1Name')], а не NodeType01.

```json
"nodeTypes": [
    {
        "name": "NodeType01",
        "placementProperties": {
            "HasSSD": "true",
            "NodeColor": "green",
            "SomeProperty": "5"
        },
    }
],
```

Вы можете создать *ограничения* на размещение службы следующим образом:

C#

```csharp
FabricClient fabricClient = new FabricClient();
StatefulServiceDescription serviceDescription = new StatefulServiceDescription();
serviceDescription.PlacementConstraints = "(HasSSD == true && SomeProperty >= 4)";
// add other required servicedescription fields
//...
await fabricClient.ServiceManager.CreateServiceAsync(serviceDescription);
```

PowerShell:

```posh
New-ServiceFabricService -ApplicationName $applicationName -ServiceName $serviceName -ServiceTypeName $serviceType -Stateful -MinReplicaSetSize 2 -TargetReplicaSetSize 3 -PartitionSchemeSingleton -PlacementConstraint "HasSSD == true && SomeProperty >= 4"
```

Если вы уверены, что все узлы NodeType01 допустимые, то можно также выбрать этот тип узла.

Одной замечательной особенностью ограничений на размещение службы является то, что они могут обновляться динамически во время выполнения. Поэтому если необходимо, то можно перемещать службу в кластере, добавлять и удалять требования и т. д. Service Fabric обеспечивает работу и доступность службы даже в том случае, если вносятся такие типы изменений.

C#:

```csharp
StatefulServiceUpdateDescription updateDescription = new StatefulServiceUpdateDescription();
updateDescription.PlacementConstraints = "NodeType == NodeType01";
await fabricClient.ServiceManager.UpdateServiceAsync(new Uri("fabric:/app/service"), updateDescription);
```

PowerShell:

```posh
Update-ServiceFabricService -Stateful -ServiceName $serviceName -PlacementConstraints "NodeType == NodeType01"
```

Ограничения на размещение (а также много других элементов управления оркестратора, о которых мы еще будем говорить) задаются для каждого именованного экземпляра службы. Обновления всегда заменяют (перезаписывают) свойства, заданные ранее.

Свойства узла определяются вместе с кластером, следовательно и обновить их можно только при обновлении кластера. При этом для обновления свойств каждый затронутый узел должен быть выведен из строя, а затем восстановлен.

## <a name="capacity"></a>Емкость
Одна из важнейших задач любого оркестратора — помощь в управлении потреблением ресурсов в кластере. Последнее, что необходимо для эффективной работы служб, — несколько узлов с высокой нагрузкой, тогда как на остальных узлах нагрузка должна быть небольшая. Наличие узлов с высокой нагрузкой приводит к конфликту ресурсов и снижению производительности, а узлы с небольшой нагрузкой представляют лишние ресурсы или увеличение расходов. Прежде чем мы рассмотрим балансировку, давайте поговорим о том, как обеспечить достаточное количество ресурсов для узлов.

В Service Fabric ресурсы представлены в виде метрик (`Metrics`). Метрики — это любые логические или физические ресурсы, которые нужно описать в Service Fabric. Метриками, например, являются атрибуты WorkQueueDepth или MemoryInMb. Дополнительные сведения о настройке и использовании метрик см. в [этой статье](service-fabric-cluster-resource-manager-metrics.md).

Метрики отличаются от ограничений на размещение и свойств узлов. Свойства узла — это статические дескрипторы самих узлов, а метрики касаются ресурсов, находящихся на узлах и потребляемых службами в момент, когда они выполняются на узле. Свойством может быть, например, HasSSD со значением true или false. Объем дискового пространства, доступного на этом твердотельном накопителе (и используемого службами), будет метрикой, например DriveSpaceInMb. В зависимости от емкости узла для атрибута DriveSpaceInMb задается объем общего места, свободного на диске. Службы будут сообщать, какая часть этой метрики используется во время выполнения.

Важно отметить, что диспетчер кластерных ресурсов Service Fabric не понимает, что означают имена метрик (так же как и для ограничений на размещение и свойств узлов). Имена метрик — это просто строки. В случае неоднозначности мы советуем объявлять единицы как часть созданных имен метрик.

При отключении *балансировки* всех ресурсов диспетчер кластерных ресурсов Service Fabric по-прежнему пытается гарантировать, что не превышена емкость ни одного узла. Обычно такой вариант возможен, если кластер в целом не слишком заполнен. Емкость — это другое *ограничение*, используемое диспетчером кластерных ресурсов, чтобы понять, какая часть ресурса используется на узле. Оставшаяся емкость также отслеживается для кластера в целом. На уровне службы и емкость, и потребление выражаются в виде метрик. Например, метрика может называться MemoryInMb, а определенный узел может иметь емкость для MemoryInMb в размере 2048 МБ. Определенная служба, выполняющаяся на этом узле, может сообщать, что в настоящий момент она потребляет 64 МБ от емкости MemoryInMb.

Во время выполнения диспетчер кластерных ресурсов отслеживает, какая часть каждого ресурса доступна на каждом узле, а какая — остается. Это определяется путем вычитания объявленного использования ресурсов каждой службой, выполняющейся на этом узле, из емкости узла. С помощью этих сведений диспетчер кластерных ресурсов Service Fabric может определить, где следует разместить или куда переместить реплики так, чтобы не была превышена емкость узлов.

C#:

```csharp
StatefulServiceDescription serviceDescription = new StatefulServiceDescription();
ServiceLoadMetricDescription metric = new ServiceLoadMetricDescription();
metric.Name = "MemoryInMb";
metric.PrimaryDefaultLoad = 64;
metric.SecondaryDefaultLoad = 64;
metric.Weight = ServiceLoadMetricWeight.High;
serviceDescription.Metrics.Add(metric);
await fabricClient.ServiceManager.CreateServiceAsync(serviceDescription);
```

PowerShell:

```posh
New-ServiceFabricService -ApplicationName $applicationName -ServiceName $serviceName -ServiceTypeName $serviceTypeName –Stateful -MinReplicaSetSize 2 -TargetReplicaSetSize 3 -PartitionSchemeSingleton –Metric @("Memory,High,64,64)
```
<center>
![Узлы и емкость кластера][Image7]
</center>

Вы можете увидеть емкость, определенную в манифесте кластера:

ClusterManifest.xml

```xml
    <NodeType Name="NodeType02">
      <Capacities>
        <Capacity Name="MemoryInMb" Value="2048"/>
        <Capacity Name="DiskInMb" Value="512000"/>
      </Capacities>
    </NodeType>
```

Для автономных развертываний используется ClusterConfig.json, а для размещенных в Azure кластеров — Template.json. В шаблоне Azure Resource Manager для кластера такие вещи, как имя типа узла, обычно параметризованные и имеют формат [parameters('vmNodeType2Name')], а не NodeType02.

```json
"nodeTypes": [
    {
        "name": "NodeType02",
        "capacities": {
            "MemoryInMb": "2048",
            "DiskInMb": "512000"
        }
    }
],
```

Нагрузка службы может изменяться также динамически (и собственно, это стандартная ситуация). Предположим, что нагрузка реплики изменилась с 64 МБ на 1024 МБ, но оставшаяся емкость узла, на котором она выполнялась, составляла только 512 МБ (от метрики MemoryInMb). В этом случае расположение, где в данный момент находится реплика или экземпляр, станет недопустимым, так как на этом узле недостаточно места. Это также может произойти, если при совместном использовании всех реплик и экземпляров на этом узле будет превышена его емкость. В любом случае диспетчер кластерных ресурсов должен включиться и уменьшить загруженность узла, переместив одну или несколько реплик или экземпляров с этого узла на другие. После перемещения реплик диспетчер кластерных ресурсов пытается свести к минимуму стоимость перемещения. Дополнительные сведения о стоимости перемещения см. в [этой статье](service-fabric-cluster-resource-manager-movement-cost.md).

## <a name="cluster-capacity"></a>Емкость кластера
Так как же предотвратить переполнение кластера в целом? Если используется динамическая нагрузка, диспетчер кластерных ресурсов может сделать немного. В службах могут наблюдаться всплески нагрузки независимо от действий, предпринятых диспетчером кластерных ресурсов. В результате кластер с большим резервом сегодня может оказаться довольно маломощным при повышении нагрузки завтра. При этом существуют некоторые встроенные элементы управления, которые помогут избежать основных проблем. Первое, что можно сделать, — это предотвратить создание новых рабочих нагрузок, которые могут привести к переполнению кластера.

Предположим, что вы собираетесь создать службу без отслеживания состояния и что некоторые ее нагрузки связаны с кластером (больше о передаче данных о динамической нагрузке и нагрузке по умолчанию поговорим позже). Предположим, что этой службе нужна метрика DiskSpaceInMb и что она будет использовать 5 единиц DiskSpaceInMb для каждого экземпляра службы. Вы хотите создать три экземпляра службы. Отлично! Это значит, что в кластере должно быть 15 единиц метрики DiskSpaceInMb только для того, чтобы можно было создать эти экземпляры службы. Диспетчер кластерных ресурсов постоянно подсчитывает общую емкость и потребление для каждой метрики, поэтому он легко может определить достаточный объем места в кластере. Если места недостаточно, диспетчер кластерных ресурсов отклоняет вызов создания службы.

Так как единственное требование заключается в наличии 15 доступных единиц, это пространство может быть распределено несколькими различными путями. Например, это может быть одна оставшаяся единица емкости на 15 различных узлах или три оставшиеся единицы емкости на 5 разных узлах. Пока диспетчер кластерных ресурсов может изменить расположение, чтобы на трех узлах были доступны пять единиц, он обязательно разместит службу. Такие изменения почти всегда возможны, если только кластер не заполнен окончательно или все службы занимают слишком много места (или и то, и другое).

## <a name="buffered-capacity"></a>Емкость буфера
Другая возможность, которая помогает диспетчеру кластерных ресурсов управлять общей емкостью кластера, — добавление к емкости, выделенной на каждом узле, понятия зарезервированного буфера. Емкость буфера позволяет зарезервировать часть общей емкости узла только для размещения служб во время обновлений и сбоев узлов. Сегодня буфер глобально указан для каждой метрики для всех узлов в определении кластера. Значение, выбранное для зарезервированной емкости, зависит от количества доменов обновления и сбоя в кластере, а также от необходимой нагрузки. Большее количество доменов сбоя и обновления означает, что можно выбрать меньшее значение зарезервированной емкости. Чем больше доменов, тем меньшее количество кластеров будет недоступным во время обновлений и сбоев. Указание процента свободного места имеет смысл, только если указана емкость узла для метрики.

Пример указания емкости буфера:

ClusterManifest.xml

```xml
        <Section Name="NodeBufferPercentage">
            <Parameter Name="DiskSpace" Value="0.10" />
            <Parameter Name="Memory" Value="0.15" />
            <Parameter Name="SomeOtherMetric" Value="0.20" />
        </Section>
```

Для автономных развертываний используется ClusterConfig.json, а для размещенных в Azure кластеров — Template.json.

```json
"fabricSettings": [
  {
    "name": "NodeBufferPercentage",
    "parameters": [
      {
          "name": "DiskSpace",
          "value": "0.10"
      },
      {
          "name": "Memory",
          "value": "0.15"
      },
      {
          "name": "SomeOtherMetric",
          "value": "0.20"
      }
    ]
  }
]
```

При недостаточной емкости буфера для метрики создание служб завершается сбоем. Это гарантирует, что в кластере сохраняется достаточно запасного дополнительного места, а обновления и сбои не приводят к превышению емкости узлов. Емкость буфера указывать необязательно, но мы советуем добавить ее во всех кластерах, определяющих емкость для метрики.

Диспетчер кластерных ресурсов предоставляет эти сведения с помощью PowerShell и API-интерфейсов запросов. Это позволяет видеть параметры емкости буфера, общую емкость и текущее потребление для каждой метрики, используемой кластером. Здесь мы видим пример таких сведений:

```posh
PS C:\Users\user> Get-ServiceFabricClusterLoadInformation
LastBalancingStartTimeUtc : 9/1/2016 12:54:59 AM
LastBalancingEndTimeUtc   : 9/1/2016 12:54:59 AM
LoadMetricInformation     :
                            LoadMetricName        : Metric1
                            IsBalancedBefore      : False
                            IsBalancedAfter       : False
                            DeviationBefore       : 0.192450089729875
                            DeviationAfter        : 0.192450089729875
                            BalancingThreshold    : 1
                            Action                : NoActionNeeded
                            ActivityThreshold     : 0
                            ClusterCapacity       : 189
                            ClusterLoad           : 45
                            ClusterRemainingCapacity : 144
                            NodeBufferPercentage  : 10
                            ClusterBufferedCapacity : 170
                            ClusterRemainingBufferedCapacity : 125
                            ClusterCapacityViolation : False
                            MinNodeLoadValue      : 0
                            MinNodeLoadNodeId     : 3ea71e8e01f4b0999b121abcbf27d74d
                            MaxNodeLoadValue      : 15
                            MaxNodeLoadNodeId     : 2cc648b6770be1bc9824fa995d5b68b1
```

## <a name="next-steps"></a>Дальнейшие действия
* Сведения об архитектуре и потоке информации в диспетчере кластерных ресурсов см. в [этой статье](service-fabric-cluster-resource-manager-architecture.md).
* Определение метрик дефрагментации — один из способов объединения нагрузки на узлах вместо ее рассредоточения. Сведения о настройке дефрагментации см. в [этой статье](service-fabric-cluster-resource-manager-defragmentation-metrics.md).
* Начните с самого начала, [изучив общие сведения о диспетчере кластерных ресурсов Service Fabric](service-fabric-cluster-resource-manager-introduction.md)
* Чтобы узнать, как диспетчер кластерных ресурсов управляет нагрузкой кластера и балансирует ее, ознакомьтесь со статьей о [балансировке нагрузки](service-fabric-cluster-resource-manager-balancing.md)

[Image1]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-fault-domains.png
[Image2]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-uneven-fault-domain-layout.png
[Image3]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-fault-and-upgrade-domains-with-placement.png
[Image4]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-fault-and-upgrade-domain-layout-strategies.png
[Image5]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-layout-different-workloads.png
[Image6]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-placement-constraints-node-properties.png
[Image7]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-nodes-and-capacity.png

