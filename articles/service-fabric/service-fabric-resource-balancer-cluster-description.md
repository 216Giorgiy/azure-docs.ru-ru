<properties
   pageTitle="Описание кластера для балансировщика ресурсов"
   description="Добавление описания кластера для балансировщика ресурсов"
   services="service-fabric"
   documentationCenter=".net"
   authors="GaugeField"
   manager="timlt"
   editor=""/>

<tags
   ms.service="Service-Fabric"
   ms.devlang="dotnet"
   ms.topic="article"
   ms.tgt_pltfrm="NA"
   ms.workload="NA"
   ms.date="09/03/2015"
   ms.author="masnider"/>

# Описание кластера

В балансировщике ресурсов Service Fabric есть несколько механизмов для описания кластера. Во время работы балансировщик ресурсов использует эти данные, чтобы размещение служб, работающих в кластере, обеспечивало их высокую доступность, а ресурсы кластера использовались максимально. В балансировщике ресурсов кластер описывается через домены сбоя, домены обновления, свойства узлов и емкость узлов. Кроме того, в балансировщике есть несколько настроек, с помощью которых можно изменить производительность.

## Основные понятия

### Домены сбоя

Домены сбоя позволяют администраторам кластера определять физические узлы, на которых из-за наличия общих физических зависимостей (например, источников электропитания и сетевых контроллеров) могут одновременно возникать сбои. Домены сбоя обычно представляют собой иерархии, выстроенные на основе общих зависимостей. Чем выше в дереве домена сбоя расположена точка отказа, тем на большем количестве узлов может одновременно возникнуть сбой. На рисунке ниже показано несколько узлов, которые упорядочены в иерархические домены сбоя в следующем порядке: центр обработки данных, стеллаж и блейд-сервер.

![Домены сбоя][Image1]

 Во время работы диспетчер ресурсов Service Fabric рассматривает домены сбоя в кластере и пытается распределить реплики службы, чтобы все они находились в разных доменах сбоя. Этот процесс обеспечивает сохранение доступности и состояния службы в случае отказа какого-либо домена. На приведенном ниже рисунке изображены реплики службы, которые размещены в нескольких доменах сбоя, хотя их можно было бы разместить всего лишь в одном или двух доменах.

![Домены сбоя][Image2]

Домены сбоя настраиваются в манифесте кластера. Для каждого узла определяется расположение в конкретном домене сбоя. Во время работы диспетчер ресурсов объединяет отчеты всех узлов и создает полный набор сведений обо всех доменах сбоя в системе.

### Домены обновления

Домены обновления — это еще один набор данных, которые использует диспетчер ресурсов. Домены обновления, как и домены сбоя, описывают наборы узлов, которые почти одновременно завершают работу, чтобы выполнить обновления. Домены обновления не являются иерархическими и могут рассматриваться как теги.

Домены сбоя определяются физической структурой узлов в кластере, а домены обновления определяет администратор кластера на основе политик. Политики связаны с обновлениями в кластере. Чем больше у вас доменов обновления, тем более гибким будет процесс обновления, потому что таким образом не только уменьшается влияние на кластер и запущенные службы, но и предотвращаются какие-либо сбои, связанные с воздействием на большое количество служб. Если увеличить количество доменов обновления, может увеличиться и время, необходимое для того, чтобы выполнить обновление в кластере. Этот нюанс зависит от других политик (например, от политики, которая определяет, сколько служба Service Fabric должна ждать после обновления домена, прежде чем перейти к следующему).

Поэтому диспетчер ресурсов собирает сведения о доменах обновления и распределяет реплики между доменами обновления в кластере так же, как и при работе с доменами сбоя. Домены обновления могут как соответствовать, так и не соответствовать доменам сбоя, но обычно они не должны соответствовать друг другу. На рисунке ниже изображен слой из нескольких доменов обновления, наложенный на слой ранее определенных доменов сбоя. Диспетчер ресурсов распределяет реплики в доменах, чтобы они не нагромождались в каком-то одном домене сбоя или обновления. Таким образом обеспечивается высокая доступность службы даже тогда, когда в кластере выполняется обновление или возникает сбой.

![Домены обновления][Image3]

Домены обновления и домены сбоя настраиваются в определении узла в манифесте кластера, как показано ниже.

``` xml
<Infrastructure>
    <WindowsServer>
      <NodeList>
        <Node NodeTypeRef="NodeType01" IsSeedNode="true" IPAddressOrFQDN="localhost" NodeName="Node1" FaultDomain="fd:/RACK1/BLADE1" UpgradeDomain="ud:/UD1"/>
        <Node NodeTypeRef="NodeType01" IsSeedNode="false" IPAddressOrFQDN="localhost" NodeName="Node2" FaultDomain="fd:/RACK2/BLADE1" UpgradeDomain="ud:/UD2"/>
        <Node NodeTypeRef="NodeType01" IsSeedNode="true" IPAddressOrFQDN="localhost" NodeName="Node3" FaultDomain="fd:/RACK3/BLADE2" UpgradeDomain="ud:/UD3"/>
        <Node NodeTypeRef="NodeType01" IsSeedNode="false" IPAddressOrFQDN="localhost" NodeName="Node4" FaultDomain="fd:/RACK2/BLADE2" UpgradeDomain="ud:/UD1"/>
        <Node NodeTypeRef="NodeType01" IsSeedNode="true" IPAddressOrFQDN="localhost" NodeName="Node5" FaultDomain="fd:/RACK1/BLADE2" UpgradeDomain="ud:/UD2"/>
        </NodeList>
    </WindowsServer>
</Infrastructure>
```
- В развертываниях Azure домены сбоя и обновления назначаются автоматически. Поэтому определение ваших узлов и ролей в параметре Infrastructure для Azure не содержит данные о доменах сбоя и обновления.

### Свойства узла
Свойства узла — это определяемые пользователем пары «ключ — значение», которые содержат дополнительные метаданные для того или иного узла. К свойством узла относится, например, наличие у него жесткого диска или видеоадаптера, количество шпинделей в его жестком диске, число ядер и другие физические свойства.

В свойствах узла можно указывать также более абстрактные свойства, которые тоже будут учитываться при размещении службы. Например, нескольким узлам в кластере можно назначить «цвет», что позволит разделить кластер на разные разделы. В приведенном ниже примере кода видно, что свойства узлов определяются в манифесте кластера в разделах с определениями типов узлов. Затем эти сведения можно применить к нескольким узлам кластера.

Свойства размещения NodeName, NodeType, FaultDomain и UpgradeDomain имеют значения по умолчанию. Платформа Service Fabric автоматически задает эти значения при создании службы, и вам остается только их принять. Не указывайте собственные свойства размещения с такими же именами.

``` xml
<NodeTypes>
  <NodeType Name="NodeType1">
    <PlacementProperties>
      <Property Name="HasDisk" Value="true"/>
      <Property Name="SpindleCount" Value="4"/>
      <Property Name="HasGPU" Value="true"/>
      <Property Name="NodeColor" Value="blue"/>
      <Property Name="NodeName" Value="Node1"/>
      <Property Name="NodeType" Value="NodeType1"/>
      <Property Name="FaultDomain" Value="fd:/RACK1/BLADE1"/>
      <Property Name="UpgradeDomain" Value="ud:/UD1"/>
    </PlacementProperties>
  </NodeType>
    <NodeType Name="NodeType2">
    <PlacementProperties>
      <Property Name="HasDisk" Value="false"/>
      <Property Name="SpindleCount" Value="-1"/>
      <Property Name="HasGPU" Value="false"/>
      <Property Name="NodeColor" Value="green"/>
    </PlacementProperties>
  </NodeType>
</NodeTypes>
```

Во время работы балансировщик ресурсов использует сведения о свойствах узла для того, чтобы разместить службы, требующие определенных возможностей, на соответствующих узлах.

### Емкость узла
Емкость узла — это пара «ключ — значение», которая определяет имя и количество конкретного ресурса, доступного для использования на конкретном узле. В примере кода ниже приведен узел с метрикой емкости MemoryInMb. Для узла выделено 2048 МБ памяти. Этот объем доступен по умолчанию. Как и свойства узла, емкость определяется в манифесте кластера.

``` xml
<NodeType Name="NodeType03">
  <Capacities>
    <Capacity Name="MemoryInMB" Value="2048"/>
    <Capacity Name="DiskSpaceInGB" Value="1024"/>
  </Capacities>
</NodeType>
```
![Емкость узла][Image4]

Так как работающие на узле службы могут изменять требования к емкости (посредством передачи данных о своей нагрузке), балансировщик ресурсов периодически проверяет, не превышена ли на узле какая-либо метрика емкости. Если метрика превышена, балансировщик ресурсов может переместить службы на менее загруженные узлы, чтобы уменьшить количество конфликтов, повысить общую производительность и оптимизировать использование ресурсов.

Хотя метрика может описываться в типе узла в разделе свойств, ее лучше указывать как емкость (при условии, что она является свойством узла, которое может потребляться во время выполнения). В таком случае служба, которая зависит от «минимальных требований к оборудованию», сможет опрашивать узел с ограничениями на размещение. Это может понадобиться, если ресурс потребляется другими службами в среде выполнения. Мы рекомендуем дополнительно указывать метрику как емкость, чтобы балансировщик ресурсов мог выполнять дополнительные действия.

Когда создаются новые службы, балансировщик ресурсов кластера Service Fabric использует сведения о емкости существующих узлов и использовании существующих служб, чтобы определить, достаточно ли емкости для размещения всей новой службы. Если емкости недостаточно, запрос на создание службы отклоняется с сообщением об ошибке, в котором указано, что у кластера недостаточно емкости.


### Настройки кластера для балансировщика ресурсов

В манифесте кластера указывается несколько разных значений конфигурации, которые определяют общее поведение балансировщика ресурсов.

- Пороговые значения балансировки определяют степень дисбаланса кластера по определенным метрикам, на основании чего балансировщик ресурсов предпринимает соответствующее действие. Пороговое значение балансировки — это максимальное отношение между максимально и минимально используемыми узлами, после превышения которого балансировщик ресурсов повторно балансирует кластер.

На приведенном ниже рисунке показаны два примера, в которых пороговое значение балансировки для заданной метрики равно 10.

![Пороговое значение балансировки][Image5]

Сейчас при определении «использования» узла не учитывается его размер (емкость). В расчет берется только фактическое использование узла для заданной метрики.

В этом примере кода показано, что пороговые значения балансировки настраиваются для каждой метрики отдельно с помощью элемента FabricSettings в манифесте кластера.

``` xml
<FabricSettings>
  <Section Name="MetricBalancingThresholds">
    <Parameter Name="MetricName" Value="10"/>
  </Section>
</FabricSettings>
```

- Пороговые значения активности определяют, как часто будет запускаться балансировщик ресурсов, путем сужения круга возможных ситуаций, на которые будет реагировать балансировщик ресурсов при высоких нагрузках. Так, если кластер не очень нагружен по определенной метрике, балансировщик ресурсов не запустится, даже если по этой метрике в кластере наблюдается значительный дисбаланс. Эта мера предотвращает расход ресурсов на балансировку кластера, которая даст всего лишь незначительное преимущество. На рисунке ниже указанное пороговое значение балансировки для метрики равно 4, а пороговое значение активности — 1536.

![Пороговое значение активности][Image6] Обратите внимание, что балансировщик ресурсов запустится только в том случае, когда для одной метрики будет превышено пороговое значение и активности, и балансировки. Активация одной из двух метрик не приведет к запуску балансировщика ресурсов.

В этом примере кода показано, что пороговые значения активности, так же как и пороговые значения балансировки, настраиваются для каждой метрики отдельно с помощью элемента FabricSettings в манифесте кластера.

``` xml
<FabricSettings>
  <Section Name="MetricActivityThresholds">
    <Parameter Name="MetricName" Value="1536"/>
  </Section>
</FabricSettings>
```

- PLBRefreshInterval определяет, как часто балансировщик ресурсов должен просматривать сведения, в которых нужно проверить нарушение ограничений. К нарушениям ограничений относятся: нарушения ограничений на размещение, наличие служб с меньшим количеством экземпляров или реплик, чем необходимо, наличие узлов с превышением емкости для некоторых метрик, наличие перегруженных доменов сбоя или обновления, дисбаланс кластера (используются пороговые значения балансировки и активности и данные о текущей нагрузке узлов в кластере). Интервал обновления задается в секундах и по умолчанию равен 1. Частотой проверки ограничений и размещения служб можно управлять также с помощью двух новых параметров (MinConstraintCheckInterval и MinPlacementInterval). Если эти параметры определены, параметр PLBRefreshInterval не используется и не может быть определен.

- MinConstraintCheckInterval определяет, как часто балансировщик ресурсов должен просматривать сведения, в которых нужно проверить нарушение ограничений. К нарушениям ограничений относятся: нарушения ограничений на размещение, наличие служб с меньшим количеством экземпляров или реплик, чем необходимо, наличие узлов с превышением емкости для некоторых метрик, наличие перегруженных доменов сбоя или обновления, дисбаланс кластера (используются пороговые значения балансировки и активности и данные о текущей нагрузке узлов в кластере). Интервал проверки ограничений задается в секундах. Если интервал не определен, его значение по умолчанию будет равно значению PLBRefreshInterval (оба значения нельзя указать одновременно).

- MinPlacementInterval определяет, как часто балансировщик ресурсов проверяет наличие новых экземпляров или реплик, которые нужно разместить, и пытается разместить их. Интервал размещения задается в секундах. Если интервал размещения не определен, его значение по умолчанию будет равно значению PLBRefreshInterval (оба значения нельзя указать одновременно).

- MinLoadBalancingInterval устанавливает минимальный период времени между циклами балансировки ресурсов. Если балансировщик ресурсов выполнил действие на основе проверки сведений, которая выполнялась в течение последнего интервала PLBRefreshInterval, он не запустится повторно на протяжении как минимум такого же периода времени. Интервал задается в секундах и по умолчанию равен 5.

Обратите внимание, что эти значения агрессивны, но обычно балансировка нагрузки в кластере возникает только тогда, когда для заданной метрики достигаются пороговые значения балансировки и активности. В приведенном ниже примере кода показано, что, если для конкретного кластера не требуется точная и активная балансировка нагрузки, агрессивность этих значений можно уменьшить с помощью настроек в элементе FabricSettings. В этом примере конфигурации минимальный период времени между двумя проверками ограничений составляет 10 секунд, между проверками на необходимость размещения — 5 секунд, а балансировка нагрузки будет выполняться каждые 5 минут. В этом случае параметр PLBRefreshInterval не определяется.

``` xml
<Section Name="PlacementAndLoadBalancing">
  <Parameter Name="MinPlacementInterval" Value="5" />
  <Parameter Name="MinConstraintChecknterval" Value="10" />
  <Parameter Name="MinLoadBalancingInterval" Value="600" />
</Section>
```

Во втором примере конфигурации параметры PLBRefreshInterval и MinLoadBalancingInterval определены. Так как параметр PLBRefreshInterval равен 2 секундам, значение 2 будет задано также для MinPlacementInterval и MinConstraintCheckInterval.

``` xml
<Section Name="PlacementAndLoadBalancing">
  <Parameter Name="PLBRefreshInterval" Value="2" />
  <Parameter Name="MinLoadBalancingInterval" Value="600" />
</Section>
```

<!--Every topic should have next steps and links to the next logical set of content to keep the customer engaged-->
## Дальнейшие действия

Дополнительные сведения см. в статье [Архитектура балансировщика ресурсов](service-fabric-resource-balancer-architecture.md).


[Image1]: media/service-fabric-resource-balancer-cluster-description/FD1.png
[Image2]: media/service-fabric-resource-balancer-cluster-description/FD2.png
[Image3]: media/service-fabric-resource-balancer-cluster-description/UD.png
[Image4]: media/service-fabric-resource-balancer-cluster-description/NC.png
[Image5]: media/service-fabric-resource-balancer-cluster-description/Config.png
[Image6]: media/service-fabric-resource-balancer-cluster-description/Thresholds.png
 

<!---HONumber=Sept15_HO2-->