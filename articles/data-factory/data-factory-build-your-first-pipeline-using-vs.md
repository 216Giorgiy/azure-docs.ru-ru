<properties
	pageTitle="Создание первого конвейера фабрики данных Azure с помощью Visual Studio"
	description="В этом учебнике вы создадите образец конвейера фабрики данных Azure с помощью Visual Studio."
	services="data-factory"
	documentationCenter=""
	authors="spelluru"
	manager="jhubbard"
	editor="monicar"/>

<tags
	ms.service="data-factory"
	ms.workload="data-services"
	ms.tgt_pltfrm="na"
	ms.devlang="na"
	ms.topic="article" 
	ms.date="08/18/2015"
	ms.author="spelluru"/>

# Создание первого конвейера фабрики данных Azure с помощью Visual Studio
> [AZURE.SELECTOR]
- [Tutorial Overview](data-factory-build-your-first-pipeline.md)
- [Using Data Factory Editor](data-factory-build-your-first-pipeline-using-editor.md)
- [Using PowerShell](data-factory-build-your-first-pipeline-using-powershell.md)
- [Using Visual Studio](data-factory-build-your-first-pipeline-using-vs.md)


Из этой статьи вы узнаете, как создать конвейер с помощью Visual Studio. В учебнике рассматриваются следующие действия:

2.	Создание связанных служб (хранилищ данных и служб вычислений).
3.	Создание набора данных.
3.	Создание конвейера.
4.	Создание фабрики данных и развертывание связанных служб, набора данных и конвейера. 

Здесь не приводятся общие сведения о службе фабрики данных Azure. Подробный обзор службы см. в статье [Введение в фабрику данных Azure](data-factory-introduction.md).

## Пошаговое руководство. Создание и развертывание сущностей фабрики данных с помощью Visual Studio 

### Предварительные требования

На вашем компьютере должны быть установлены следующие компоненты: Visual Studio 2013. Скачайте пакет SDK Azure для Visual Studio 2013. Перейдите к [Странице загрузки Azure](http://azure.microsoft.com/downloads/) и нажмите **Установить VS 2013** в разделе **.NET**.


### Создание проекта Visual Studio 
1. Запустите **Visual Studio 2013**. Щелкните **Файл**, наведите указатель мыши на пункт **Создать** и щелкните **Проект**. Откроется диалоговое окно **Новый проект**.  
2. В диалоговом окне **Новый проект** выберите шаблон **DataFactory** и нажмите **Пустой проект фабрики данных**. Если вы не видите шаблон DataFactory, закройте Visual Studio, установите пакет SDK Azure для Visual Studio 2013 и снова откройте Visual Studio.  

	![Диалоговое окно "Новый проект"](./media/data-factory-build-your-first-pipeline-using-vs/new-project-dialog.png)

3. Введите **имя** проекта, **расположение** и имя для **решения**, а затем нажмите кнопку **ОК**.

	![Обозреватель решений](./media/data-factory-build-your-first-pipeline-using-vs/solution-explorer.png)

### Создание связанных служб
На этом этапе вы свяжете учетную запись хранения Azure и кластер Azure HDInsight по требованию с фабрикой данных, а затем создадите набор данных, представляющий результаты выполнения сценария Hive.


#### Создание связанной службы хранения Azure


4. Щелкните правой кнопкой мыши **Связанные службы** в обозревателе решений, наведите указатель мыши на команду **Добавить** и нажмите **Новый элемент**.      
5. В диалоговом окне **Добавление нового элемента** выберите в списке пункт **Связанные службы хранилища Azure** и нажмите кнопку **Добавить**. 

	![Новая связанная служба](./media/data-factory-build-your-first-pipeline-using-vs/new-linked-service-dialog.png)
 
3. Замените **accountname** и **accountkey** на имя вашей учетной записи хранения Azure и ее ключ.

	![Связанная служба хранилища Azure](./media/data-factory-build-your-first-pipeline-using-vs/azure-storage-linked-service.png)

4. Сохраните файл **AzureStorageLinkedService1.json**.

#### Создание связанной службы Azure HDInsight
Теперь вы создадите связанную службу для кластера HDInsight по требованию, который будет использоваться для выполнения сценария Hive.

1. В **обозревателе решений** щелкните правой кнопкой мыши **Связанные службы**, наведите указатель мыши на команду **Добавить** и нажмите **Новый элемент**.
2. Выберите **Связанная служба HDInsight по запросу**, а затем нажмите **Добавить**. 
3. Замените фрагмент **JSON** на приведенный ниже код.

		{
		  "name": "HDInsightOnDemandLinkedService",
		  "properties": {
		    "type": "HDInsightOnDemand",
		    "typeProperties": {
		      "version": "3.1",
		      "clusterSize": 1,
		      "timeToLive": "00:30:00",
		      "linkedServiceName": "AzureStorageLinkedService1"
		    }
		  }
		}
	
	В следующей таблице приведены описания свойств JSON, используемых в этом фрагменте кода.
	
	Свойство | Описание
	-------- | -----------
	Version (версия) | Указывает, что версия создаваемого кластера HDInsight — 3.1. 
	ClusterSize (размер кластера) | Создает кластер HDInsight с одним узлом. 
	TimeToLive (срок жизни) | Указывает, сколько времени может простаивать кластер HDInsight, прежде чем он будет удален.
	JobsContainer (контейнер заданий) | Указывает имя контейнера заданий, в котором будут храниться журналы, создаваемые HDInsight.
	linkedServiceName (имя связанной службы) | Указывает имя учетной записи хранения, в которой будут храниться журналы, создаваемые HDInsight.

4. Сохраните файл **HDInsightOnDemandLinkedService1.json**.
 
### Создание выходного набора данных
Теперь вы создадите выходной набор данных, представляющий данные, которые хранятся в хранилище BLOB-объектов Azure.

1. В **обозревателе решений** щелкните команду **Добавить** правой кнопкой мыши и выберите пункт **Новый элемент**. 
2. Выберите из списка пункт **Большой двоичный объект Azure** и нажмите кнопку **Добавить**. 
3. Замените фрагмент **JSON** на приведенный ниже. Он создает набор данных с именем **AzureBlobOutput** и задает структуру данных, получаемых с помощью сценария Hive. Кроме того, нужно указать, что результаты будут храниться в контейнере больших двоичных объектов с именем **data** и в папке с именем **partitioneddata**. В разделе **availability** указывается частота, с которой будет создаваться выходной набор данных (ежемесячно).
	
		{
		  "name": "AzureBlobOutput",
		  "properties": {
		    "type": "AzureBlob",
		    "linkedServiceName": "AzureStorageLinkedService1",
		    "typeProperties": {
		      "folderPath": "data/partitioneddata",
		      "format": {
		        "type": "TextFormat",
		        "columnDelimiter": ","
		      }
		    },
		    "availability": {
		      "frequency": "Month",
		      "interval": 1
		    }
		  }
		}

4. Сохраните файл **AzureBlobLocation1.json**.


### Создание конвейера
На этом этапе вы создадите свой первый конвейер.

1. В **обозревателе решений** щелкните правой кнопкой мыши **Конвейеры**, наведите указатель мыши на команду **Добавить** и выберите **Новый элемент**. 
2. Выберите в списке пункт **Конвейер преобразования Hive** и нажмите кнопку **Добавить**. 
3. Замените фрагмент **JSON** на приведенный ниже код.

	> [AZURE.IMPORTANT]Замените свойство **storageaccountname** именем вашей учетной записи хранения.

		{
		  "name": "MyFirstPipeline",
		  "properties": {
		    "description": "My first Azure Data Factory pipeline",
		    "activities": [
		      {
		        "type": "HDInsightHive",
		        "typeProperties": {
		          "scriptPath": "script/partitionweblogs.hql",
		          "scriptLinkedService": "AzureStorageLinkedService1",
		          "defines": {
		            "partitionedtable": "wasb://data@<storageaccountname>.blob.core.windows.net/partitioneddata"
		          }
		        },
		        "outputs": [
		          {
		            "name": "AzureBlobOutput"
		          }
		        ],
		        "policy": {
		          "concurrency": 1,
		          "retry": 3
		        },
		        "name": "RunSampleHiveActivity",
		        "linkedServiceName": "HDInsightOnDemandLinkedService"
		      }
		    ],
		    "start": "2014-01-01",
		    "end": "2014-01-02"
		  }
		}

 	Этот фрагмент создает конвейер из одного действия, использующего Hive для обработки данных в кластере HDInsight.
	
	Файл сценария Hive **partitionweblogs.hql** хранится в учетной записи хранения Azure (задается с помощью свойства scriptLinkedService с именем **AzureStorageLinkedService1**) и в контейнере с именем **script**.

	Раздел **extendedProperties** используется для настройки параметров среды выполнения, которые будут передаваться в сценарий Hive в качестве значений конфигурации (например, ${hiveconf:PartitionedData}).

	Активный период конвейера задается с помощью свойств **start** и **end**.

	В JSON действия указывается, что сценарий Hive будет выполняться с использованием ресурса, указанного связанной службой **HDInsightOnDemandLinkedService**.
3. Сохраните файл **HiveActivity1.json**.

### Добавление partitionweblogs.hql в качестве зависимости 

1. В **окне обозревателя решений** щелкните **Зависимости** правой кнопкой мыши, выберите **Добавить** и нажмите кнопку **Существующий элемент**.  
2. Перейдите в папку **C:\\ADFGettingStarted**, выберите файл **partitionweblogs.hql** и нажмите кнопку **Добавить**. 

При публикации решения в рамках следующего шага HQL-файл загружается в контейнер сценариев, расположенный в хранилище больших двоичных объектов.

### Публикация и развертывание сущностей фабрики данных

18. Щелкните правой кнопкой мыши проект в обозревателе решений и выберите **Опубликовать**. 
19. Если вы видите диалоговое окно **Войдите в учетную запись Майкрософт**, введите данные учетной записи с подпиской Azure и нажмите кнопку **Войти**.
20. Вы должны увидеть следующее диалоговое окно:

	![Диалоговое окно «Опубликовать»](./media/data-factory-build-your-first-pipeline-using-vs/publish.png)

21. На странице «Настройка фабрики данных» выполните следующие действия.
	1. Выберите **Создать новую фабрику данных**.
	2. Введите в качестве **имени** фабрики **FirstPipelineUsingVS**.
	3. Выберите соответствующую подписку в поле **Подписка**. 
	4. Выберите **группу ресурсов** для создаваемой фабрики данных. 
	5. Выберите **регион** для фабрики данных. 
	6. Нажмите **Далее** для перехода на страницу **Публикация элементов**. (Нажмите на **ВКЛАДКУ** для выхода из поля «Имя», если кнопка **Далее** недоступна.) 
23. На странице **Публикация элементов** убедитесь, что выбраны все сущности данных фабрики, и нажмите кнопку **Далее** для перехода на страницу **Сводка**.     
24. Просмотрите сводку и нажмите кнопку **Далее** для запуска процесса развертывания и просмотра **Состояния развертывания**.
25. На странице **Состояние развертывания** вы увидите состояние процесса развертывания. После завершения развертывания нажмите кнопку «Готово». 
 

## Использование обозревателя серверов для просмотра сущностей фабрики данных

1. В **Visual Studio** щелкните **Вид** в меню и нажмите кнопку **Обозреватель серверов**.
2. В окне обозревателя серверов разверните элементы **Azure** и **Фабрика данных**. Если вы видите окно **Вход в Visual Studio**, введите данные **учетной записи**, связанной с подпиской Azure, и нажмите кнопку **Продолжить**. Введите **пароль** и нажмите **Войти**. Visual Studio пытается получить сведения обо всех фабриках данных Azure в подписке. Вы увидите состояние операции в окне **Список задач фабрики данных**.

	![Обозреватель серверов](./media/data-factory-build-your-first-pipeline-using-vs/server-explorer.png)
3. Чтобы создать проект Visual Studio на основе существующей фабрики данных, щелкните фабрику данных правой кнопкой мыши и выберите пункт **Экспорт фабрики данных в новый проект**.

	![Экспорт фабрики данных](./media/data-factory-build-your-first-pipeline-using-vs/export-data-factory-menu.png)

## Обновление средств фабрик данных для Visual Studio

Чтобы обновить средства фабрики данных Azure для Visual Studio, выполните следующие действия.

1. Нажмите **Средства** и выберите **Расширения и обновления**.
2. Выберите пункт **Обновления** слева, а затем — **Коллекция Visual Studio**.
3. Выберите **Средства фабрики данных Azure для Visual Studio** и нажмите кнопку **Обновить**. Если эта запись отсутствует, у вас уже установлена последняя версия средства. 

Инструкции по использованию портала предварительной версии Azure для мониторинга конвейера и наборов данных, созданных с помощью этого учебника, см. в разделе [Мониторинг наборов данных и конвейера](data-factory-monitor-manage-pipelines.md).
 

## Дальнейшие действия
В этой статье вы создали конвейер с действием преобразования (действие HDInsight), которое выполняет сценарий Hive в кластере HDInsight по требованию. Сведения о том, как копировать данные из хранилища больших двоичных объектов Azure в SQL Azure с помощью действия копирования, см. в учебнике [Копирование данных из хранилища больших двоичных объектов Azure в Azure SQL](data-factory-get-started.md).
  
## Отправить отзыв
Мы будем очень благодарны за ваш отзыв об этой статье. Уделите несколько минут тому, чтобы отправить его [по электронной почте](mailto:adfdocfeedback@microsoft.com?subject=data-factory-build-your-first-pipeline-using-vs.md).

<!---HONumber=Sept15_HO3-->