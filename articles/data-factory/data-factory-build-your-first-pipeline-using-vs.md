<properties
	pageTitle="Создание первого конвейера фабрики данных Azure с помощью Visual Studio"
	description="В этом учебнике вы создадите образец конвейера фабрики данных Azure с помощью Visual Studio."
	services="data-factory"
	documentationCenter=""
	authors="spelluru"
	manager="jhubbard"
	editor="monicar"/>

<tags
	ms.service="data-factory"
	ms.workload="data-services"
	ms.tgt_pltfrm="na"
	ms.devlang="na"
	ms.topic="hero-article" 
	ms.date="11/02/2015"
	ms.author="spelluru"/>

# Создание первого конвейера фабрики данных Azure с помощью Visual Studio
> [AZURE.SELECTOR]
- [Tutorial Overview](data-factory-build-your-first-pipeline.md)
- [Using Data Factory Editor](data-factory-build-your-first-pipeline-using-editor.md)
- [Using PowerShell](data-factory-build-your-first-pipeline-using-powershell.md)
- [Using Visual Studio](data-factory-build-your-first-pipeline-using-vs.md)


Из этой статьи вы узнаете, как создать конвейер с помощью Visual Studio. В учебнике рассматриваются следующие действия:

2.	Создание связанных служб (хранилищ данных и служб вычислений).
3.	Создание набора данных.
3.	Создание конвейера.
4.	Создание фабрики данных и развертывание связанных служб, набора данных и конвейера. 

Здесь не приводятся общие сведения о службе фабрики данных Azure. Подробный обзор службы см. в статье [Введение в фабрику данных Azure](data-factory-introduction.md).

> [AZURE.IMPORTANT]Перед выполнением этого учебника, пожалуйста, перейдите к статье [Обзор учебника](data-factory-build-your-first-pipeline.md) и выполните необходимые предварительные действия.

## Пошаговое руководство. Создание и развертывание сущностей фабрики данных с помощью Visual Studio 

### Предварительные требования

На вашем компьютере должны быть установлены следующие компоненты:

- Visual Studio 2013 или Visual Studio 2015.
- Загрузите пакет SDK Azure для Visual Studio 2013 или Visual Studio 2015. Перейдите к [Странице загрузки Azure](http://azure.microsoft.com/downloads/) и щелкните **VS 2013** или **VS2015** в разделе **.NET**.
- Загрузите последнюю версию подключаемого модуля фабрики данных Azure для Visual Studio: [VS 2013](https://visualstudiogallery.msdn.microsoft.com/754d998c-8f92-4aa7-835b-e89c8c954aa5) или [VS 2015](https://visualstudiogallery.msdn.microsoft.com/371a4cf9-0093-40fa-b7dd-be3c74f49005). При использовании Visual Studio 2013 можно обновить подключаемый модуль, выполнив следующие действия: в меню выберите команду **Инструменты** -> **Расширения и обновления** -> **Online** -> **Коллекции Visual Studio** -> **Инструменты фабрики данных Microsoft Azure для Visual Studio** -> **Обновить**. 
	
	

### Создание проекта Visual Studio 
1. Запустите **Visual Studio 2013** или **Visual Studio 2015**. Щелкните **Файл**, наведите указатель мыши на пункт **Создать** и щелкните **Проект**. Откроется диалоговое окно **Новый проект**.  
2. В диалоговом окне **Новый проект** выберите шаблон **Фабрика данных** и нажмите **Пустой проект фабрики данных**.   

	![Диалоговое окно "Новый проект"](./media/data-factory-build-your-first-pipeline-using-vs/new-project-dialog.png)

3. Введите **имя** проекта, **расположение** и имя **решения**, а затем нажмите кнопку **ОК**.

	![Обозреватель решений](./media/data-factory-build-your-first-pipeline-using-vs/solution-explorer.png)

### Создание связанных служб
На этом этапе вы свяжете учетную запись хранения Azure и кластер Azure HDInsight по требованию с фабрикой данных, а затем создадите набор данных, представляющий результаты выполнения сценария Hive.


#### Создание связанной службы хранения Azure


4. Щелкните правой кнопкой мыши **Связанные службы** в обозревателе решений, наведите указатель мыши на команду **Добавить** и нажмите **Новый элемент**.      
5. В диалоговом окне **Добавление нового элемента** выберите в списке пункт **Связанные службы хранилища Azure** и нажмите кнопку **Добавить**. 

	![Новая связанная служба](./media/data-factory-build-your-first-pipeline-using-vs/new-linked-service-dialog.png)
 
3. Замените **accountname** и **accountkey** на имя вашей учетной записи хранения Azure и ее ключ.

	![Связанная служба хранилища Azure](./media/data-factory-build-your-first-pipeline-using-vs/azure-storage-linked-service.png)

4. Сохраните файл **AzureStorageLinkedService1.json**.

#### Создание связанной службы Azure HDInsight
Теперь вы создадите связанную службу для кластера HDInsight по требованию, который будет использоваться для выполнения сценария Hive.

1. В **обозревателе решений** щелкните правой кнопкой мыши **Связанные службы**, выберите **Добавить** и нажмите **Новый элемент**.
2. Выберите **Связанная служба HDInsight по запросу**, а затем нажмите **Добавить**. 
3. Замените фрагмент **JSON** на приведенный ниже код.

		{
		  "name": "HDInsightOnDemandLinkedService",
		  "properties": {
		    "type": "HDInsightOnDemand",
		    "typeProperties": {
		      "version": "3.1",
		      "clusterSize": 1,
		      "timeToLive": "00:30:00",
		      "linkedServiceName": "AzureStorageLinkedService1"
		    }
		  }
		}
	
	В следующей таблице приведены описания свойств JSON, используемых в этом фрагменте кода.
	
	Свойство | Описание
	-------- | -----------
	Version (версия) | Указывает, что версия создаваемого кластера HDInsight — 3.1. 
	ClusterSize (размер кластера) | Создает кластер HDInsight с одним узлом. 
	TimeToLive (срок жизни) | Указывает, сколько времени может простаивать кластер HDInsight, прежде чем он будет удален.
	linkedServiceName (имя связанной службы) | Указывает имя учетной записи хранения, в которой будут храниться журналы, создаваемые HDInsight.

4. Сохраните файл **HDInsightOnDemandLinkedService1.json**.
 
### Создание выходного набора данных
Теперь вы создадите выходной набор данных, представляющий данные, которые хранятся в хранилище BLOB-объектов Azure.

1. В **обозревателе решений** щелкните команду **Добавить** правой кнопкой мыши и выберите пункт **Новый элемент**. 
2. Выберите в списке пункт **BLOB-объект Azure** и нажмите кнопку **Добавить**. 
3. Замените фрагмент **JSON** на приведенный ниже. Будет создан набор данных с именем **AzureBlobOutput** и определена структура данных, получаемых с помощью сценария Hive. Кроме того, нужно указать, что результаты будут храниться в контейнере больших двоичных объектов с именем **data** в папке с именем **partitioneddata**. В разделе **availability** указывается частота, с которой будет создаваться выходной набор данных (ежемесячно).
	
		{
		  "name": "AzureBlobOutput",
		  "properties": {
		    "type": "AzureBlob",
		    "linkedServiceName": "AzureStorageLinkedService1",
		    "typeProperties": {
		      "folderPath": "data/partitioneddata",
		      "format": {
		        "type": "TextFormat",
		        "columnDelimiter": ","
		      }
		    },
		    "availability": {
		      "frequency": "Month",
		      "interval": 1
		    }
		  }
		}

4. Сохраните файл **AzureBlobLocation1.json**.


### Создание конвейера
На этом этапе вы создадите свой первый конвейер.

1. В **обозревателе решений** щелкните правой кнопкой мыши **Конвейеры**, выберите команду **Добавить** и щелкните **Новый элемент**. 
2. Выберите в списке пункт **Конвейер преобразования Hive** и нажмите кнопку **Добавить**. 
3. Замените фрагмент **JSON** на приведенный ниже код.

	> [AZURE.IMPORTANT]Замените свойство **storageaccountname** именем вашей учетной записи хранения.

		{
		  "name": "MyFirstPipeline",
		  "properties": {
		    "description": "My first Azure Data Factory pipeline",
		    "activities": [
		      {
		        "type": "HDInsightHive",
		        "typeProperties": {
		          "scriptPath": "script/partitionweblogs.hql",
		          "scriptLinkedService": "AzureStorageLinkedService1",
		          "defines": {
		            "partitionedtable": "wasb://data@<storageaccountname>.blob.core.windows.net/partitioneddata"
		          }
		        },
		        "outputs": [
		          {
		            "name": "AzureBlobOutput"
		          }
		        ],
		        "policy": {
		          "concurrency": 1,
		          "retry": 3
		        },
		        "name": "RunSampleHiveActivity",
		        "linkedServiceName": "HDInsightOnDemandLinkedService"
		      }
		    ],
		    "start": "2014-01-01",
		    "end": "2014-01-02"
		  }
		}

 	Этот фрагмент создает конвейер из одного действия, использующего Hive для обработки данных в кластере HDInsight.
	
	Файл сценария Hive **partitionweblogs.hql** хранится в учетной записи хранения Azure (задается с помощью свойства scriptLinkedService с именем **AzureStorageLinkedService1**) в контейнере с именем **script**.

	Раздел **defines** используется для настройки параметров среды выполнения, которые будут переданы в сценарий Hive в качестве значений конфигурации (например, ${hiveconf:PartitionedData}).

	Активный период конвейера задается с помощью свойств **start** и **end**.

	В действии JSON вы указываете, что сценарий Hive будет выполняться на компьютере, который определяет связанная служба **HDInsightOnDemandLinkedService**.
3. Сохраните файл **HiveActivity1.json**.

### Добавление partitionweblogs.hql в качестве зависимости 

1. В **окне обозревателя решений** щелкните **Зависимости** правой кнопкой мыши, выберите команду **Добавить** и щелкните **Существующий элемент**.  
2. Перейдите в папку **C:\\ADFGettingStarted**, выберите файл **partitionweblogs.hql** и щелкните **Добавить**. 

При публикации решения в рамках следующего шага HQL-файл загружается в контейнер сценариев, расположенный в хранилище больших двоичных объектов.

### Публикация и развертывание сущностей фабрики данных

18. В обозревателе решений щелкните правой кнопкой мыши проект и выберите **Опубликовать**. 
19. Когда отобразиться диалоговое окно **Вход в учетную запись Майкрософт**, введите данные учетной записи для подписки Azure и нажмите кнопку **Войти**.
20. Вы должны увидеть следующее диалоговое окно:

	![Диалоговое окно «Опубликовать»](./media/data-factory-build-your-first-pipeline-using-vs/publish.png)

21. На странице «Настройка фабрики данных» выполните следующие действия.
	1. Выберите элемент **Создать фабрику данных**.
	2. Присвойте фабрике **имя** **FirstPipelineUsingVS**. 
	
		> [AZURE.IMPORTANT]Имя фабрики данных Azure должно быть глобально уникальным. Если при публикации появится сообщение об ошибке **Имя фабрики данных FirstPipelineUsingVS недоступно**, измените имя (например, в формате ваше\_имя\_FirstPipelineUsingVS). Ознакомьтесь с разделом [Фабрика данных — правила именования](data-factory-naming-rules.md), чтобы узнать о правилах именования артефактов фабрики данных.
		> 
		> В будущем имя фабрики данных может быть зарегистрировано в качестве DNS-имени и, следовательно, стать отображаемым.
	3. Выберите соответствующую подписку в поле **Подписка**. 
	4. Выберите **группу ресурсов** для создаваемой фабрики данных. 
	5. Выберите **регион** для фабрики данных. 
	6. Нажмите кнопку **Далее**, чтобы перейти на страницу **Публикация элементов**. (Нажмите клавишу **TAB**, чтобы выйти из поля «Имя», если кнопка **Далее** недоступна). 
23. На странице **Публикация элементов** убедитесь, что выбраны все сущности фабрик данных. Затем нажмите кнопку **Далее**, чтобы перейти на страницу **Сводка**.     
24. Просмотрите сводку и нажмите кнопку **Далее**, чтобы запустить процесс развертывания и просмотреть **состояние развертывания**.
25. На странице **Состояние развертывания** вы увидите состояние процесса развертывания. После завершения развертывания нажмите кнопку «Готово». 
 

## Использование обозревателя серверов для просмотра сущностей фабрики данных

1. В меню **Visual Studio** выберите элемент **Вид** и щелкните **Обозреватель сервера**.
2. В окне обозревателя сервера разверните элементы **Azure** и **Фабрика данных**. Когда отобразится окно **Вход в Visual Studio**, введите данные **учетной записи**, связанной с вашей подпиской Azure, и нажмите кнопку **Продолжить**. Введите **пароль** и нажмите кнопку **Войти**. Visual Studio пытается получить сведения обо всех фабриках данных Azure в подписке. В окне **Список задач фабрики данных** будет отображено состояние операции.

	![Обозреватель серверов](./media/data-factory-build-your-first-pipeline-using-vs/server-explorer.png)
3. Чтобы создать проект Visual Studio на основе существующей фабрики данных, щелкните фабрику данных правой кнопкой мыши и выберите пункт **Экспорт фабрики данных в новый проект**.

	![Экспорт фабрики данных](./media/data-factory-build-your-first-pipeline-using-vs/export-data-factory-menu.png)

## Обновление средств фабрик данных для Visual Studio

Чтобы обновить средства фабрики данных Azure для Visual Studio, выполните следующие действия.

1. Щелкните в меню пункт **Сервис** и выберите элемент **Расширения и обновления**.
2. На панели слева выберите пункт **Обновления**, а затем — **Коллекция Visual Studio**.
3. Выберите **Средства фабрики данных Azure для Visual Studio** и нажмите кнопку **Обновить**. Если эта запись отсутствует, у вас уже установлена последняя версия средства. 

Инструкции по использованию портала предварительной версии Azure для мониторинга конвейера и наборов данных, созданных с помощью этого руководства, см. в разделе [Мониторинг наборов данных и конвейера](data-factory-monitor-manage-pipelines.md).
 

## Дальнейшие действия
В этой статье вы создали конвейер с действием преобразования (действие HDInsight), которое выполняет сценарий Hive в кластере HDInsight по требованию. Сведения о том, как копировать данные из хранилища больших двоичных объектов Azure в SQL Azure с помощью действия копирования, см. в статье [Руководство по копированию данных из хранилища больших двоичных объектов Azure в Azure SQL](data-factory-get-started.md).
  

<!---HONumber=AcomDC_1125_2015-->