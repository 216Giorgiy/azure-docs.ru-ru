<properties 
	pageTitle="Перемещение данных — шлюз управления данными | Microsoft Azure"
	description="Настройка шлюза данных для перемещения данных между локальными узлами и облаком. Используйте шлюз управления данными в фабрике данных Azure для перемещения данных." 
    keywords="шлюз данных, интеграции данных, перемещение данных, учетные данные шлюза"
	services="data-factory" 
	documentationCenter="" 
	authors="spelluru" 
	manager="jhubbard" 
	editor="monicar"/>

<tags 
	ms.service="data-factory" 
	ms.workload="data-services" 
	ms.tgt_pltfrm="na" 
	ms.devlang="na" 
	ms.topic="article" 
	ms.date="09/12/2016" 
	ms.author="spelluru"/>

# Перемещение данных между локальными источниками и облаком с помощью шлюза управления данными
Эта статья содержит общие сведения об интеграции данных, хранящихся в локальных и облачных хранилищах данных, с помощью фабрики данных. В статье используются понятия, описанные в статье [Действия по перемещению данных](data-factory-data-movement-activities.md) и других основополагающих статьях о фабрике данных: [наборы данных](data-factory-create-datasets.md) и [конвейеры](data-factory-create-pipelines.md).

## Шлюз управления данными
Шлюз управления данными необходимо установить на локальный компьютер, чтобы обеспечить перемещение данных из локального хранилища данных и в него. Шлюз можно установить на том же компьютере, на котором размещается хранилище, или на другом компьютере. Важно, чтобы шлюз мог подключиться к хранилищу данных. Все дополнительные сведения о шлюзе управления данными см. в статье [Шлюз управления данными](data-factory-data-management-gateway.md).

Приведенное ниже пошаговое руководство поможет вам создать фабрику данных с конвейером, который перемещает данные из локальной базы данных SQL Server в большой двоичный объект Azure. В рамках этого пошагового руководства вы установите и настроите шлюз управления данными на своем компьютере.

## Пошаговое руководство: копирование локальных данных в облако
  
## Создание фабрики данных
На этом этапе вы с помощью портала Azure создадите экземпляр фабрики данных Azure с именем **ADFTutorialOnPremDF**. Фабрику данных также можно создать с помощью командлетов фабрики данных Azure.

1.	После входа на [портал Azure](https://portal.azure.com) щелкните **Создать** в нижнем левом углу, выберите **Анализ данных** в колонке **Создать**, а затем щелкните **Фабрика данных** в колонке **Анализ данных**.

	![Создать -> Фабрика данных](./media/data-factory-move-data-between-onprem-and-cloud/NewDataFactoryMenu.png)
  
6. В колонке **Создать фабрику данных** выполните следующие действия.
	1. Введите **имя** **ADFTutorialOnPremDF**.
	2. Щелкните **ИМЯ ГРУППЫ РЕСУРСОВ** и выберите **ADFTutorialResourceGroup**. Вы можете выбрать существующую группу ресурсов или создать новую. Чтобы создать группу ресурсов:
		1. Щелкните **Создать новую группу ресурсов**.
		2. В колонке **Создать группу ресурсов** введите **имя** для группы ресурсов и нажмите кнопку **ОК**.

7. Обратите внимание, что в колонке **Новая фабрика данных** установлен флажок **Add to Startboard** (Добавить на начальную панель).

	![Добавить на начальную панель](./media/data-factory-move-data-between-onprem-and-cloud/OnPremNewDataFactoryAddToStartboard.png)

8. В колонке **Новая фабрика данных** щелкните **Создать**.

	Имя фабрики данных Azure должно быть глобально уникальным. Получив сообщение об ошибке **Имя фабрики данных "ADFTutorialOnPremDF" недоступно**, измените имя фабрики данных (например, на yournameADFTutorialOnPremDF) и попробуйте создать ее еще раз. Выполняя оставшиеся действия, описанные в этом руководстве, вместо ADFTutorialOnPremDF используйте именно это имя.

9. Найдите уведомления, возникшие в процессе создания, нажав кнопку **Уведомления** в строке заголовка, как показано ниже. Чтобы закрыть окно уведомлений, нажмите эту кнопку еще раз.

	![Раздел "УВЕДОМЛЕНИЯ"](./media/data-factory-move-data-between-onprem-and-cloud/OnPremNotificationsHub.png)

11. После завершения процедуры создания фабрики данных появится колонка **Фабрика данных**, как показано на рисунке ниже.

	![Домашняя страница фабрики данных](./media/data-factory-move-data-between-onprem-and-cloud/OnPremDataFactoryHomePage.png)

## Создание шлюза
5. В колонке **ФАБРИКА ДАННЫХ** щелкните плитку **Разработка и развертывание**, чтобы запустить **редактор** для фабрики данных.

	![Плитка "Создание и развертывание"](./media/data-factory-move-data-between-onprem-and-cloud/author-deploy-tile.png)
6.	В редакторе фабрики данных щелкните **... (многоточие)** на панели инструментов, а затем **Новый шлюз данных**.

	![«Новый шлюз данных» на панели инструментов](./media/data-factory-move-data-between-onprem-and-cloud/NewDataGateway.png)
2. В колонке **Создать** в поле **Имя** введите **adftutorialgateway** и нажмите кнопку **ОК**.

	![Колонка "Создать шлюз"](./media/data-factory-move-data-between-onprem-and-cloud/OnPremCreateGatewayBlade.png)

3. В колонке **Настройка** щелкните **Установить непосредственно на этот компьютер**. Это позволит скачать пакет установки для шлюза, а также установить, настроить и зарегистрировать шлюз на компьютере.

	> [AZURE.NOTE] 
	Используйте Internet Explorer или другой веб-браузер, совместимый с Microsoft ClickOnce.
	> 
	> Если вы используете браузер Chrome, перейдите в [интернет-магазин Chrome](https://chrome.google.com/webstore/), введите ClickOnce в строке поиска, а затем выберите и установите одно из расширений ClickOnce.
	>  
	> То же самое (установку надстройки) сделайте и в случае с браузером Firefox. Нажмите кнопку **Открыть меню** на панели инструментов (**три горизонтальные линии** в правом верхнем углу), щелкните **Надстройки**, введите ключевое слово "ClickOnce" в строке поиска, выберите одно из расширений ClickOnce и установите его.

	![Шлюз — колонка "Настройка"](./media/data-factory-move-data-between-onprem-and-cloud/OnPremGatewayConfigureBlade.png)

	Это самый простой способ (одним щелчком) скачать, установить, настроить и зарегистрировать шлюз в один прием. Вы увидите, что на компьютере установлено приложение **Microsoft Data Management Gateway Configuration Manager**. Вы также можете найти исполняемый файл **ConfigManager.exe** в папке по следующему пути: **C:\\Program Files\\Microsoft Data Management Gateway\\2.0\\Shared**.

	Шлюз также можно скачать и установить вручную, используя ссылки в этой колонке. Затем вы можете зарегистрировать его с помощью ключа, указанного в текстовом поле **СОЗДАТЬ КЛЮЧ**.
	
	Все дополнительные сведения о шлюзе см. в статье [Шлюз управления данными](data-factory-data-management-gateway.md).

	>[AZURE.NOTE] Для успешной установки шлюза управления данными и его настройки вы должны обладать правами администратора на локальном компьютере. В локальную группу Windows "Пользователи шлюза управления данными" можно добавить дополнительных пользователей. Участники этой группы могут использовать диспетчер конфигурации шлюза управления данными для настройки шлюза.

5. Подождите несколько минут и запустите на компьютере приложение **Диспетчер конфигураций шлюза управления данными**. Для этого введите текст **шлюз управления данными** в окне **Поиск**. Вы также можете найти исполняемый файл **ConfigManager.exe** в папке по следующему пути: **C:\\Program Files\\Microsoft Data Management Gateway\\2.0\\Shared**.

	![Диспетчер конфигурации шлюза](./media/data-factory-move-data-between-onprem-and-cloud/OnPremDMGConfigurationManager.png)

6. Подождите, пока не будут выполнены следующие условия:
	2. В поле **Имя шлюза** установлено значение **adftutorialgateway**.
	4. В строке состояния внизу отображается надпись **Подключение к облачной службе установлено** и **зеленая галочка**.

	На вкладке **Главная** можно также выполнить следующее: **зарегистрировать** шлюз, используя ключ с портала Azure, с помощью кнопки "Зарегистрировать"; **остановить** службу узла шлюза управления данными на компьютере шлюза; **запланировать установку обновлений** на определенное время суток; просмотреть, когда шлюз был **обновлен последний раз**.

8. Переключитесь на вкладку **Параметры**. Сертификат, указанный в разделе **Сертификат**, используется для шифрования и расшифровки учетных данных локального хранилища данных, указанного на портале. Щелкните **Изменить**, чтобы использовать собственный сертификат (необязательно). По умолчанию шлюз использует сертификат, который автоматически создан службой фабрики данных.

	![Конфигурация сертификата шлюза](./media/data-factory-move-data-between-onprem-and-cloud/gateway-certificate.png)

	Кроме того, на вкладке "Параметры" можно: просмотреть или экспортировать сертификат, используемый шлюзом; изменить конечную точку HTTPS, используемую шлюзом.
9. (Необязательно.) Для устранения проблем в работе шлюза вы можете использовать подробное ведение журнала. Чтобы включить эту функцию, перейдите на вкладку **Диагностика** и установите флажок **Включить запись подробных сведений в журнал**. Данные журналов можно изучить в **Обозревателе событий**, открыв узел **Журналы приложений и служб** -> **Шлюз управления данными**.

	![Вкладка «Диагностика»](./media/data-factory-move-data-between-onprem-and-cloud/diagnostics-tab.png)

	На вкладке **Диагностика** можно выполнить также такие действия:
	
	- Щелкните **Проверить подключение**, чтобы проверить подключение к локальному источнику данных через шлюз.
	- Щелкните **Просмотреть журналы**, чтобы просмотреть журнал шлюза управления данными в окне просмотра событий.
	- Щелкните **Оправить журналы**, чтобы отправить ZIP-файл с журналами за последние семь дней в корпорацию Майкрософт. Это поможет упростить устранение неполадок, с которыми вы сталкиваетесь.
10. На портале Azure нажмите кнопку **ОК** в колонке **Настройка**, а затем в колонке **Новый шлюз данных**.
6. В иерархической структуре слева найдите элемент **adftutorialgateway** в узле **Шлюзы данных**. Щелкните его, чтобы увидеть связанные файлы JSON.
	

## Создание связанных служб 
На этом этапе вы создадите две связанные службы: **AzureStorageLinkedService** и **SqlServerLinkedService**. Служба **SqlServerLinkedService** связывает с фабрикой данных локальную базу данных SQL Server, а служба **AzureStorageLinkedService** — хранилище BLOB-объектов Azure. Далее, это руководство поможет вам создать конвейер, который копирует данные из локальной базы данных SQL Server в службу хранилища BLOB-объектов Azure.

#### Добавление связанной службы в локальную базу данных SQL Server
1.	В **редакторе фабрики данных** щелкните **Создать хранилище данных** на панели инструментов и выберите **SQL Server**.

	![Создать связанную службу SQL Server](./media/data-factory-move-data-between-onprem-and-cloud/NewSQLServer.png)
3.	В **редакторе JSON** выполните следующие действия.
	1. Для параметра **gatewayName** укажите значение **adftutorialgateway**.
	2. При использовании проверки подлинности Windows:
		1. В строке подключения **connectionString** задайте для параметра **Встроенная система безопасности** значение **true**, укажите **имя сервера** базы данных и **имя базы данных** и удалите **идентификатор пользователя** и **пароль**.
		3. Укажите имя пользователя и пароль в свойствах **userName** и **password**.
		
				"typeProperties": {
            		"connectionString": "Data Source=<servername>;Initial Catalog=<databasename>;Integrated Security=True;",
            		"gatewayName": "adftutorialgateway",
            		"userName": "<Specify user name if you are using Windows Authentication. Example: <domain>\<user>",
            		"password": "<Specify password for the user account>"
        		}

	4. При использовании проверки подлинности SQL:
		1. Укажите **имя сервера** базы данных, **имя базы данных**, **идентификатор пользователя** и **пароль** в строке **connectionString**.
		2. Удалите из JSON два последних свойства: **userName** и **password**.
		3. Удалите символ `,` (запятая)** в конце строки, в которой указано значение свойства **gatewayName**.

				"typeProperties": {
            		"connectionString": "Data Source=<servername>;Initial Catalog=<databasename>;Integrated Security=False;User ID=<username>;Password=<password>;",
	           		"gatewayName": "<Name of the gateway that the Data Factory service should use to connect to the on-premises SQL Server database>"
    		    }
	
		Учетные данные **зашифрованы** с помощью сертификата, который принадлежит службе фабрики данных. Если вы хотите использовать вместо него сертификат, связанный со шлюзом управления данными, см. раздел, посвященный [безопасной настройке учетных данных](#set-credentials-and-security).
    
2.	Нажмите кнопку **Развернуть** на панели команд, чтобы развернуть связанную службу SQL Server.

#### Добавление связанной службы для учетной записи хранения Azure
 
1. В **редакторе фабрики данных** щелкните **Создать хранилище данных** на панели команд и выберите **Хранилище Azure**.
2. В поле **Имя учетной записи** введите имя учетной записи хранения Azure.
3. В поле **Ключ учетной записи** введите ключ учетной записи хранения Azure.
4. Щелкните **Развернуть**, чтобы развернуть службу **AzureStorageLinkedService**.
   
 
## Создание наборов данных
На этом этапе вы создадите наборы входных и выходных данных, которые представляют собой входные и выходные данные для операции копирования (из локальной базы данных SQL Server в хранилище BLOB-объектов Azure). Перед созданием наборов данных или таблиц (прямоугольных наборов данных) необходимо сделать следующее (после списка есть подробные шаги):

- в базе данных SQL Server, которую вы добавили в фабрику данных как связанную службу, создайте таблицу с именем **emp** и вставьте в таблицу пару записей в качестве примера;
- В учетной записи хранения BLOB-объектов Azure, которую вы добавили в качестве связанной службы в фабрику данных, создайте контейнер BLOB-объектов с именем **adftutorial**.

### Подготовка локальной связанной службы SQL Server для учебника

1. В базе данных SQL Server, которую вы указали для локальных связанных служб (**SqlServerLinkedService**), используйте следующий сценарий SQL, чтобы создать в базе данных таблицу **emp**.


        CREATE TABLE dbo.emp
		(
			ID int IDENTITY(1,1) NOT NULL, 
			FirstName varchar(50),
			LastName varchar(50),
    		CONSTRAINT PK_emp PRIMARY KEY (ID)
		)
		GO
 

2. Вставьте несколько образцов в таблицу:


        INSERT INTO emp VALUES ('John', 'Doe')
		INSERT INTO emp VALUES ('Jane', 'Doe')



### Создание входной таблицы

1. В **редакторе фабрики данных** щелкните на панели команд элемент **Создать набор данных** и выберите пункт **Таблица SQL Server**.
2.	Замените сценарий JSON в области справа на следующий текст:

		{
		  "name": "EmpOnPremSQLTable",
		  "properties": {
		    "type": "SqlServerTable",
		    "linkedServiceName": "SqlServerLinkedService",
		    "typeProperties": {
		      "tableName": "emp"
		    },
		    "external": true,
		    "availability": {
		      "frequency": "Hour",
		      "interval": 1
		    },
		    "policy": {
		      "externalData": {
		        "retryInterval": "00:01:00",
		        "retryTimeout": "00:10:00",
		        "maximumRetry": 3
		      }
		    }
		  }
		}

	Обратите внимание на следующее.
	
	- **type** имеет значение **SqlServerTable**.
	- **tableName** имеет значение **emp**.
	- Для параметра **linkedServiceName** установлено значение **SqlServerLinkedService** (вы создали эту связанную службу в шаге 2).
	- Если входная таблица не создается другим конвейером фабрики данных Azure, для параметра **external** следует задать значение **true**. Это означает, что входные данные создаются вне службы фабрики данных Azure. При необходимости можно указать любые внешние политики данных с помощью **externalData** в разделе **Policy**.

	Дополнительную информацию о свойствах JSON-сценариев см. в статье [Справка по использованию JSON-сценариев][json-script-reference].

2. Нажмите кнопку **Развернуть** на панели команд, чтобы развернуть набор данных (таблица представляет собой прямоугольный набор данных). Убедитесь, что в заголовке окна отображается сообщение **ТАБЛИЦА УСПЕШНО РАЗВЕРНУТА**.


### Создание выходной таблицы

1.	В **редакторе фабрики данных** щелкните на панели команд **Создать набор данных** и выберите **Хранилище больших двоичных объектов Azure**.
2.	Замените сценарий JSON в области справа на следующий текст:

		{
		  "name": "OutputBlobTable",
		  "properties": {
		    "type": "AzureBlob",
		    "linkedServiceName": "AzureStorageLinkedService",
		    "typeProperties": {
		      "folderPath": "adftutorial/outfromonpremdf",
		      "format": {
		        "type": "TextFormat",
		        "columnDelimiter": ","
		      }
		    },
		    "availability": {
		      "frequency": "Hour",
		      "interval": 1
		    }
		  }
		}
  
	Обратите внимание на следующее;
	
	- для параметра **type** задано значение **AzureBlob**;
	- для параметра **linkedServiceName** установлено значение **AzureStorageLinkedService** (вы создали эту связанную службу на шаге 2);
	- для параметра **folderPath** установлено значение **adftutorial/outfromonpremdf**, где outfromonpremdf — это папка в контейнере adftutorial (если контейнера **adftutorial** еще не существует, создайте его);
	- параметр **availability** имеет значение **hourly** (параметру **frequency** присваивается значение **hour**, а параметру **interval** — значение **1**). Служба фабрики данных каждый час создает срез выходных данных в таблице **emp** в базе данных SQL Azure.

	Если параметр **fileName** для **входной таблицы** не задан, все файлы и большие двоичные объекты из входной папки (**folderPath**) считаются входными данными. Если указать fileName в JSON, только указанный файл или большой двоичный объект рассматриваются как входные данные. Примеры файлов см. в [учебнике][adf-tutorial].
 
	Если не указать **fileName** (имя файла) для **выходной таблицы**, созданные в каталоге **folderPath** файлы получат имена в таком формате: Data.<Guid>.txt (например, Data.0a405f8a-93ff-4c6f-b3be-f69616f1df7a.txt.).

	Для динамической установки папки **folderPath** и имени **fileName** на основе времени **SliceStart**, используйте свойство partitionedBy В следующем примере folderPath использует год, месяц и день из SliceStart (время начала обработки среза), а в fileName используется время (часы) из SliceStart. Например, если срез выполняется для временной отметки 2014-10-20T08:00:00, folderName получает значение wikidatagateway/wikisampledataout/2014/10/20, а fileName – 08.csv.

	  	"folderPath": "wikidatagateway/wikisampledataout/{Year}/{Month}/{Day}",
        "fileName": "{Hour}.csv",
        "partitionedBy": 
        [
        	{ "name": "Year", "value": { "type": "DateTime", "date": "SliceStart", "format": "yyyy" } },
            { "name": "Month", "value": { "type": "DateTime", "date": "SliceStart", "format": "MM" } }, 
            { "name": "Day", "value": { "type": "DateTime", "date": "SliceStart", "format": "dd" } }, 
            { "name": "Hour", "value": { "type": "DateTime", "date": "SliceStart", "format": "hh" } } 
        ],

 

	Дополнительную информацию о свойствах JSON-сценариев см. в статье [Справка по использованию JSON-сценариев][json-script-reference].

2.	Нажмите кнопку **Развернуть** на панели команд, чтобы развернуть набор данных (таблица представляет собой прямоугольный набор данных). Убедитесь, что в заголовке окна отображается сообщение **ТАБЛИЦА УСПЕШНО РАЗВЕРНУТА**.
  

## Создание конвейера
На этом шаге вы создадите **конвейер** одним **действием копирования**, для выполнения которого **EmpOnPremSQLTable** будет использоваться как входные данные, а **OutputBlobTable** — как выходные данные.

1.	В колонке **ФАБРИКА ДАННЫХ** щелкните плитку **Разработка и развертывание**, чтобы запустить **редактор** для фабрики данных.

	![Плитка "Создание и развертывание"](./media/data-factory-move-data-between-onprem-and-cloud/author-deploy-tile.png)
2.	Нажмите кнопку **Создать конвейер** на панели команд. Если вы не видите эту кнопку, нажмите **... (многоточие)**, чтобы отобразить ее.
2.	Замените сценарий JSON в области справа на следующий текст:
	
			{
				"name": "ADFTutorialPipelineOnPrem",
				"properties": {
		    	"description": "This pipeline has one Copy activity that copies data from an on-prem SQL to Azure blob",
		    	"activities": [
		      	{
		        	"name": "CopyFromSQLtoBlob",
			        "description": "Copy data from on-prem SQL server to blob",
		        	"type": "Copy",
		        	"inputs": [
		          	{
		            	"name": "EmpOnPremSQLTable"
		          	}
		        	],
		        	"outputs": [
		          	{
			            "name": "OutputBlobTable"
			          }
			        ],
			        "typeProperties": {
			          "source": {
			            "type": "SqlSource",
			            "sqlReaderQuery": "select * from emp"
			          },
			          "sink": {
			            "type": "BlobSink"
			          }
			        },
			        "Policy": {
			          "concurrency": 1,
			          "executionPriorityOrder": "NewestFirst",
			          "style": "StartOfInterval",
			          "retry": 0,
			          "timeout": "01:00:00"
			        }
			      }
			    ],
			    "start": "2016-07-05T00:00:00Z",
			    "end": "2016-07-06T00:00:00Z",
			    "isPaused": false
			  }
			}

	Обратите внимание на следующее.
 
	- В разделе действий есть только действие, для параметра **type** которого задано значение **Copy**.
	- для параметра действия **input** установлено значение **EmpOnPremSQLTable**, а для **output** — **OutputBlobTable**;
	- В разделе **transformation** в качестве **типа источника** задано значение **SqlSource**, а в качестве **типа приемника** — **BlobSink**.
	- Для свойства **sqlReaderQuery** типа **SqlSource** задан тип SQL-запроса `select * from emp`.

	Замените значение свойства **start** текущей датой, а значение свойства **end** — датой следующего дня. Даты начала и окончания должны быть в [формате ISO](http://en.wikipedia.org/wiki/ISO_8601). Например, 2014-10-14T16:32:41Z. Время **окончания** указывать не обязательно, однако в этом примере мы будем его использовать.
	
	Если не указать значение свойства **end**, оно вычисляется по формуле «**время начала + 48 часов**». Чтобы запустить конвейер в течение неопределенного срока, укажите значение **9/9/9999** для свойства **end**.
	
	Вы определяете интервал времени, в рамках которого выполняются срезы данных на основе свойств **доступности**, определенных для каждой таблицы фабрики данных Azure.
	
	В этом примере получено 24 среза данных, так как они создаются каждый час.

	
2. Нажмите кнопку **Развернуть** на панели команд, чтобы развернуть набор данных (таблица представляет собой прямоугольный набор данных). Убедитесь, что в заголовке окна отображается сообщение **КОНВЕЙЕР УСПЕШНО РАЗВЕРНУТ**.
5. Теперь закройте колонку **Редактор**, щелкнув **X**. Щелкните **X** снова, чтобы закрыть колонку ADFTutorialDataFactory с представлением панели инструментов и дерева. При отображении сообщения **Несохраненные редакторы будут отклонены** щелкните **ОК**.
6. После этого следует вернуться к колонке **ФАБРИКА ДАННЫХ** для фабрики **ADFTutorialOnPremDF**.

**Поздравляем!** Вы успешно создали фабрику данных Azure, связанные службы, таблицы и конвейер, а также выполнили планирование конвейера.

#### Просмотр фабрики данных в представлении схемы 
1. На **портале Azure** щелкните элемент **Схема** на домашней странице фабрики данных **ADFTutorialOnPremDF**.

	![Ссылка на схему](./media/data-factory-move-data-between-onprem-and-cloud/OnPremDiagramLink.png)

2. Вы должны увидеть схему, аналогичную приведенной ниже:

	![Представление схемы](./media/data-factory-move-data-between-onprem-and-cloud/OnPremDiagramView.png)

	Можно увеличивать и уменьшать масштаб, выбирать 100%-й масштаб или масштаб по размеру, автоматически размещать конвейеры и таблицы, а также отображать сведения из журнала обращений и преобразований (выделение восходящих и нисходящих элементов для выбранных элементов). Дважды щелкните объект (входную или выходную таблицу либо конвейер), чтобы просмотреть его свойства.

## Отслеживание конвейера
На этом шаге используется портал Azure для мониторинга фабрики данных Azure. Вы также можете использовать командлеты PowerShell для мониторинга наборов данных и конвейеров. Дополнительные сведения о мониторинге см. в статье [Мониторинг конвейеров и управление ими](data-factory-monitor-manage-pipelines.md).

1. Перейдите на **портал Azure** (если вы закрыли страницу портала).
2. Если колонка для **ADFTutorialOnPremDF** закрыта, откройте ее, щелкнув **ADFTutorialOnPremDF** на **начальной панели**.
3. Вы увидите **количество** и **имена** таблиц, а также конвейер, который вы создали в этой колонке.

	![Домашняя страница фабрики данных](./media/data-factory-move-data-between-onprem-and-cloud/OnPremDiagramView.png)
4. Теперь щелкните плитку **Наборы данных**.
5. В колонке **Наборы данных** щелкните **EmpOnPremSQLTable**.

	![Срезы EmpOnPremSQLTable](./media/data-factory-move-data-between-onprem-and-cloud/OnPremSQLTableSlicesBlade.png)

6. Обратите внимание, что срезы данных до текущего момента времени уже выполнены и все они находятся в состоянии **Готов**. Это результат того, что вы вставили данные в базу данных SQL Server, и они находились там все время. Убедитесь, что в разделе **Проблемные срезы** в нижней части окна не показаны срезы.
	
	Оба списка, **Недавно обновленные срезы** и **Срезы, в которых недавно произошел сбой**, сортируются по **ПОСЛЕДНЕМУ ВРЕМЕНИ ОБНОВЛЕНИЯ**. Время обновления среза изменяется в таких ситуациях:

	Вы обновляете состояние среза вручную, например используя командлет **Set-AzureRmDataFactorySliceStatus** или щелкнув **ВЫПОЛНИТЬ** в колонке **СРЕЗ** для этого среза.
	
	В ходе выполнения состояние среза меняется (например, выполнение началось, выполнение завершилось сбоем, выполнение завершилось успешно и т. п.).

	Щелкните заголовок списков или **…** (многоточие), чтобы просмотреть расширенный список срезов. Чтобы отфильтровать срезы, выберите пункт **Фильтр** на панели инструментов.

	Чтобы вместо этого просмотреть срезы данных, отсортированные по времени начала и окончания среза, щелкните плитку **Срезы данных (по времени среза)**.
7. Затем в колонке **Наборы данных** щелкните **OutputBlobTable**.

	![Срезы OputputBlobTable](./media/data-factory-move-data-between-onprem-and-cloud/OutputBlobTableSlicesBlade.png)
8. Убедитесь, что срезы выполнены вплоть до текущего времени и состояние каждого из них — **Готово**. Подождите, пока значение состояния срезов до текущего времени **Готово**.
9. Щелкните любой срез данных из списка, чтобы отобразить колонку **Срез данных**.

	![Колонка среза данных](./media/data-factory-move-data-between-onprem-and-cloud/DataSlice.png)

	Если срез не находится в состоянии **Готов**, вы можете увидеть восходящие срезы, которые не находятся в состоянии готовности и блокируют выполнение текущего среза в списке **Неготовые восходящие срезы**.

10. Щелкните **выполненное действие** в нижней части списка, чтобы просмотреть **дополнительные сведения о выполнении операции**.

	![Колонка "Сведения о выполняемых действиях"](./media/data-factory-move-data-between-onprem-and-cloud/ActivityRunDetailsBlade.png)

11. Закройте все колонки, щелкая значок **X**, пока
12. не вернетесь к начальной колонке **ADFTutorialOnPremDF**.
14. (Необязательно) Щелкните **Конвейеры**, а затем — **ADFTutorialOnPremDF** и просмотрите параметры входных таблиц (**Использовано**) или выходных таблиц (**Выполнено**).
15. Используйте инструменты, такие как **обозреватель хранилищ Azure** для проверки выходных данных.

	![Обозреватель хранилищ Azure](./media/data-factory-move-data-between-onprem-and-cloud/OnPremAzureStorageExplorer.png)

## Дальнейшие действия

- Все дополнительные сведения о шлюзе управления данными см. в статье [Шлюз управления данными](data-factory-data-management-gateway.md).
- Чтобы узнать, как перемещать данные из исходного хранилища данных в приемник данных с помощью действия копирования, ознакомьтесь со статьей [Копирование данных из хранилища BLOB-объектов Azure в базу данных SQL с помощью фабрики данных](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md).

<!---HONumber=AcomDC_0914_2016-->