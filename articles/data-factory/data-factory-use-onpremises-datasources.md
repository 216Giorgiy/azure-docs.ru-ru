<properties 
	pageTitle="Использование конвейера для работы с локальными данными | Фабрика данных Azure" 
	description="Узнайте, как зарегистрировать локальный источник данных в фабрике данных Azure и копировать данные из источника данных и в него." 
	services="data-factory" 
	documentationCenter="" 
	authors="spelluru" 
	manager="jhubbard" 
	editor="monicar"/>

<tags 
	ms.service="data-factory" 
	ms.workload="data-services" 
	ms.tgt_pltfrm="na" 
	ms.devlang="na" 
	ms.topic="article" 
	ms.date="07/10/2015" 
	ms.author="spelluru"/>

# Использование конвейера для работы с локальными данными

Чтобы использовать конвейеры в фабрике данных Azure для работы с локальным источником данных, его необходимо добавить в качестве связанной службы в фабрику данных с помощью портала управления Azure или службы Azure PowerShell.
 
Чтобы добавить локальный источник данных в качестве связанной службы в фабрику данных, сначала вам необходимо скачать и установить шлюз управления данными Microsoft на локальный компьютер и настроить связанную службу для локального источника данных, чтобы использовать шлюз.


## <a href="DMG"></a> Шлюз управления данными

**Шлюз управления данными** — это программное обеспечение, которое подключает локальные источники данных к облачным службам безопасным и управляемым способом. Шлюз управления данными предоставляет перечисленные ниже возможности.

- **Подключение к локальным данным для доступа к гибридным данным** — вы можете подключить локальные данные к облачным службам, чтобы воспользоваться их преимуществами, но сохранив бизнес-данные в локальной среде.
- **Определение безопасного прокси-сервера данных** — вы можете определить локальные источники данных, доступ к которым предоставляется через шлюз управления данными, чтобы шлюз проверял подлинность запросов данных от облачных служб и защищал локальные источники данных.
- **Управление шлюзом для полного управления** — вам будут предоставлены все возможности мониторинга и ведения журнала для всех действий, выполняемых внутри шлюза управления данными, для управления и контроля.
- **Эффективное перемещение данных** — данные передаются в параллельном режиме и не зависят от временных сетевых проблем благодаря логике автоматического повторения.


В шлюзе управления данными предусмотрен полный набор функциональных возможностей для подключения локальных данных.

- **Отсутствие вмешательства в работу корпоративного брандмауэра** — шлюз управления данными начинает работать сразу после установки. Вам не потребуется открывать соединение брандмауэра или вносить изменения в инфраструктуру корпоративной сети. 
- **Шифрование учетных данных с помощью вашего сертификата** — учетные данные, используемые для подключения к источникам данных, будут зашифрованы с помощью сертификата, который полностью принадлежит пользователю. Без сертификата никто, даже сотрудник корпорации Майкрософт, не сможет расшифровать учетные данные и преобразовать их в обычный текст.

### Особенности использования шлюза управления данными
1.	Один экземпляр шлюза управления данными можно использовать для нескольких локальных источников данных, но учтите, что **шлюз связан с фабрикой данных Azure** и его невозможно использовать совместно с другой фабрикой данных.
2.	На компьютере может быть установлен **только один экземпляр шлюза управления данными**. Предположим, у вас есть две фабрики данных, которым необходимо получить доступ к локальным источникам данных. В этом случае вам необходимо установить шлюзы на двух локальных компьютерах, чтобы каждый шлюз был связан с отдельной фабрикой данных.
3.	**Шлюз не должен находиться в той же машине, что и источник данных**, но, если он расположен вблизи источника данных, это сокращает количество времени, требуемого для подключения шлюза к этому источнику. Мы рекомендуем установить шлюз на компьютере, отличном от того, на котором размещен локальный источник данных, чтобы шлюз не конкурировал за использование ресурсов с источником данных.
4.	У вас может быть **несколько шлюзов на разных компьютерах, подключенных к одному и тому же локальному источнику данных**. Например, имеется два шлюза, обслуживающие две фабрики данных, однако один и тот же локальный источник данных зарегистрирован в обеих фабриках данных. 
5.	Если вы уже установили шлюз на компьютер, который обслуживает сценарий **Power BI**, установите **отдельный шлюз для фабрики данных Azure** в другой машине.

### Рекомендации по портам и безопасности 
- Программа установки шлюза управления данными открывает на компьютере шлюза порты **8050** и **8051**. Эти порты используются **диспетчером учетных данных** (приложением ClickOnce), который позволяет задавать учетные данные для локальной связанной службы и проверять подключение к источнику данных. Эти порты недоступны для подключений из Интернета и их не требуется открывать в корпоративном брандмауэре.
- При копировании данных в локальную базу данных SQL Server из базы данных Azure SQL (или наоборот) убедитесь в следующем.
 
	- Брандмауэр на компьютере шлюза разрешает исходящие соединения TCP по **TCP**-порту **1433**.
	- В [параметрах брандмауэра Azure SQL](https://msdn.microsoft.com/library/azure/jj553530.aspx) **IP-адрес компьютера шлюза** добавлен в список **разрешенных IP-адресов**.

- При копировании данных из локального сервера SQL Server в любое место назначения, когда используются разные компьютер шлюза и компьютер с сервером SQL Server, выполните следующие действия: [настройте брандмауэр Windows](https://msdn.microsoft.com/library/ms175043.aspx) на компьютере с сервером SQL Server таким образом, чтобы шлюз мог получать доступ к базе данных через порты, которые прослушивает экземпляр SQL Server. Для экземпляра по умолчанию это порт 1433.

- Запустите **диспетчер учетных данных** на компьютере, который имеет подключение к шлюзу управления данными, чтобы получить возможность задавать учетные данные для источника данных и тестировать подключение к нему.

### Предварительные требования к установке шлюза 

1.	Поддерживаемые **операционные системы**: Windows 7, Windows 8/8.1, Windows Server 2008 R2, Windows Server 2012.
2.	Рекомендуемая **конфигурация** для компьютера шлюза: четырехъядерный процессор с тактовой частотой не менее 2 ГГц, не менее 8 ГБ ОЗУ и 80 ГБ дискового пространства.
3.	Когда хост-компьютер переходит в спящий режим, шлюз не может отвечать на запросы данных. Поэтому перед установкой шлюза на компьютере следует настроить соответствующую **схему управления питанием**. Если компьютер настроен на использование режима гибернации, во время установки шлюза отобразится соответствующее сообщение.  

Шлюз управления данными сериализует и десериализует данные источника и приемника на компьютере, на котором он размещен. Он также выполняет преобразования типов данных во время их копирования. Во время операции копирования шлюз считывает данные из источника в буфер памяти, и в это же время другой поток модуля записи записывает содержимое буфера в приемник. Поскольку во время пиковой нагрузки на машине с размещенным на ней шлюзом одновременно могут работать несколько копий, потребление памяти и ресурсов ЦП гораздо выше, чем во время простоя. Таким образом, для хост-компьютера, на котором запущен шлюз управления данными, может потребоваться больше ресурсов, чем указано в минимальной рекомендуемой конфигурации машины, либо меньше (во время простоев).


## Пошаговое руководство

Это руководство поможет вам создать фабрику данных с конвейером, который позволяет перемещать данные из локальной базы данных SQL Server в большой двоичный объект Azure.

## Шаг 1. Создание фабрики данных Azure
На этом шаге вы создадите экземпляр фабрики данных Azure с именем **ADFTutorialOnPremDF**, используя портал управления Azure. Фабрику данных также можно создать с помощью командлетов фабрики данных Azure.

1.	После входа на [портал предварительной версии Azure][azure-preview-portal] щелкните **СОЗДАТЬ** в нижнем левом углу, выберите **Анализ данных** в колонке **Создать**, затем щелкните **Фабрика данных** в колонке **Анализ данных**.

	![Создать -> Фабрика данных][image-data-factory-new-datafactory-menu]
  
6. В колонке **Создать фабрику данных** выполните следующие действия.
	1. Введите **имя** **ADFTutorialOnPremDF**.
	2. Щелкните **ИМЯ ГРУППЫ РЕСУРСОВ** и выберите **ADFTutorialResourceGroup** (если вы ознакомились с учебником из статьи [Приступая к работе с фабрикой данных][adfgetstarted]). Вы можете выбрать существующую группу ресурсов или создать новую группу. Чтобы создать новую группу ресурсов:
		1. Щелкните **Создать новую группу ресурсов**.
		2. В колонке **Создать группу ресурсов** введите **имя** для группы ресурсов и нажмите кнопку **ОК**.

7. Обратите внимание, что в колонке **Новая фабрика данных** установлен флажок **Добавить на начальную панель**.

	![Добавить на начальную панель][image-data-factory-add-to-startboard]

8. В колонке **Новая фабрика данных** щелкните **Создать**.

	Имя фабрики данных Azure должно быть глобально уникальным. Получив сообщение об ошибке **Имя фабрики данных «ADFTutorialOnPremDF» недоступно**, измените имя фабрики данных (например, на yournameADFTutorialOnPremDF) и попробуйте создать ее еще раз. Выполняя оставшиеся действия, описанные в этом руководстве, вместо ADFTutorialOnPremDF используйте именно это имя.

9. Обращайте внимание на уведомления, возникающие в процессе создания, в разделе **УВЕДОМЛЕНИЯ** слева. Щелкните **X**, чтобы закрыть колонку **УВЕДОМЛЕНИЯ**, если она открыта.

	![Раздел "УВЕДОМЛЕНИЯ"][image-data-factory-notifications-hub]

11. По завершении создания вы увидите колонку **Фабрика данных**, как показано ниже.

	![Домашняя страница фабрики данных][image-data-factory-datafactory-homepage]

## Шаг 2. Создание шлюза управления данными
5.	В колонке **Фабрика данных** для экземпляра **ADFTutorialOnPremDF** щелкните **Связанные службы**. 

	![Домашняя страница фабрики данных][image-data-factory-home-age]

2.	В колонке **Связанные службы** щелкните **+ Шлюз данных**.

	![Связанные службы — кнопка "Добавить шлюз"][image-data-factory-linkedservices-add-gateway-button]

2. В колонке **Создать** в поле **Имя** введите **adftutorialgateway** и нажмите кнопку **ОК**.

	![Колонка "Создать шлюз"][image-data-factory-create-gateway-blade]

3. В колонке **Настройка** щелкните **Установить непосредственно на этот компьютер**. Это позволит скачать пакет установки для шлюза, а также установить, настроить и зарегистрировать шлюз на компьютере.

	> [AZURE.NOTE]Используйте Internet Explorer или другой браузер, совместимый с Microsoft ClickOnce.

	![Шлюз — колонка "Настройка"][image-data-factory-gateway-configure-blade]

	Это самый простой способ (одним щелчком) скачать, установить, настроить и зарегистрировать шлюз в один шаг. Вы увидите, что на компьютере установлено приложение **Microsoft Data Management Gateway Configuration Manager**. Вы также можете найти исполняемый файл **ConfigManager.exe** в папке по следующему пути: **C:\\Program Files\\Microsoft Data Management Gateway\\1.0\\Shared**.

	Шлюз также можно скачать и установить вручную, используя ссылки в этой колонке. Затем вы можете зарегистрировать его с помощью ключа, указанного в текстовом поле **ЗАРЕГИСТРИРОВАТЬ С ПОМОЩЬЮ КЛЮЧА**.
	
	Дополнительную информацию о шлюзе, в том числе рекомендации и важные особенности, см. в разделе [Шлюз управления данными](#DMG).

	>[AZURE.NOTE]Для успешной установки шлюза управления данными и его настройки вы должны обладать правами администратора на локальном компьютере. В локальную группу Windows "Пользователи шлюза управления данными" можно добавить дополнительных пользователей. Участники этой группы смогут использовать диспетчер конфигурации шлюза управления данными для настройки шлюза.

4. Щелкните раздел **УВЕДОМЛЕНИЯ** слева. Дождитесь появления сообщения **Быстрая установка adftutorialgateway успешно завершена** в колонке **Уведомления**.

	![Быстрая установка выполнена успешно][express-setup-succeeded]
5. Нажмите кнопку **ОК** в колонке **Создать**, а затем щелкните колонку **Новый шлюз данных**.
6. Закройте колонку **Связанные службы** (нажмите кнопку **X** в правом верхнем углу) и снова откройте колонку **Связанные службы**, чтобы увидеть последнее состояние шлюза. 
7. Убедитесь, что **состояние** шлюза — **Подключено**. 

	![Состояние шлюза][gateway-status]
5. Запустите на компьютере приложение **Microsoft Data Management Gateway Configuration Manager**.

	![Диспетчер конфигурации шлюза][image-data-factory-gateway-configuration-manager]

6. Подождите, пока установятся следующие значения:
	1. Если для **состояния** службы не установлено значение **Запущена**, щелкните **Запустить службу**, чтобы запустить ее, и подождите минуту, пока другие поля не обновятся.
	2. В поле **Имя шлюза** установлено значение **adftutorialgateway**.
	3. Для **имени экземпляра** установлено значение **adftutorialgateway**.
	4. Для **состояния шлюза** установлено значение **Зарегистрирован**.
	5. В строке состояния внизу отображается надпись **Установлено подключение к облачной службе шлюза управления данными** и **зеленый флажок**.
	
7. Убедитесь, чтобы в колонке **Связанные службы** для **состояния** шлюза установлено значение **Хорошее**.
8. Закройте все колонки, пока не дойдете до домашней страницы **Фабрика данных**. 

## Шаг 2. Создание связанных служб 
На этом шаге вы создадите две связанные службы: **StorageLinkedService** и **SqlServerLinkedService**. Служба **SqlServerLinkedService** связывается с локальной базой данных SQL Server, а связанная служба **StorageLinkedService** связывает службы хранилища больших двоичных объектов Azure с **ADFTutorialDataFactory**. Далее это руководство поможет вам создать конвейер, который позволит копировать данные из локальной базы данных SQL Server в службу хранилища больших двоичных объектов Azure.

### Добавление связанной службы в локальную базу данных SQL Server
1.	В колонке **ФАБРИКА ДАННЫХ** щелкните плитку **Создать и развернуть**, чтобы запустить **редактор** для фабрики данных.

	![Плитка «Создание и развертывание»][image-author-deploy-tile]

	Подробный обзор редактора фабрики данных см. в разделе [Редактор фабрики данных][data-factory-editor].

2.	В **редакторе фабрики данных** нажмите кнопку **Создать хранилище данных** на панели инструментов и выберите **Локальная база данных SQL** в раскрывающемся меню.

	![Кнопка "Создать хранилище данных" в редакторе][image-editor-newdatastore-onpremsql-button]
    
3.	В панели справа должен появиться шаблон JSON для создания связанной службы локального сервера SQL Server. ![Связанная служба локальной базы данных SQ — параметры][image-editor-newdatastore-onpremsql-settings]

4.	На панели JSON выполните указанные ниже действия.
	1.	Для свойства **gatewayName** укажите **adftutorialgateway**, чтобы заменить весь текст, заключенный в двойные кавычки.  
	2.	При использовании **проверки подлинности SQL**: 
		1.	В свойстве **connectionString** замените **<servername>**, **<databasename>**, **<username>** и **<password>** на имя локального сервера SQL Server, имя базы данных, имя учетной записи пользователя и пароль. Чтобы указать имя экземпляра, используйте экранирующий символ: . Пример: **сервер\\имя\_экземпляра**. 	
		2.	Удалите последние два свойства (**username** и **password**) из файла JSON, а также удалите **запятую (,)** в конце последней строки в оставшейся части сценария JSON.
		
				{
				  "name": "SqlServerLinkedService",
				  "properties": {
				    "type": "OnPremisesSqlServer",
				    "typeProperties": {
				      "ConnectionString": "Data Source=<servername>;Initial Catalog=<databasename>;Integrated Security=False;User ID=<username>;Password=<password>;",
				      "gatewayName": "adftutorialgateway"
				    }
				  }
				}
	3.	При использовании **проверки подлинности Windows**:
		1. В свойстве **connectionString** замените **<servername>** и **<databasename>** именем вашего локального сервера SQL Server и базы данных. Задайте для параметра **Встроенная система безопасности** значение **True**. Удалите из строки подключения **идентификатор** и **пароль**.
			
				{
				  "name": "SqlServerLinkedService",
				  "properties": {
				    "type": "OnPremisesSqlServer",
				    "typeProperties": {
				      "ConnectionString": "Data Source=<servername>;Initial Catalog=<databasename>;Integrated Security=True;",
				      "gatewayName": "adftutorialgateway",
				      "username": "<Specify user name if you are using Windows Authentication>",
				      "password": "<Specify password for the user account>"
				    }
				  }
				}		
		
6. Нажмите кнопку **Развернуть** на панели инструментов, чтобы развернуть SqlServerLinkedService. Убедитесь, что в строке заголовка отображается сообщение **СВЯЗАННАЯ СЛУЖБА УСПЕШНО СОЗДАНА**. В представлении дерева слева также должна появиться служба **SqlServerLinkedService**.
		   
	![Успешное развертывание SqlServerLinkedService][image-editor-sql-linked-service-successful]
	
  
Для создания связанной службы SQL Server также можно нажать кнопку **Создать хранилище данных** в колонке **Связанные службы**. В этом случае вам потребуется задать учетные данные для источника данных с помощью диспетчера учетных данных ClickOnce, который установлен на компьютере с доступом к порталу. Если для доступа к порталу используется компьютер, отличный от компьютера шлюза, необходимо убедиться в том, что диспетчер учетных данных может подключиться к компьютеру шлюза. Если приложению не удается подключиться к компьютеру шлюза, вы не сможете задать учетные данные для источника данных и проверить подключение к нему.

#### Добавление связанной службы для учетной записи хранения Azure
 
1. В **редакторе** нажмите кнопку **Создать хранилище данных** на панели инструментов и выберите **Хранилище Azure** из раскрывающегося меню. На панели справа должен появиться шаблон JSON для создания связанной службы хранилища Azure. 

	![Кнопка "Создать хранилище данных" в редакторе][image-editor-newdatastore-button]
    
6. Замените **<accountname>** и **<accountkey>** именем и ключом учетной записи хранения Azure.

	![Хранилище больших двоичных объектов JSON в редакторе][image-editor-blob-storage-json]
	
	Подробную информацию о свойствах JSON см. в [Справочнике по сценариям JSON](http://go.microsoft.com/fwlink/?LinkId=516971).

6. Нажмите кнопку **Развернуть** на панели инструментов, чтобы развернуть StorageLinkedService. Убедитесь, что в строке заголовка отображается сообщение **СВЯЗАННАЯ СЛУЖБА УСПЕШНО СОЗДАНА**.

	![Развертывание хранилища больших двоичных объектов в редакторе][image-editor-blob-storage-deploy]

 
## Шаг 3. Создание наборов входных и выходных данных
В этом шаге вы создадите наборы входных и выходных данных, которые представляют собой входные и выходные данные для операции копирования (из локальной базы данных SQL в хранилище больших двоичных объектов Azure). Перед созданием наборов данных или таблиц (прямоугольных наборов данных) необходимо сделать следующее (после списка есть подробные шаги):

- в базе данных SQL Server, которую вы добавили в фабрику данных как связанную службу, создайте таблицу с именем **emp** и вставьте в таблицу пару записей в качестве примера;
- - если вы не ознакомились с учебником из статьи[Приступая к работе с фабрикой данных Azure][adfgetstarted], создайте контейнер больших двоичных объектов с именем **adftutorial** в учетной записи службы хранилища больших двоичных объектов Azure, которую вы добавили в фабрику данных как связанную службу.

### Подготовка локальной связанной службы SQL Server для учебника

1. В базе данных SQL Server, которую вы указали для локальных связанных служб (**SqlServerLinkedService**), используйте следующий сценарий SQL, чтобы создать в базе данных таблицу **emp**.


        CREATE TABLE dbo.emp
		(
			ID int IDENTITY(1,1) NOT NULL, 
			FirstName varchar(50),
			LastName varchar(50),
    		CONSTRAINT PK_emp PRIMARY KEY (ID)
		)
		GO
 

2. Вставьте несколько образцов в таблицу:


        INSERT INTO emp VALUES ('John', 'Doe')
		INSERT INTO emp VALUES ('Jane', 'Doe')



### Создание входной таблицы

1.	В **редакторе фабрики данных** щелкните на панели команд **Создать набор данных** и выберите **Локальный SQL**. 
2.	Замените сценарий JSON в области справа на следующий текст:    

		{
		  "name": "EmpOnPremSQLTable",
		  "properties": {
		    "type": "SqlServerTable",
		    "linkedServiceName": "SqlServerLinkedService",
		    "typeProperties": {
		      "tableName": "emp"
		    },
		    "external": true,
		    "availability": {
		      "frequency": "Hour",
		      "interval": 1
		    },
		    "policy": {
		      "externalData": {
		        "retryInterval": "00:01:00",
		        "retryTimeout": "00:10:00",
		        "maximumRetry": 3
		      }
		    }
		  }
		}

	Обратите внимание на следующее:
	
	- **type** имеет значение **SqlServerTable**.
	- **tableName** имеет значение **emp**.
	- Для параметра **linkedServiceName** установлено значение **SqlServerLinkedService** (вы создали эту связанную службу в шаге 2).
	- Для входной таблицы, которая не создается другим конвейером фабрики данных Azure, необходимо установить для свойства **external** значение **true**. При необходимости можно настроить политики в разделе **externalData**.   

	Подробную информацию о свойствах JSON см. в [Справочнике по сценариям JSON][json-script-reference].

2. Нажмите кнопку **Развернуть** на панели команд, чтобы развернуть набор данных (таблица представляет собой прямоугольный набор данных). Убедитесь, что в заголовке окна отображается сообщение **ТАБЛИЦА УСПЕШНО РАЗВЕРНУТА**.


### Создание выходной таблицы

1.	В **редакторе фабрики данных** щелкните на панели команд **Создать набор данных** и выберите **Хранилище больших двоичных объектов Azure**.
2.	Замените сценарий JSON в области справа на следующий текст: 

		{
		  "name": "OutputBlobTable",
		  "properties": {
		    "type": "AzureBlob",
		    "linkedServiceName": "StorageLinkedService",
		    "typeProperties": {
		      "folderPath": "adftutorial/outfromonpremdf",
		      "format": {
		        "type": "TextFormat",
		        "columnDelimiter": ","
		      }
		    },
		    "availability": {
		      "frequency": "Hour",
		      "interval": 1
		    }
		  }
		}
  
	Обратите внимание на следующее;
	
	- **type** имеет значение **AzureBlob**.
	- для параметра **linkedServiceName** установлено значение **StorageLinkedService** (вы создали эту связанную службу в шаге 2);
	- для параметра **folderPath** установлено значение **adftutorial/outfromonpremdf**, где outfromonpremdf — это папка в контейнере adftutorial; вам просто нужно создать контейнер **adftutorial**;
	- **availability** имеет значение **hourly** (**frequency** имеет значение **hour**, а **interval** имеет значение **1**). служба фабрики данных будет создавать срез выходных данных каждый час в таблице **emp** в базе данных SQL Azure. 

	Если вы не зададите параметр **fileName** для **входной таблицы**, все файлы или большие двоичные объекты из входной папки (**folderPath**) будут считаться входными файлами или объектами. Если указать fileName в JSON, только указанный файл или большой двоичный объект рассматриваются как входные данные. Примеры файлов см. в [учебнике][adf-tutorial].
 
	Если не указать **fileName** для **выходной таблицы**, то созданные в**folderPath** файлы получают имена в следующем формате: Data.<Guid>.txt (например: Data.0a405f8a-93ff-4c6f-b3be-f69616f1df7a.txt.).

	Для динамической установки папки **folderPath** и имени **fileName** на основе времени **SliceStart**, используйте свойство partitionedBy В следующем примере folderPath использует год, месяц и день из SliceStart (время начала обработки среза), а в fileName используется время (часы) из SliceStart. Например, если срез выполняется для временной отметки 2014-10-20T08:00:00, folderName получает значение wikidatagateway/wikisampledataout/2014/10/20, а fileName – 08.csv.

	  	"folderPath": "wikidatagateway/wikisampledataout/{Year}/{Month}/{Day}",
        "fileName": "{Hour}.csv",
        "partitionedBy": 
        [
        	{ "name": "Year", "value": { "type": "DateTime", "date": "SliceStart", "format": "yyyy" } },
            { "name": "Month", "value": { "type": "DateTime", "date": "SliceStart", "format": "MM" } }, 
            { "name": "Day", "value": { "type": "DateTime", "date": "SliceStart", "format": "dd" } }, 
            { "name": "Hour", "value": { "type": "DateTime", "date": "SliceStart", "format": "hh" } } 
        ],

 

	Подробную информацию о свойствах JSON см. в [справочнике по сценариям JSON][json-script-reference].

2.	Нажмите кнопку **Развернуть** на панели команд, чтобы развернуть набор данных (таблица представляет собой прямоугольный набор данных). Убедитесь, что в заголовке окна отображается сообщение **ТАБЛИЦА УСПЕШНО РАЗВЕРНУТА**.
  

## Шаг 4. Создание и запуск конвейера
На этом шаге вы создадите **конвейер** одним **действием копирования**, для выполнения которого **EmpOnPremSQLTable** будет использоваться как входные данные, а **OutputBlobTable** — как выходные данные.

1.	Нажмите кнопку **Создать конвейер** на панели команд. Если вы не видите эту кнопку, нажмите **... (многоточие)**, чтобы отобразить ее.
2.	Замените сценарий JSON в области справа на следующий текст:   


		{
		  "name": "ADFTutorialPipelineOnPrem",
		  "properties": {
		    "description": "This pipeline has one Copy activity that copies data from an on-prem SQL to Azure blob",
		    "activities": [
		      {
		        "name": "CopyFromSQLtoBlob",
		        "description": "Copy data from on-prem SQL server to blob",
		        "type": "Copy",
		        "inputs": [
		          {
		            "name": "EmpOnPremSQLTable"
		          }
		        ],
		        "outputs": [
		          {
		            "name": "OutputBlobTable"
		          }
		        ],
		        "typeProperties": {
		          "source": {
		            "type": "SqlSource",
		            "sqlReaderQuery": "select * from emp"
		          },
		          "sink": {
		            "type": "BlobSink"
		          }
		        },
		        "Policy": {
		          "concurrency": 1,
		          "executionPriorityOrder": "NewestFirst",
		          "style": "StartOfInterval",
		          "retry": 0,
		          "timeout": "01:00:00"
		        }
		      }
		    ],
		    "start": "2015-02-13T00:00:00Z",
		    "end": "2015-02-14T00:00:00Z",
		    "isPaused": false
		  }
		}
	Обратите внимание на следующее.
 
	- В разделе действий есть только действие, для параметра **type** которого установлено значение **Copy**.
	- для параметра действия **input** установлено значение **EmpOnPremSQLTable**, а для **output** — **OutputBlobTable**;
	- В разделе **transformation** в качестве **типа источника** установлено **SqlSource**, а в качестве **типа приемника** — **BlobSink**.
- для свойства **sqlReaderQuery** типа **SqlSource** задан вид SQL-запроса **select * from emp**.

	Замените значение свойства **start** текущей датой, а значение свойства **end** — датой следующего дня. Даты начала и окончания должны быть в [формате ISO](http://ru.wikipedia.org/wiki/ISO_8601). Например, 2014-10-14T16:32:41Z. Время **окончания** указывать не обязательно, однако в этом примере мы будем его использовать.
	
	Если не указать значение свойства **end**, оно вычисляется по формуле «**дата начала + 48 часов**». Чтобы запустить конвейер в течение неопределенного срока, укажите значение **9/9/9999** для свойства **end**.
	
	Вы определяете интервал времени, в который будут выполняться срезы данных на основе свойств **доступности**, определенных для каждой таблицы фабрики данных Azure.
	
	В приведенном выше примере будет 24 среза данных, так как срезы данных производятся каждый час.
	
2. Нажмите кнопку **Развернуть** на панели команд, чтобы развернуть набор данных (таблица представляет собой прямоугольный набор данных). Убедитесь, что в заголовке окна отображается сообщение **КОНВЕЙЕР УСПЕШНО РАЗВЕРНУТ**.
5. Теперь закройте колонку **Редактор**, щелкнув **X**. Щелкните **X** снова, чтобы закрыть колонку ADFTutorialDataFactory с представлением панели инструментов и дерева. При отображении сообщения **Несохраненные редакторы будут отклонены** щелкните **ОК**.
6. После этого следует вернуться к колонке **ФАБРИКА ДАННЫХ** для фабрики **ADFTutorialOnPremDF**.

**Поздравляем!** Вы успешно создали фабрику данных Azure, связанные службы, таблицы и конвейер, а также выполнили планирование конвейера.

### Просмотр фабрики данных в представлении схемы 
1. На портале **предварительной версии Azure** щелкните плитку **Схемы** на домашней странице фабрики данных **ADFTutorialOnPremDF**.

	![Ссылка на схему][image-data-factory-diagram-link]

2. Вы должны увидеть схему, аналогичную приведенной ниже:

	![Представление схемы][image-data-factory-diagram-view]

	Можно увеличивать и уменьшать масштаб, выбирать 100%-й масштаб или масштаб по размеру, автоматически размещать конвейеры и таблицы, а также отображать сведения из журнала обращений и преобразований (выделение восходящих и нисходящих элементов для выбранных элементов). Дважды щелкните объект (входную/выходную таблицу или конвейер), чтобы просмотреть его свойства.

## Шаг 5. Мониторинг наборов данных и конвейеров
В этом шаге вы будете использовать портал Azure для мониторинга фабрики данных Azure. Вы также можете использовать командлеты PowerShell для мониторинга наборов данных и конвейеров. Дополнительную информацию об использовании командлетов для мониторинга см. в статье [Мониторинг и управление фабрикой данных Azure с помощью Azure PowerShell][monitor-manage-powershell].

1. Перейдите на **портал предварительной версии Azure** (если вы закрыли страницу портала).
2. Если колонка для **ADFTutorialOnPremDF** закрыта, откройте ее, щелкнув **ADFTutorialOnPremDF** на **начальной панели**.
3. Вы увидите **количество** и **имена** таблиц, а также конвейер, который вы создали в этой колонке.

	![Домашняя страница фабрики данных][image-data-factory-homepage-2]
4. Теперь щелкните плитку **Наборы данных**.
5. В колонке **Наборы данных** щелкните **EmpOnPremSQLTable**.

	![Срезы EmpOnPremSQLTable][image-data-factory-onprem-sqltable-slices]

6. Обратите внимание, что срезы данных до текущего момента времени уже выполнены и все они находятся в состоянии **Готов**. Это результат того, что вы вставили данные в базу данных SQL Server, и они находились там все время. Убедитесь, что в разделе **Проблемные срезы** в нижней части окна не показаны срезы.


	Оба списка, **Недавно обновленные срезы** и **Срезы, в которых недавно произошел сбой**, сортируются по **ПОСЛЕДНЕМУ ВРЕМЕНИ ОБНОВЛЕНИЯ**. Время обновления среза изменяется в таких ситуациях:
    

	-  Вы обновляете состояние среза вручную, например используя командлет **Set-AzureDataFactorySliceStatus** или щелкнув **ВЫПОЛНИТЬ** в колонке **СРЕЗ** для этого среза.
	-  В ходе выполнения состояние среза меняется (например, выполнение началось, выполнение завершилось со сбоем, выполнение завершилось успешно и т. п.).
 
	Щелкните заголовок списков или **... (многоточие)**, чтобы просмотреть расширенный список срезов. Чтобы отфильтровать срезы, выберите пункт **Фильтр** на панели инструментов.
	
	Чтобы вместо этого просмотреть срезы данных, отсортированные по времени начала и окончания среза, щелкните плитку **Срезы данных (по времени среза)**.

7. Затем в колонке **Наборы данных** щелкните **OutputBlobTable**.

	![Срезы OputputBlobTable][image-data-factory-output-blobtable-slices]
8. Убедитесь, что выполнены срезы вплоть до текущего времени и состояние каждого из них — **Готово**. Подождите, пока значение состояния срезов до текущего времени **Готово**.
9. Щелкните любой срез данных в списке для отображения колонки **СРЕЗ ДАННЫХ**.

	![Колонка среза данных][image-data-factory-dataslice-blade]

	Если срез не находится в состоянии **Готов**, вы можете увидеть восходящие срезы, которые не находятся в состоянии готовности и блокируют выполнение текущего среза в списке **Неготовые восходящие срезы**.

10. Щелкните **выполненное действие** в нижней части списка, чтобы просмотреть **дополнительную информацию о выполненном действии**.

	![Колонка "Сведения о выполняемых действиях"][image-data-factory-activity-run-details]

11. Нажмите кнопку **X**, чтобы закрыть все колонки, пока вы не вернетесь к главной колонке для **ADFTutorialOnPremDF**.
14. (Необязательное действие) Щелкните плитку **Конвейеры**, а затем — конвейер **ADFTutorialOnPremDF** и детализируйте входные таблицы (**Использовано**) или выходные таблицы (**Выполнено**).
15. Используйте инструменты, такие как **обозреватель хранилищ Azure** для проверки выходных данных.

	![Обозреватель хранилищ Azure][image-data-factory-stroage-explorer]


## Создание и регистрация шлюза с использованием Azure PowerShell 
В этом разделе описывается, как создать и зарегистрировать шлюз с использованием командлетов Azure PowerShell.

1. Запустите модуль **Azure PowerShell** в режиме администратора. 
2. Командлеты фабрики данных Azure доступны в режиме **AzureResourceManager**. Выполните следующую команду, чтобы перейти в режим **AzureResourceManager**:     

        switch-azuremode AzureResourceManager


2. Используйте командлет **New-AzureDataFactoryGateway** для создания логического шлюза следующим образом:

		New-AzureDataFactoryGateway -Name <gatewayName> -DataFactoryName <dataFactoryName> -ResourceGroupName ADF –Description <desc>

	**Пример команды и выходных данных**:


		PS C:\> New-AzureDataFactoryGateway -Name MyGateway -DataFactoryName $df -ResourceGroupName ADF –Description “gateway for walkthrough”

		Name              : MyGateway
		Description       : gateway for walkthrough
		Version           :
		Status            : NeedRegistration
		VersionStatus     : None
		CreateTime        : 9/28/2014 10:58:22
		RegisterTime      :
		LastConnectTime   :
		ExpiryTime        :
		ProvisioningState : Succeeded


3. Используйте командлет **New-AzureDataFactoryGatewayKey**, чтобы создать регистрационный ключ для вновь созданного шлюза, и сохраните этот ключ в локальной переменной **$Key**:

		New-AzureDataFactoryGatewayKey -GatewayName <gatewayname> -ResourceGroupName ADF -DataFactoryName <dataFactoryName>

	
	**Пример результата выполнения команды:**


		PS C:\> $Key = New-AzureDataFactoryGatewayKey -GatewayName MyGateway -ResourceGroupName ADF -DataFactoryName $df 

	
4. В Azure PowerShell перейдите к папке C:\\Program Files\\Microsoft Data Management Gateway\\1.0\\PowerShellScript** и выполните сценарий **RegisterGateway.ps1**, связанный с локальной переменной **$Key**, как показано в следующей команде, чтобы зарегистрировать агент клиента, установленный на вашем компьютере с логическим шлюзом, созданным ранее.

		PS C:\> .\RegisterGateway.ps1 $Key.GatewayKey
		
		Agent registration is successful!

5. Вы можете использовать командлет **Get-AzureDataFactoryGateway**, чтобы получить список шлюзов своей фабрики данных. Если в поле **Состояние** указано значение **Подключено**, шлюз готов к эксплуатации.

		Get-AzureDataFactoryGateway -DataFactoryName <dataFactoryName> -ResourceGroupName ADF

Вы можете удалить шлюз, используя командлет **Remove-AzureDataFactoryGateway**, или обновить описание шлюза, используя командлет **Set-AzureDataFactoryGateway**. Дополнительную информацию о синтаксисе и другую информацию об этих командлетах см. в «Справочных материалах по командлетам фабрики данных».



[monitor-manage-using-powershell]: data-factory-monitor-manage-using-powershell.md
[adf-getstarted]: data-factory-get-started.md
[adf-tutorial]: data-factory-tutorial.md
[use-custom-activities]: data-factory-use-custom-activities.md
[use-pig-and-hive-with-data-factory]: data-factory-pig-hive-activities.md
[troubleshoot]: data-factory-troubleshoot.md
[data-factory-introduction]: data-factory-introduction.md
[data-factory-editor]: data-factory-editor.md

[developer-reference]: http://go.microsoft.com/fwlink/?LinkId=516908
[cmdlet-reference]: http://go.microsoft.com/fwlink/?LinkId=517456


[64bit-download-link]: http://go.microsoft.com/fwlink/?LinkId=517623
[32bit-download-link]: http://go.microsoft.com/fwlink/?LinkId=517624

[azure-preview-portal]: http://portal.azure.com
[adfgetstarted]: data-factory-get-started.md
[monitor-manage-powershell]: data-factory-monitor-manage-using-powershell.md





[json-script-reference]: http://go.microsoft.com/fwlink/?LinkId=516971
[cmdlet-reference]: http://go.microsoft.com/fwlink/?LinkId=517456



[azure-powershell-install]: http://azure.microsoft.com/documentation/articles/install-configure-powershell/

[image-author-deploy-tile]: ./media/data-factory-use-onpremises-datasources/author-deploy-tile.png
[image-editor-newdatastore-button]: ./media/data-factory-use-onpremises-datasources/editor-newdatastore-button.png
[image-editor-blob-storage-json]: ./media/data-factory-use-onpremises-datasources/editor-blob-storage-settings.png
[image-editor-blob-storage-deploy]: ./media/data-factory-use-onpremises-datasources/editor-deploy-blob-linked-service.png
[image-editor-newdatastore-onpremsql-button]: ./media/data-factory-use-onpremises-datasources/editor-newdatastore-onpremsql-button.png
[image-editor-newdatastore-onpremsql-settings]: ./media/data-factory-use-onpremises-datasources/editor-onprem-sql-settings.png
[image-editor-sql-linked-service-successful]: ./media/data-factory-use-onpremises-datasources/editor-sql-linked-service-successful.png

[gateway-status]: ./media/data-factory-use-onpremises-datasources/gateway-status.png
[express-setup-succeeded]: ./media/data-factory-use-onpremises-datasources/express-setup-succeeded.png
[image-data-factory-onprem-new-everything]: ./media/data-factory-use-onpremises-datasources/OnPremNewEverything.png

[image-data-factory-onprem-datastorage-cache-backup]: ./media/data-factory-use-onpremises-datasources/OnPremDataStorageCacheBackup.png

[image-data-factory-onprem-datastorage-see-all]: ./media/data-factory-use-onpremises-datasources/OnPremDataStorageSeeAll.png

[image-data-factory-onprem-dataservices-blade]: ./media/data-factory-use-onpremises-datasources/OnPremDataServicesBlade.png

[image-data-factory-onprem-datafactory-preview-blade]: ./media/data-factory-use-onpremises-datasources/DataFactoryPreviewBlade.png

[image-data-factory-onprem-create-resource-group]: ./media/data-factory-use-onpremises-datasources/OnPremCreateResourceGroup.png

[image-data-factory-add-to-startboard]: ./media/data-factory-use-onpremises-datasources/OnPremNewDataFactoryAddToStartboard.png

[image-data-factory-notifications-hub]: ./media/data-factory-use-onpremises-datasources/OnPremNotificationsHub.png

[image-data-factory-datafactory-homepage]: ./media/data-factory-use-onpremises-datasources/OnPremDataFactoryHomePage.png

[image-data-factory-startboard]: ./media/data-factory-use-onpremises-datasources/OnPremStartboard.png

[image-data-factory-linkedservices-blade]: ./media/data-factory-use-onpremises-datasources/OnPremLinkedServicesBlade.png

[image-data-factory-linkedservices-add-datastore-button]: ./media/data-factory-use-onpremises-datasources/ONPremLinkedServicesAddDataStoreButton.png

[image-data-factory-new-datastore-blade]: ./media/data-factory-use-onpremises-datasources/OnPremNewDataStoreBlade.png

[image-data-factory-get-storage-key]: ./media/data-factory-use-onpremises-datasources/OnPremGetStorageKey.png

[image-data-factory-linkedservices-add-gateway-button]: ./media/data-factory-use-onpremises-datasources/OnPremLinkedServicesAddGaewayButton.png

[image-data-factory-create-gateway-blade]: ./media/data-factory-use-onpremises-datasources/OnPremCreateGatewayBlade.png

[image-data-factory-gateway-configure-blade]: ./media/data-factory-use-onpremises-datasources/OnPremGatewayConfigureBlade.png

[image-data-factory-gateway-configuration-manager]: ./media/data-factory-use-onpremises-datasources/OnPremDMGConfigurationManager.png

[image-data-factory-oneclick-install]: ./media/data-factory-use-onpremises-datasources/OnPremOneClickInstall.png

[image-data-factory-diagram-link]: ./media/data-factory-use-onpremises-datasources/OnPremDiagramLink.png

[image-data-factory-diagram-view]: ./media/data-factory-use-onpremises-datasources/OnPremDiagramView.png

[image-data-factory-homepage-2]: ./media/data-factory-use-onpremises-datasources/OnPremDataFactoryHomePage2.png

[image-data-factory-stroage-explorer]: ./media/data-factory-use-onpremises-datasources/OnPremAzureStorageExplorer.png

[image-data-factory-home-age]: ./media/data-factory-use-onpremises-datasources/DataFactoryHomePage.png

[image-data-factory-onprem-sqltable-slices]: ./media/data-factory-use-onpremises-datasources/OnPremSQLTableSlicesBlade.png

[image-data-factory-output-blobtable-slices]: ./media/data-factory-use-onpremises-datasources/OutputBlobTableSlicesBlade.png

[image-data-factory-dataslice-blade]: ./media/data-factory-use-onpremises-datasources/DataSlice.png

[image-data-factory-activity-run-details]: ./media/data-factory-use-onpremises-datasources/ActivityRunDetailsBlade.png

[image-data-factory-new-datafactory-menu]: ./media/data-factory-use-onpremises-datasources/NewDataFactoryMenu.png

[image-data-factory-preview-portal-storage-key]: ./media/data-factory-get-started/PreviewPortalStorageKey.png

<!---HONumber=August15_HO6-->