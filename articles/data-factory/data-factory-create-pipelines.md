<properties 
	pageTitle="Создание конвейеров, цепочки действий и расписаний для них в фабрике данных | Microsoft Azure" 
	description="Узнайте, как создать конвейер данных в фабрике данных Azure для перемещения и преобразования данных. Создание управляемого данными рабочего процесса, который предоставит готовую к использованию информацию." 
    keywords="конвейер данных, управляемый данными рабочий процесс"
	services="data-factory" 
	documentationCenter="" 
	authors="spelluru" 
	manager="jhubbard" 
	editor="monicar"/>

<tags 
	ms.service="data-factory" 
	ms.workload="data-services" 
	ms.tgt_pltfrm="na" 
	ms.devlang="na" 
	ms.topic="article"
	ms.date="06/27/2016" 
	ms.author="spelluru"/>

# Конвейеры и действия в фабрике данных Azure: создание конвейеров, цепочки действий и расписаний для них
Из этой статьи вы узнаете о конвейерах и действиях в фабрике данных Azure. Также вы научитесь с их помощью создавать комплексные рабочие процессы, управляемые данными, для конкретных задач и сценариев: от индивидуальных рекомендаций по товарам до анализа маркетинговой кампании.

> [AZURE.NOTE] Предполагается, что вы уже изучили статьи [Введение в службу фабрики данных Azure](data-factory-introduction.md) и [Создание наборов данных](data-factory-create-datasets.md). Если у вас нет практического опыта создания фабрик данных, рекомендуем для лучшего понимания статьи изучить руководство [Начало работы с фабрикой данных Azure](data-factory-build-your-first-pipeline.md).

## Что такое конвейер данных
**Конвейеры — это логические группы действий**. С помощью конвейеров действия объединяются в блоки для выполнения определенных задач. Чтобы лучше понять сущность конвейеров, необходимо сначала разобраться с понятием «действия».

## Что такое действие
Действия определяют то, что нужно выполнить с вашими данными. Каждое действие принимает некоторое количество [наборов данных](data-factory-create-datasets.md) на входе и создает один или несколько наборов данных на выходе. **Действие — это единица управления в фабрике данных Azure.**

Действие копирования, например, может использоваться для управления копированием данных из одного набора данных в другой. Аналогичным образом можно использовать действие Hive HDInsight для отправки запроса Hive к кластеру Azure HDInsight для преобразования или анализа данных. Фабрика данных Azure предоставляет широкий выбор [действий для преобразования, анализа](data-factory-data-transformation-activities.md) и [перемещения данных](data-factory-data-movement-activities.md). Кроме того, вы можете создать действие .NET для выполнения собственного кода.

Рассмотрите два указанных далее набора данных.

**Набор данных Azure SQL**

Таблица MyTable содержит столбец timestampcolumn, который помогает определить дату и время вставки данных в базу данных.

	{
	  "name": "AzureSqlInput",
	  "properties": {
	    "type": "AzureSqlTable",
	    "linkedServiceName": "AzureSqlLinkedService",
	    "typeProperties": {
	      "tableName": "MyTable"
	    },
	    "external": true,
	    "availability": {
	      "frequency": "Hour",
	      "interval": 1
	    },
	    "policy": {
	      "externalData": {
	        "retryInterval": "00:01:00",
	        "retryTimeout": "00:10:00",
	        "maximumRetry": 3
	      }
	    }
	  }
	}

**Набор данных BLOB-объекта Azure**

Данные копируются в новый BLOB-объект каждый час, и путь, указываемый для объекта, отображает дату и время по часам.

	{
	  "name": "AzureBlobOutput",
	  "properties": {
	    "type": "AzureBlob",
	    "linkedServiceName": "StorageLinkedService",
	    "typeProperties": {
	      "folderPath": "mycontainer/myfolder/yearno={Year}/monthno={Month}/dayno={Day}/hourno={Hour}",
	      "partitionedBy": [
	        {
	          "name": "Year",
	          "value": {
	            "type": "DateTime",
	            "date": "SliceStart",
	            "format": "yyyy"
	          }
	        },
	        {
	          "name": "Month",
	          "value": {
	            "type": "DateTime",
	            "date": "SliceStart",
	            "format": "%M"
	          }
	        },
	        {
	          "name": "Day",
	          "value": {
	            "type": "DateTime",
	            "date": "SliceStart",
	            "format": "%d"
	          }
	        },
	        {
	          "name": "Hour",
	          "value": {
	            "type": "DateTime",
	            "date": "SliceStart",
	            "format": "%H"
	          }
	        }
	      ],
	      "format": {
	        "type": "TextFormat",
	        "columnDelimiter": "\t",
	        "rowDelimiter": "\n"
	      }
	    },
	    "availability": {
	      "frequency": "Hour",
	      "interval": 1
	    }
	  }
	}


Действие копирования в приведенном ниже примере конвейера копирует данные из Azure SQL в хранилище BLOB-объектов Azure. Каждый час оно принимает таблицу Azure SQL в качестве входного набора данных и записывает в хранилище BLOB-объектов Azure данные в виде набора данных AzureBlobOutput. Выходной набор данных тоже создается каждый час. Сведения о том, как копирование данных распределено во времени, см. в разделе [Планирование и выполнение](#scheduling-and-execution). Для этого конвейера задан 3-часовой период активности: с 2015-01-01T08:00:00 до 2015-01-01T11:00:00.

**Конвейер:**
	
	{  
	    "name":"SamplePipeline",
	    "properties":{  
	    "start":"2015-01-01T08:00:00",
	    "end":"2015-01-01T11:00:00",
	    "description":"pipeline for copy activity",
	    "activities":[  
	      {
	        "name": "AzureSQLtoBlob",
	        "description": "copy activity",
	        "type": "Copy",
	        "inputs": [
	          {
	            "name": "AzureSQLInput"
	          }
	        ],
	        "outputs": [
	          {
	            "name": "AzureBlobOutput"
	          }
	        ],
	        "typeProperties": {
	          "source": {
	            "type": "SqlSource",
	            "SqlReaderQuery": "$$Text.Format('select * from MyTable where timestampcolumn >= \\'{0:yyyy-MM-dd HH:mm}\\' AND timestampcolumn < \\'{1:yyyy-MM-dd HH:mm}\\'', WindowStart, WindowEnd)"
	          },
	          "sink": {
	            "type": "BlobSink"
	          }
	        },
	       "scheduler": {
	          "frequency": "Hour",
	          "interval": 1
	        },
	        "policy": {
	          "concurrency": 1,
	          "executionPriorityOrder": "OldestFirst",
	          "retry": 0,
	          "timeout": "01:00:00"
	        }
	      }
	     ]
	   }
	}

Теперь у вас есть общее представление о том, что такое действие, и можно вернуться к рассмотрению конвейеров.
 
Конвейеры — это логические группы действий. С помощью конвейеров действия объединяются в блоки для выполнения определенных задач. **Конвейер является также единицей развертывания действий и управления ими.** Например, логически связанные действия можно объединить в один конвейер, чтобы одновременно управлять их состоянием.

Выходной набор данных одного действия в конвейере может являться входным для другого действия в том же или другом конвейере в зависимости от того, какие отношения определены между действиями. Подробные сведения см. в разделе [Планирование и выполнение](#chaining-activities).

Создание конвейера в фабрике данных Azure, как правило, состоит из следующих этапов:

1.	Создание фабрики данных (если не создана).
2.	Создание связанной службы для каждого хранилища данных или службы вычислений.
3.	Создание входных и выходных наборов данных.
4.	Создание конвейера с действиями, которые обрабатывают определенные на предыдущем этапе наборы данных.

![Сущности фабрики данных](./media/data-factory-create-pipelines/entities.png)

Рассмотрим определение конвейера подробнее.

## Анатомия конвейера  

В общем виде структуру конвейера можно представить приведенным ниже образом.

	{
	    "name": "PipelineName",
	    "properties": 
	    {
	        "description" : "pipeline description",
	        "activities":
	        [
	
	        ],
			"start": "<start date-time>",
			"end": "<end date-time>"
	    }
	}

В разделе **activities** можно определить одно или несколько действий. Каждое действие имеет приведенную ниже структуру верхнего уровня.

	{
	    "name": "ActivityName",
	    "description": "description", 
	    "type": "<ActivityType>",
	    "inputs":  "[]",
	    "outputs":  "[]",
	    "linkedServiceName": "MyLinkedService",
	    "typeProperties":
	    {
	
	    },
	    "policy":
	    {
	    }
	    "scheduler":
	    {
	    }
	}

В приведенной ниже таблице описаны свойства, используемые в определениях JSON действия и конвейера.

Тег | Описание | Обязательно
--- | ----------- | --------
name | Имя действия или конвейера. Содержит имя операции, которую должно выполнять действие или конвейер.<br/><ul><li>Максимальное количество символов — 260.</li><li>Имя должно начинаться с цифры, буквы или знака подчеркивания (\_).</li><li>Не разрешается использовать следующие символы: . + ? / < > * % & : \</li></ul> | Да
description | Текст, описывающий, для чего используется действие или конвейер | Да
type | Задает тип действия. Разные типы действий описаны в статьях, посвященных [действиям перемещения данных](data-factory-data-movement-activities.md) и [действиям преобразования данных](data-factory-data-transformation-activities.md). | Да
inputs | Входные таблицы, используемые действием:<br/><br/> // одна входная таблица<br/>"inputs": [ { "name": "inputtable1" } ],<br/><br/>// две входные таблицы <br/>"inputs": [ { "name": "inputtable1" }, { "name": "inputtable2" } ], | Да
outputs | Выходные таблицы, используемые действием: // одна выходная таблица<br/>"outputs": [ { "name": “outputtable1” } ],<br/><br/>// две входные таблицы <br/>"outputs": [ { "name": “outputtable1” }, { "name": “outputtable2” } ], | Да
linkedServiceName (имя связанной службы) | Имя связанной службы, используемой действием. <br/><br/>Для действия может потребоваться указать службу, связанную с обязательной вычислительной средой. | Да — для действия HDInsight и действия пакетной оценки показателей машинного обучения Azure; <br/><br/>Нет — для всех остальных
typeProperties | Свойства в разделе typeProperties зависят от типа действия. Дополнительные сведения см. в статьях с описанием конкретных действий. | Нет
policy | Политики, которые влияют на поведение во время выполнения действия. Если для этого свойства не задано значение, используются стандартные политики. Подробнее описано ниже. | Нет
start | Дата и время начала работы конвейера. Задается в [формате ISO](http://en.wikipedia.org/wiki/ISO_8601). Например, 2014-10-14T16:32:41Z. <br/><br/>Можно указать местное время, например восточное поясное время (EST). Например, время 6:00 по часовому поясу EST задается так: 2016-02-27T06:00:00**-05:00**.<br/><br/> Свойства start и end определяют активный период работы конвейера. Срезы выходных данных создаются только в этот активный период. | Нет<br/><br/>Если задано значение для свойства end, необходимо задать значение и для свойства start. <br/><br/>Чтобы создать конвейер, время начала и окончания можно не указывать. Но если требуется задать активный период работы конвейера, следует указать оба значения. Если вы не указали время начала и окончания при создании конвейера, их можно установить позже с помощью командлета Set-AzureRmDataFactoryPipelineActivePeriod.
end | Дата и время завершения работы конвейера. Не является обязательным и задается в формате ISO. Например: 2014-10-14T17:32:41Z<br/><br/>Можно указать местное время, например восточное поясное время (EST). Например, время 6:00 по часовому поясу EST задается так: 2016-02-27T06:00:00**-05:00**.<br/><br/> Чтобы работа конвейера не была ограничена во времени, укажите для свойства end значение 9999-09-09. | Нет<br/><br/>Если вы указываете значение свойства start, также нужно указать и значение свойства end.<br/><br/>См. примечания к свойству **start**.
isPaused | Если задано значение true, конвейер не будет выполняться. Значение по умолчанию — false. Это свойство можно использовать для включения или отключения. | Нет 
scheduler | Свойство scheduler позволяет задать расписание выполнения действия. Для него предусмотрен такой же набор подсвойств, что и для [свойства availability в наборе данных](data-factory-create-datasets.md#Availability). | Нет |   
| pipelineMode | Определяет метод планирования работы конвейера. Допустимые значения: scheduled (используется по умолчанию), onetime.<br/><br/>Значение scheduled означает, что конвейер будет запускаться с указанной периодичностью в соответствии с его активным периодом (временем начала и окончания). "Однократно" означает, что конвейер будет запускаться только один раз. В настоящее время изменить или обновить однократные конвейеры после их создания нельзя. Подробные сведения об однократном запуске см. в разделе [Однократный конвейер](data-factory-scheduling-and-execution.md#onetime-pipeline). | Нет | 
| expirationTime; | Период времени после создания, в течение которого конвейер является допустимым и должен оставаться подготовленным. Если на момент завершения этого периода у конвейера не будет активных, невыполненных или ожидающих выполнения запусков, конвейер будет автоматически удален. | Нет | 
| datasets | Список наборов данных для использования действиями, определенными в конвейере. Его можно использовать для определения наборов данных, характерных для этого конвейера и не определенных в фабрике данных. Наборы данных, определенные в этом конвейере, могут использоваться только этим конвейером и не предназначены для совместного доступа. Дополнительные сведения см. в разделе [Контекст наборов данных](data-factory-create-datasets.md#scoped-datasets).| Нет |  
 

## Типы действий для перемещения и преобразования данных
Фабрика данных Azure позволяет выполнять широкий спектр действий для [перемещения](data-factory-data-movement-activities.md) и [преобразования данных](data-factory-data-transformation-activities.md).

### Политики
Политики влияют на поведение во время выполнения действия, особенно при обработке среза таблицы. В следующей таблице приведено несколько примеров.

Свойство | Допустимые значения | Значение по умолчанию | Описание
-------- | ----------- | -------------- | ---------------
concurrency | Целое число <br/><br/> Максимальное значение — 10 | 1 | Количество одновременных запусков действия. <br/><br/> Определяет количество параллельных запусков одного действия для обработки разных срезов. Высокое значение этого свойства ускорит обработку большого набора доступных данных. 
executionPriorityOrder | NewestFirst<br/><br/>OldestFirst | OldestFirst | Определяет порядок обработки срезов данных.<br/><br/>Предположим, есть два ожидающих обработки среза (от 16:00 и от 17:00). Если для свойства executionPriorityOrder задано значение NewestFirst, то срез от 17:00 будет обработан первым. Аналогично, если для executionPriorityORder задано значение OldestFIrst, то первым будет обработан срез от 16:00. 
retry | Целое число<br/><br/>Максимальное значение — 10 | 3 | Число повторных попыток обработки данных до того, как срез перейдет в состояние Failure (сбой). Выполнение действия со срезом данных повторяется указанное количество раз. Повторная попытка выполняется сразу после неудачной.
timeout | TimeSpan | 00:00:00 | Время ожидания для действия. Пример: 00:10:00 (время ожидания — 10 минут).<br/><br/>Если значение не указано или равно 0, время ожидания не ограничено.<br/><br/>Если время обработки среза превышает время ожидания, система отменяет текущую обработку и начинает новую. Количество повторов зависит от значения свойства retry. Когда время ожидания истекает, состояние среза меняется на TimedOut.
delay | TimeSpan | 00:00:00 | Определяет задержку перед обработкой среза данных. <br/><br/> Выполнение действия для среза данных начинается после того, как истекает время задержки (Delay). <br/><br/> Пример: 00:10:00 (10-минутная задержка).
longRetry | Целое число<br/><br/>Максимальное значение — 10 | 1 | Количество интервальных повторных попыток, после которых срез переходит в состояние сбоя.<br/><br/>Попытки longRetry выполняются с интервалом, определяемым свойством longRetryInterval. Используйте свойство longRetry, если повторные попытки необходимо выполнять с паузами. Если заданы значения и для retry, и для longRetry, каждая попытка longRetry будет включать столько попыток, сколько задано параметром retry (таким образом, общее количество попыток определяется как произведение значений retry и longRetry).<br/><br/>Например, для действия может быть настроена такая политика:<br/>retry=3;<br/>longRetry=2;<br/>longRetryInterval=01:00:00.<br/><br/> Предположим, что требуется обработать только один срез (состояние — Waiting) и при каждом выполнении действия происходит сбой. Первые три попытки будут выполнены подряд. После каждой повторной попытки срез будет находиться в состоянии Retry. После трех попыток состояние среза изменится на LongRetry.<br/><br/>Через час (значение свойства longRetryInteval) будут выполнены еще три попытки подряд. После этого состояние среза изменится на Failed и дальнейшие попытки предприниматься не будут. Таким образом, всего было предпринято шесть попыток.<br/><br/>Примечание. Если какая-либо из попыток завершается успешно, срез переходит в состояние Ready и дальнейшие попытки не выполняются.<br/><br/>Свойство longRetry можно использовать в ситуациях, когда зависимые данные поступают в неопределенное время или если среда обработки данных нестабильна. В таких случаях последовательные попытки могут оказаться бесполезными, а интервальные, напротив, могут привести к желаемому результату.<br/><br/>Предупреждение. Не задавайте высокие значения для свойств longRetry и longRetryInterval, так как высокие значения могут быть признаками более глубоких системных проблем, которые игнорируются. 
longRetryInterval | TimeSpan | 00:00:00 | Период времени между длительными повторными попытками. 

## Цепочки действий
Если в конвейере несколько действий, и они не зависят друг от друга (выходные данные одного действия не являются входными данными другого действия), то действия могут выполняться параллельно, если срезы входных данных для действий готовы.

Можно объединить в цепочку два действия, используя выходной набор данных одного действия как входной набор данных другого действия. Действия могут находиться в одном конвейере или разных конвейерах. Второе действие выполняется только после успешного завершения первого.

Например, рассмотрим следующий случай.
 
1.	В конвейере P1 есть действие A1, для которого требуется внешний входной набор данных D1. Оно создает **выходной** набор данных **D2**.
2.	В конвейере P2 есть действие A2, для которого требуется **ввод** из набора данных **D2**. Оно создает выходной набор данных D3.
 
В этом случае действие A1 будет выполняться, когда доступны внешние данные и достигнута запланированная частота доступности. Действие A2 будет выполняться, когда доступны запланированные срезы из D2 и достигнута запланированная частота доступности. В случае ошибки в одном из срезов в наборе данных D2 действие A2 не запустится для этого среза, пока он не станет доступным.

Представление схемы будет выглядеть следующим образом.

![Построение цепочки действий в двух конвейерах](./media/data-factory-create-pipelines/chaining-two-pipelines.png)

Представление схемы с обоими действиями в одном конвейере будет выглядеть следующим образом.

![Построение цепочки действий в одном конвейере](./media/data-factory-create-pipelines/chaining-one-pipeline.png)

## Планирование и выполнение
Выше мы рассмотрели, что представляют собой конвейеры и действия, как они определяются и для чего используются в фабрике данных Azure. Теперь рассмотрим, как они выполняются.

Конвейер работает только в период активности, то есть между временем начала и окончания. Он не работает до времени начала и после времени окончания. Если конвейер приостановлен, он не будет работать независимо от значений времени начала и окончания. Запустить конвейер можно только в том случае, если он не находится в приостановленном состоянии. Строго говоря, выполняется не сам конвейер, а действия в нем. Однако это происходит в общем контексте конвейера.

Сведения о планировании и выполнении в фабрике данных Azure см. в разделе [Планирование и выполнение](data-factory-scheduling-and-execution.md).

### Параллельная обработка срезов
В JSON-определении действия задайте для параметра **concurrency** значение больше 1, чтобы во время выполнения несколько срезов обрабатывались параллельно несколькими экземплярами действия. Это весьма полезно при обработке срезов, заполненных в прошлом.


## Создание конвейера и управление им
Фабрика данных Azure позволяет создавать и развертывать конвейеры (содержащие одно или несколько действий) различными способами.

### Использование портала Azure

1. Перейдите на [портал Azure](https://portal.azure.com/).
2. Выберите экземпляр фабрики данных Azure, в котором хотите создать конвейер.
3. Щелкните элемент **Создание и развертывание** в группе связанных элементов **Сводка**.
 
	![Плитка «Создание и развертывание»](./media/data-factory-create-pipelines/author-deploy-tile.png)

4. Нажмите кнопку **Создать конвейер** на панели команд.

	![Кнопка «Создать конвейер»](./media/data-factory-create-pipelines/new-pipeline-button.png)

5. В окне редактора отобразится шаблон JSON конвейера.

	![Редактор конвейера](./media/data-factory-create-pipelines/pipeline-in-editor.png)

6. По окончании создания конвейера нажмите кнопку **Развернуть** на панели команд, чтобы развернуть конвейер.

	**Примечание.** Во время развертывания служба фабрики данных Azure выполнит ряд проверок, помогающих избежать распространенных проблем. В случае ошибки появится соответствующее сообщение. Устраните ошибку, а затем повторно разверните созданный конвейер. Для обновления и удаления конвейера можно использовать редактор.

Полное пошаговое руководство по созданию фабрики данных с конвейером см. в статье [Создание первой фабрики данных Azure с помощью портала Azure и редактора фабрики данных](data-factory-build-your-first-pipeline-using-editor.md).

### Использование подключаемого модуля Visual Studio
Для создания и развертывания конвейеров в фабрике данных Azure можно использовать Visual Studio. Полное пошаговое руководство по созданию фабрики данных с конвейером см. в статье [Создание первой фабрики данных Azure с помощью Microsoft Visual Studio](data-factory-build-your-first-pipeline-using-vs.md).


### Использование Azure PowerShell
Для создания конвейеров в фабрике данных Azure можно использовать Azure PowerShell. Предположим, вы определили JSON конвейера в файле, который находится в папке c:\\DPWikisample.json. В следующем примере показано, как можно загрузить его в экземпляр фабрики данных Azure.

	New-AzureRmDataFactoryPipeline -ResourceGroupName ADF -Name DPWikisample -DataFactoryName wikiADF -File c:\DPWikisample.json

Полное пошаговое руководство по созданию фабрики данных с конвейером см. в статье [Создание первой фабрики данных Azure с помощью Azure PowerShell](data-factory-build-your-first-pipeline-using-powershell.md).

### Использование пакета .NET SDK
Вы можете создавать и развертывать конвейеры, используя пакет .NET SDK. Это способ является программным. Дополнительные сведения см. в разделе [Создание, отслеживание фабрик данных Azure и управление ими программным способом](data-factory-create-data-factories-programmatically.md).


### Использование шаблона Azure Resource Manager (ARM)
Можно создать и развернуть конвейер, используя шаблон Azure Resource Manager (ARM). Дополнительные сведения см. в статье [Руководство. Создание фабрики данных Azure с помощью шаблона диспетчера ресурсов Azure](data-factory-build-your-first-pipeline-using-arm.md).

### Использование интерфейса REST API
Вы можете создавать и развертывать конвейеры также с помощью интерфейса REST API. Это способ является программным. Дополнительные сведения см. в разделе [Создание и обновление конвейеров](https://msdn.microsoft.com/library/azure/dn906741.aspx).


## Управление и мониторинг  
После развертывания конвейера можно управлять им, срезами и циклами выполнения и наблюдать за их состояниями. Дополнительные сведения см. в разделе [Мониторинг конвейеров и управление ими](data-factory-monitor-manage-pipelines.md).

## Дальнейшие действия

- Изучите [планирование и выполнение в фабрике данных Azure](data-factory-scheduling-and-execution.md).
- Ознакомьтесь с информацией о функциях [перемещения](data-factory-data-movement-activities.md) и [преобразования данных](data-factory-data-transformation-activities.md) в фабрике данных Azure.
- Ознакомьтесь со сведениями об [управлении и мониторинге в фабрике данных Azure](data-factory-monitor-manage-pipelines.md).
- [Создайте и разверните свой первый конвейер](data-factory-build-your-first-pipeline.md).

<!---HONumber=AcomDC_0629_2016-->