---
title: Создание конвейеров, цепочки действий и расписаний для них в фабрике данных | Microsoft Docs
description: Узнайте, как создать конвейер данных в фабрике данных Azure для перемещения и преобразования данных. Создание управляемого данными рабочего процесса, который предоставит готовую к использованию информацию.
keywords: конвейер данных, управляемый данными рабочий процесс
services: data-factory
documentationcenter: ''
author: sharonlo101
manager: jhubbard
editor: monicar

ms.service: data-factory
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 09/12/2016
ms.author: shlo

---
# <a name="pipelines-and-activities-in-azure-data-factory"></a>Конвейеры и действия в фабрике данных Azure
В этой статье вы ознакомитесь с принципом работы конвейеров и действий в фабрике данных Azure и узнаете, как с их помощью создавать комплексные рабочие процессы, управляемые данными, для сценариев перемещения и обработки данных.  

> [!NOTE]
> Предполагается, что вы уже изучили [общие сведения фабрике данных Azure](data-factory-introduction.md). Если у вас нет практического опыта создания фабрик данных, рекомендуем для лучшего понимания статьи изучить руководство [Начало работы с фабрикой данных Azure](data-factory-build-your-first-pipeline.md) .  
> 
> 

## <a name="what-is-a-data-pipeline?"></a>Что такое конвейер данных
**Конвейер** — это группа логически связанных **действий**. С его помощью действия объединяются в блок для выполнения определенной задачи. Чтобы лучше понять сущность конвейеров, необходимо сначала разобраться с понятием «действия». 

## <a name="what-is-an-activity?"></a>Что такое действие
Действия определяют то, что нужно выполнить с вашими данными. Каждое действие принимает некоторое количество [наборов данных](data-factory-create-datasets.md) на входе и создает один или несколько наборов данных на выходе. 

Действие копирования, например, может использоваться для управления копированием данных из одного хранилища данных в другое. Аналогичным образом можно использовать действие Hive HDInsight для отправки запроса Hive к кластеру Azure HDInsight для преобразования данных. Фабрика данных Azure позволяет выполнять широкий набор действий по [преобразованию](data-factory-data-transformation-activities.md) и [перемещению данных](data-factory-data-movement-activities.md). Кроме того, вы можете создать действие .NET для выполнения собственного кода. 

## <a name="sample-copy-pipeline"></a>Пример конвейера копирования
В следующем примере конвейера содержится одно действие типа **Copy** in the **действий** . В этом примере [действие копирования](data-factory-data-movement-activities.md) копирует данные из хранилища BLOB-объектов Azure в базу данных SQL Azure. 

    {
      "name": "CopyPipeline",
      "properties": {
        "description": "Copy data from a blob to Azure SQL table",
        "activities": [
          {
            "name": "CopyFromBlobToSQL",
            "type": "Copy",
            "inputs": [
              {
                "name": "InputDataset"
              }
            ],
            "outputs": [
              {
                "name": "OutputDataset"
              }
            ],
            "typeProperties": {
              "source": {
                "type": "BlobSource"
              },
              "sink": {
                "type": "SqlSink",
                "writeBatchSize": 10000,
                "writeBatchTimeout": "60:00:00"
              }
            },
            "Policy": {
              "concurrency": 1,
              "executionPriorityOrder": "NewestFirst",
              "retry": 0,
              "timeout": "01:00:00"
            }
          }
        ],
        "start": "2016-07-12T00:00:00Z",
        "end": "2016-07-13T00:00:00Z"
      }
    } 

Обратите внимание на следующие моменты.

* В разделе действий доступно только одно действие, параметр **type** которого имеет значение **Copy**.
* Для этого действия параметру input присвоено значение **InputDataset**, а параметру output — значение **OutputDataset**.
* В разделе **typeProperties** в качестве типа источника указано **BlobSource**, а в качестве типа приемника — **SqlSink**.

Полное пошаговое руководство по созданию этого конвейера см. в разделе [Копирование данных из хранилища BLOB-объектов Azure в базу данных SQL с помощью фабрики данных](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md). 

## <a name="sample-transformation-pipeline"></a>Пример конвейера преобразования
В следующем примере конвейера содержится одно действие типа **HDInsightHive** in the **действий** . В этом примере [действие HDInsight Hive](data-factory-hive-activity.md) преобразовывает данные из хранилища BLOB-объектов Azure, запуская файл сценария Hive в кластере Azure HDInsight Hadoop. 

    {
        "name": "TransformPipeline",
        "properties": {
            "description": "My first Azure Data Factory pipeline",
            "activities": [
                {
                    "type": "HDInsightHive",
                    "typeProperties": {
                        "scriptPath": "adfgetstarted/script/partitionweblogs.hql",
                        "scriptLinkedService": "AzureStorageLinkedService",
                        "defines": {
                            "inputtable": "wasb://adfgetstarted@<storageaccountname>.blob.core.windows.net/inputdata",
                            "partitionedtable": "wasb://adfgetstarted@<storageaccountname>.blob.core.windows.net/partitioneddata"
                        }
                    },
                    "inputs": [
                        {
                            "name": "AzureBlobInput"
                        }
                    ],
                    "outputs": [
                        {
                            "name": "AzureBlobOutput"
                        }
                    ],
                    "policy": {
                        "concurrency": 1,
                        "retry": 3
                    },
                    "scheduler": {
                        "frequency": "Month",
                        "interval": 1
                    },
                    "name": "RunSampleHiveActivity",
                    "linkedServiceName": "HDInsightOnDemandLinkedService"
                }
            ],
            "start": "2016-04-01T00:00:00Z",
            "end": "2016-04-02T00:00:00Z",
            "isPaused": false
        }
    }

Обратите внимание на следующие моменты. 

* В разделе activities есть только одно действие, параметр **type** которого имеет значение **HDInsightHive**.
* Файл сценария Hive, **partitionweblogs.hql**, хранится в учетной записи хранения Azure (указывается с помощью свойства scriptLinkedService, имеющего значение **AzureStorageLinkedService**) в папке **script** в контейнере **adfgetstarted**.
* Раздел **defines** используется для настройки параметров среды выполнения, которые будут переданы в скрипт Hive в качестве значений конфигурации Hive (например, ${hiveconf:inputtable}, ${hiveconf:partitionedtable}).

Полное пошаговое руководство по созданию данного конвейера см. в разделе [Учебник. Создание первого конвейера для обработки данных с помощью кластера Hadoop](data-factory-build-your-first-pipeline.md). 

## <a name="chaining-activities"></a>Цепочки действий
Если в конвейере несколько действий и выходные данные одного действия не являются входными данными другого действия, такие действия могут выполняться параллельно, при условии, что срезы входных данных для действий готовы. 

Можно объединить в цепочку два действия, используя выходной набор данных одного действия как входной набор данных другого действия. Действия могут находиться в одном конвейере или разных конвейерах. Второе действие выполняется только после успешного завершения первого. 

Например, рассмотрим следующий случай.

1. В конвейере P1 есть действие A1, для которого требуется внешний входной набор данных D1. Оно создает **выходной** набор данных **D2**.
2. В конвейере P2 есть действие A2, для которого требуется **ввод** из набора данных **D2**. Оно создает выходной набор данных D3.

В этом случае действие A1 выполняется, когда доступны внешние данные и достигнута запланированная частота доступности.  Действие A2 выполняется, когда доступны запланированные срезы из D2 и достигнута запланированная частота доступности. В случае ошибки в одном из срезов в наборе данных D2 действие A2 не запустится для этого среза, пока он не станет доступным.

Представление схемы:

![Построение цепочки действий в двух конвейерах](./media/data-factory-create-pipelines/chaining-two-pipelines.png)

Представление схемы с обоими действиями в одном конвейере: 

![Построение цепочки действий в одном конвейере](./media/data-factory-create-pipelines/chaining-one-pipeline.png)

Чтобы узнать больше, ознакомьтесь с [планированием и выполнением](#chaining-activities). 

## <a name="scheduling-and-execution"></a>Планирование и выполнение
Выше мы рассмотрели, что представляют собой конвейеры и действия, как они определяются и для чего используются в фабрике данных Azure. Теперь рассмотрим, как они выполняются. 

Конвейер работает только в период активности, то есть между временем начала и окончания. Он не работает до времени начала и после времени окончания. Если конвейер приостановлен, он не будет работать независимо от значений времени начала и окончания. Запустить конвейер можно только в том случае, если он не находится в приостановленном состоянии. Строго говоря, выполняется не сам конвейер, а действия в нем. Однако это происходит в общем контексте конвейера. 

Сведения о планировании и выполнении в фабрике данных Azure см. в разделе [Планирование и выполнение](data-factory-scheduling-and-execution.md).

## <a name="create-pipelines"></a>Создание конвейеров
Фабрика данных Azure позволяет создавать и развертывать конвейеры (содержащие одно или несколько действий) различными способами. 

### <a name="using-azure-portal"></a>Использование портала Azure
Для создания конвейера можно использовать редактор фабрики данных на портале Azure. Полное пошаговое руководство см. в статье [Руководство. Создание первой фабрики данных Azure с помощью портала Azure](data-factory-build-your-first-pipeline-using-editor.md). 

### <a name="using-visual-studio"></a>Visual Studio
Для создания и развертывания конвейеров в фабрике данных Azure можно использовать Visual Studio. Полное пошаговое руководство см. в статье [Руководство. Создание первой фабрики данных Azure с помощью портала Azure](data-factory-build-your-first-pipeline-using-vs.md). 

### <a name="using-azure-powershell"></a>Использование Azure PowerShell
Для создания конвейеров в фабрике данных Azure можно использовать Azure PowerShell. Предположим, вы определили JSON конвейера в файле, который находится в папке c:\DPWikisample.json. В следующем примере показано, как можно передать его в экземпляр фабрики данных Azure.

    New-AzureRmDataFactoryPipeline -ResourceGroupName ADF -Name DPWikisample -DataFactoryName wikiADF -File c:\DPWikisample.json

Полное пошаговое руководство по созданию фабрики данных с конвейером см. в статье [Создание первой фабрики данных Azure с помощью Azure PowerShell](data-factory-build-your-first-pipeline-using-powershell.md). 

### <a name="using-.net-sdk"></a>Использование пакета .NET SDK
Вы можете создавать и развертывать конвейеры, используя пакет .NET SDK. Этот способ является программным. Дополнительные сведения см. в статье [Создание, отслеживание фабрик данных Azure и управление ими с помощью пакета .NET SDK фабрики данных](data-factory-create-data-factories-programmatically.md). 

### <a name="using-azure-resource-manager-template"></a>Использование шаблона Azure Resource Manager
Конвейер можно создать и развернуть, используя шаблон Azure Resource Manager. Дополнительные сведения см. в статье [Руководство. Создание фабрики данных Azure с помощью шаблона Azure Resource Manager](data-factory-build-your-first-pipeline-using-arm.md). 

### <a name="using-rest-api"></a>Использование интерфейса REST API
Вы можете создавать и развертывать конвейеры также с помощью интерфейса REST API. Этот способ является программным. Дополнительные сведения см. в статье [Создание или обновление конвейера](https://msdn.microsoft.com/library/azure/dn906741.aspx). 

## <a name="monitor-and-manage-pipelines"></a>Мониторинг конвейеров и управление ими
Когда конвейер будет развернут, вы сможете отслеживать состояние конвейеров, срезов и циклов выполнения, а также управлять этими объектами. Дополнительные сведения см. в разделе [Мониторинг конвейеров и управление ими](data-factory-monitor-manage-pipelines.md).

## <a name="pipeline-json"></a>Конвейер JSON
Рассмотрим подробнее определение конвейера в формате JSON. В общем виде структуру конвейера можно представить приведенным ниже образом.

    {
        "name": "PipelineName",
        "properties": 
        {
            "description" : "pipeline description",
            "activities":
            [

            ],
            "start": "<start date-time>",
            "end": "<end date-time>"
        }
    }

В разделе **activities** можно определить одно или несколько действий. Каждое действие имеет следующую структуру верхнего уровня.

    {
        "name": "ActivityName",
        "description": "description", 
        "type": "<ActivityType>",
        "inputs":  "[]",
        "outputs":  "[]",
        "linkedServiceName": "MyLinkedService",
        "typeProperties":
        {

        },
        "policy":
        {
        }
        "scheduler":
        {
        }
    }

В приведенной ниже таблице описаны свойства, используемые в определениях JSON действия и конвейера.

| Тег | Описание | Обязательно |
| --- | --- | --- |
| name |Имя действия или конвейера. Укажите имя, представляющее операцию, для выполнения которой настроено действие или конвейер.<br/><ul><li>Максимальное количество знаков: 260.</li><li>Должно начинаться с буквы, цифры или символа подчеркивания (_).</li><li>Следующие знаки не допускаются: ".", "+", "?", "/", "<", ">", "*", "%", "&", ":", "\\".</li></ul> |Да |
| Описание |Текст, описывающий, для чего используется действие или конвейер |Да |
| type |Задает тип действия. Разные типы действий описаны в статьях, посвященных [действиям перемещения данных](data-factory-data-movement-activities.md) и [действиям преобразования данных](data-factory-data-transformation-activities.md). |Да |
| inputs |Входные таблицы, используемые действием:<br/><br/>// одна входная таблица<br/>"inputs":  [ { "name": "inputtable1"  } ],<br/><br/>// две входные таблицы <br/>"inputs":  [ { "name": "inputtable1"  }, { "name": "inputtable2"  } ], |Да |
| outputs |Выходные таблицы, используемые действием: // одна выходная таблица<br/>"outputs":  [ { "name": “outputtable1” } ],<br/><br/>//две выходные таблицы<br/>"outputs":  [ { "name": “outputtable1” }, { "name": “outputtable2” }  ], |Да |
| linkedServiceName (имя связанной службы) |Имя связанной службы, используемой действием. <br/><br/>Для действия может потребоваться указать службу, связанную с обязательной вычислительной средой. |Да — для действия HDInsight и действия пакетной оценки показателей машинного обучения Azure;  <br/><br/>Нет — для всех остальных |
| typeProperties |Свойства в разделе typeProperties зависят от типа действия. |Нет |
| policy |Политики, которые влияют на поведение во время выполнения действия. Если для этого свойства не задано значение, используются стандартные политики. |Нет |
| start |Дата и время начала работы конвейера. Задается в [формате ISO](http://en.wikipedia.org/wiki/ISO_8601). Например, 2014-10-14T16:32:41Z. <br/><br/>Можно указать местное время, например восточное поясное время (EST). Вот пример: "2016-02-27T06:00:00**-05:00**". Это 6:00 по восточному стандартному времени.<br/><br/> Свойства start и end определяют активный период работы конвейера. Срезы выходных данных создаются только в этот активный период. |Нет<br/><br/>Если вы указываете значение свойства end, вы также должны указать значение свойства start.<br/><br/>Для создания конвейера значения времени начала и времени окончания могут быть пустыми. Если требуется задать активный период работы конвейера, следует указать оба значения. Если вы не указали время начала и окончания при создании конвейера, их можно установить позже с помощью командлета Set-AzureRmDataFactoryPipelineActivePeriod. |
| end |Дата и время завершения работы конвейера. Не является обязательным и задается в формате ISO. Например: 2014-10-14T17:32:41Z <br/><br/>Можно указать местное время, например восточное поясное время (EST). Вот пример: "2016-02-27T06:00:00**-05:00**". Это 6:00 по восточному стандартному времени.<br/><br/> Чтобы работа конвейера не была ограничена во времени, укажите для свойства end значение 9999-09-09. |Нет <br/><br/>Если вы указываете значение свойства end, вы также должны указать значение свойства start.<br/><br/>Ознакомьтесь с примечаниями к свойству **start**. |
| isPaused |Если задано значение true, конвейер не запускается. Значение по умолчанию — false. Это свойство можно использовать для включения или отключения. |Нет |
| scheduler |Свойство scheduler позволяет задать расписание выполнения действия. Для него предусмотрен такой же набор подсвойств, что и для [свойства availability в наборе данных](data-factory-create-datasets.md#Availability). |Нет |
| pipelineMode |Определяет метод планирования работы конвейера. Допустимые значения: scheduled (по умолчанию), onetime.<br/><br/>Значение scheduled означает, что конвейер будет запускаться с указанной периодичностью в соответствии с его активным периодом (временем начала и окончания). Значение onetime означает, что конвейер будет запускаться только один раз. В настоящее время изменить или обновить однократные конвейеры после их создания нельзя. Подробные сведения об однократном запуске см. в разделе [Однократный конвейер](data-factory-scheduling-and-execution.md#onetime-pipeline). |Нет |
| expirationTime; |Период времени после создания, в течение которого конвейер является допустимым и должен оставаться подготовленным. Если на момент завершения этого периода у конвейера не будет активных, невыполненных или ожидающих выполнения запусков, конвейер будет автоматически удален. |Нет |
| datasets |Список наборов данных для использования действиями, определенными в конвейере. Это свойство можно использовать для определения наборов данных, характерных для этого конвейера и не определенных в фабрике данных. Наборы данных, определенные в этом конвейере, могут использоваться только этим конвейером и не предназначены для совместного доступа. Дополнительные сведения см. в разделе [Контекст наборов данных](data-factory-create-datasets.md#scoped-datasets). |Нет |

### <a name="policies"></a>Политики
Политики влияют на поведение во время выполнения действия, особенно при обработке среза таблицы. В следующей таблице приведено несколько примеров.

| Свойство | Допустимые значения | Значение по умолчанию | Описание |
| --- | --- | --- | --- |
| concurrency |Целое число  <br/><br/> Максимальное значение — 10 |1 |Число одновременных выполнений действия.<br/><br/>Определяет количество параллельных выполнений одного действия для обработки разных срезов. Например, высокое значение этого свойства ускорит обработку большого набора доступных данных. |
| executionPriorityOrder |NewestFirst<br/><br/>OldestFirst |OldestFirst |Определяет порядок обработки срезов данных.<br/><br/>Предположим, есть два ожидающих обработки среза (от 16:00 и от 17:00). Если для свойства executionPriorityOrder задано значение NewestFirst, срез от 17:00 будет обработан первым. Точно так же, если для executionPriorityORder задано значение OldestFIrst, первым будет обработан срез от 16:00. |
| retry |Целое число <br/><br/>Максимальное значение — 10 |3 |Число повторных попыток обработки данных до того, как срез перейдет в состояние Failure (сбой). Выполнение действия со срезом данных повторяется указанное количество раз. Повторная попытка выполняется сразу после неудачной. |
| timeout |TimeSpan |00:00:00 |Время ожидания для действия. Пример: 00:10:00 (время ожидания — 10 минут).<br/><br/>Если значение не указано или равно 0, то время ожидания не ограничено.<br/><br/>Если время обработки среза превышает время ожидания, система отменяет текущую обработку и начинает новую. Количество повторов зависит от значения свойства retry. Когда время ожидания истекает, состояние среза меняется на TimedOut. |
| delay |TimeSpan |00:00:00 |Задайте задержку перед обработкой данных после начала выполнения среза.<br/><br/>Действие для среза данных запускается в ожидаемое время выполнения с указанной задержкой.<br/><br/>Пример: 00:10:00 (означает задержку в 10 минут). |
| longRetry |Целое число <br/><br/> Максимальное значение — 10 |1 |Количество длительных повторных попыток перед завершением сбоем выполнения среза.<br/><br/>Интервал между этими попытками задается свойством longRetryInterval. Используйте свойство longRetry, если повторные попытки необходимо выполнять с паузами. Если указаны свойства Retry и longRetry, то каждая попытка longRetry включает в себя попытки Retry, и максимальное число попыток равно Retry * longRetry.<br/><br/>Например, в политике действия указаны следующие параметры:<br/>Retry: 3<br/>longRetry: 2<br/>longRetryInterval: 01:00:00<br/><br/>Предположим, что существует только один выполняемый срез (в состоянии Waiting), и каждый раз при выполнении действия происходит сбой. Первые три попытки будут выполнены подряд. После каждой повторной попытки срез будет находиться в состоянии Retry. После выполнения первых трех попыток состоянием среза станет LongRetry.<br/><br/>Через час (значение свойства longRetryInterval) будут выполнены еще три попытки подряд. После этого состояние среза изменится на Failed и дальнейшие попытки предприниматься не будут. Поэтому всего было предпринято 6 попыток.<br/><br/>Если какое-либо выполнение завершится успешно, то состоянием среза станет Ready и дальнейшие попытки выполняться не будут.<br/><br/>Свойство longRetry можно использовать в ситуациях, когда зависимые данные поступают в неопределенное время или вся среда, в которой происходит обработка данных, непредсказуема. В таких случаях последовательные повторные попытки могут оказаться бесполезными, а выполненные через некоторое время, напротив, могут привести к желаемому результату.<br/><br/>Предупреждение. Не задавайте высокие значения для свойств longRetry и longRetryInterval. Как правило, более высокие значения приводят к появлению других системных проблем. |
| longRetryInterval |TimeSpan |00:00:00 |Период времени между длительными повторными попытками. |

## <a name="next-steps"></a>Дальнейшие действия
* Изучите [планирование и выполнение в фабрике данных Azure](data-factory-scheduling-and-execution.md).  
* Ознакомьтесь с информацией о функциях [перемещения](data-factory-data-movement-activities.md) и [преобразования данных](data-factory-data-transformation-activities.md) в фабрике данных Azure.
* Ознакомьтесь со сведениями об [управлении и мониторинге в фабрике данных Azure](data-factory-monitor-manage-pipelines.md).
* [Создайте и разверните свой первый конвейер](data-factory-build-your-first-pipeline.md). 

<!--HONumber=Oct16_HO2-->


