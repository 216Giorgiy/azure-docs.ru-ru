<properties 
	pageTitle="Планирование и исполнение с использованием фабрики данных" 
	description="Сведения об аспектах планирования и исполнения в модели приложений фабрики данных Azure." 
	services="data-factory" 
	documentationCenter="" 
	authors="spelluru" 
	manager="jhubbard" 
	editor="monicar"/>

<tags 
	ms.service="data-factory" 
	ms.workload="data-services" 
	ms.tgt_pltfrm="na" 
	ms.devlang="na" 
	ms.topic="article" 
	ms.date="10/20/2015" 
	ms.author="spelluru"/>

# Планирование и исполнение с использованием фабрики данных
  
Здесь объясняются аспекты планирования и исполнения в модели приложений фабрики данных. Эта статья основана на статьях [Создание конвейеров](data-factory-create-pipelines.md) и [Создание наборов данных](data-factory-create-datasets.md). Чтобы понимать ее, нужно знать основные концепции, связанные с моделью приложений фабрики данных: действия, конвейеры, связанные службы и наборы данных.

## Планирование действий

С помощью раздела **scheduler** в JSON действия можно указать регулярное расписание данного действия. Например, можно запланировать выполнение действия каждый час следующим образом.

	"scheduler": {
		"frequency": "Hour",
	    "interval": 1
	},  
    
![Пример планировщика](./media/data-factory-scheduling-and-execution/scheduler-example.png)

Как показано выше, при указании ежечасного расписания создаются запуски действия, соответствующие последовательности стыкующихся окон. Стыкующиеся окна — это ряд интервалов фиксированного размера, которые не перекрываются и не соприкасаются.
 
Для выполнения текущего действия доступ к интервалу окна времени можно получить с помощью системных переменных **WindowStart** и **WindowEnd** в JSON действия. Эти переменные вы можете использовать для различных целей в JSON действия и скриптах, связанных с действием, включая выбор данных из входных и выходных наборов данных, соответствующих временным рядам.

Дополнительные сведения о различных свойствах, доступных для планировщика, в том числе планирование с определенным смещением времени, установку режима выравнивания обработки в начале или в конце интервала окна, см. в статье [Создание конвейеров](data-factory-create-pipelines.md).

## Наборы данных и срезы данных временных рядов

Данные временного ряда — это непрерывная последовательность точек данных, состоящая, как правило, из последовательных измерений, выполненных с некоторым интервалом времени. Распространенными примерами данных временных рядов являются данные датчиков, данные телеметрии приложений и т. д.

С помощью фабрики данных Azure данные временных рядов можно обрабатывать в пакетном режиме с выполнением действий. Обычно входные данные поступают, а выходные данные требуют обработки с повторяющейся периодичностью. Эта периодичность моделируется путем указания раздела **availability** в наборе данных приведенным ниже образом.

    "availability": {
      "frequency": "Hour",
      "interval": 1
    },

Каждая единица данных, потребляемых и производимых запуском действия, называется **срезом** данных. На схеме ниже показан пример действия с входным и выходным наборами данных временных рядов, для каждого из которых доступность установлена с почасовой частотой.

![Планировщик доступности](./media/data-factory-scheduling-and-execution/availability-scheduler.png)

Почасовые срезы данных для входного и выходного наборов данных показаны на схеме выше. На схеме показаны 3 входных среза, которые готовы к обработке, и выполняемое действие 10–11AM, которое выдает выходной срез 10–11AM.

К интервалу времени, связанному с текущим обрабатываемым срезом, можно получить доступ в JSON набора данных посредством переменных **SliceStart** и **SliceEnd**.

Дополнительные сведения о различных свойствах из раздела availability см. в статье [Создание наборов данных](data-factory-create-datasets.md).

## Пример. Действие копирования, перемещающее данные из SQL Azure в BLOB-объект Azure

Давайте объединим некоторые функции в действии, вернувшись к примеру из статьи [Создание конвейеров](data-factory-create-pipelines.md), где данные копируются каждый час из таблицы SQL Azure в BLOB-объект Azure.

**Входные данные: набор данных SQL Azure**

	{
	    "name": "AzureSqlInput",
	    "properties": {
	        "published": false,
	        "type": "AzureSqlTable",
	        "linkedServiceName": "AzureSqlLinkedService",
	        "typeProperties": {
	            "tableName": "MyTable"
	        },
	        "availability": {
	            "frequency": "Hour",
	            "interval": 1
	        },
	        "external": true,
	        "policy": {}
	    }
	}


Обратите внимание, что в разделе **availability** для параметра **frequency** установлено значение **Hour**, а для параметра **interval** — значение **1**.

**Выходные данные: набор данных BLOB-объекта Azure**
	
	{
	    "name": "AzureBlobOutput",
	    "properties": {
	        "published": false,
	        "type": "AzureBlob",
	        "linkedServiceName": "StorageLinkedService",
	        "typeProperties": {
	            "folderPath": "mypath/{Year}/{Month}/{Day}/{Hour}",
	            "format": {
	                "type": "TextFormat"
	            },
	            "partitionedBy": [
	                {
	                    "name": "Year",
	                    "value": {
	                        "type": "DateTime",
	                        "date": "SliceStart",
	                        "format": "yyyy"
	                    }
	                },
	                {
	                    "name": "Month",
	                    "value": {
	                        "type": "DateTime",
	                        "date": "SliceStart",
	                        "format": "%M"
	                    }
	                },
	                {
	                    "name": "Day",
	                    "value": {
	                        "type": "DateTime",
	                        "date": "SliceStart",
	                        "format": "%d"
	                    }
	                },
	                {
	                    "name": "Hour",
	                    "value": {
	                        "type": "DateTime",
	                        "date": "SliceStart",
	                        "format": "%H"
	                    }
	                }
	            ]
	        },
	        "availability": {
	            "frequency": "Hour",
	            "interval": 1
	        }
	    }
	}


Обратите внимание, что в разделе **availability** для параметра **frequency** установлено значение **Hour**, а для параметра **interval** — значение **1**.



**Действие: действие копирования**

	{
	    "name": "SamplePipeline",
	    "properties": {
	        "description": "copy activity",
	        "activities": [
	            {
	                "type": "Copy",
	                "name": "AzureSQLtoBlob",
	                "description": "copy activity",	
	                "typeProperties": {
	                    "source": {
	                        "type": "SqlSource",
	                        "sqlReaderQuery": "$$Text.Format('select * from MyTable where timestampcolumn >= \\'{0:yyyy-MM-dd HH:mm}\\' AND timestampcolumn < \\'{1:yyyy-MM-dd HH:mm}\\'', WindowStart, WindowEnd)"
	                    },
	                    "sink": {
	                        "type": "BlobSink",
	                        "writeBatchSize": 100000,
	                        "writeBatchTimeout": "00:05:00"
	                    }
	                },
	                "inputs": [
	                    {
	                        "name": "AzureSQLInput"
	                    }
	                ],
	                "outputs": [
	                    {
	                        "name": "AzureBlobOutput"
	                    }
	                ],
	       			"scheduler": {
	          			"frequency": "Hour",
	          			"interval": 1
	        		}
	            }
	        ],
	        "start": "2015-01-01T08:00:00Z",
	        "end": "2015-01-01T11:00:00Z"
	    }
	}


В предыдущем примере показаны разделы расписания действий и доступности набора данных, для которых установлена почасовая частота. В примере показано, как с помощью переменных **WindowStart** и **WindowEnd** выбрать значимые данные для заданного запуска действия и отправить их в BLOB-объект с соответствующим динамическим путем к папке (**folderPath**), параметризованным так, чтобы для каждого часа была отдельная папка.

При выполнении 3 срезов между 8 и 11 часами утра таблица Azure и BLOB-объект из примера будут выглядеть таким образом.

Предположим, что в SQL Azure содержатся приведенные ниже данные.

![Пример ввода](./media/data-factory-scheduling-and-execution/sample-input-data.png)

При развертывании указанного выше конвейера BLOB-объект Azure будет заполнен следующим образом:

1.	Файл mypath/2015/1/1/8/Data.<Guid>.txt с данными. 

		10002345,334,2,2015-01-01 08:24:00.3130000
		10002345,347,15,2015-01-01 08:24:00.6570000
		10991568,2,7,2015-01-01 08:56:34.5300000

	**Примечание.** Вместо <Guid> будет указан фактический идентификатор GUID. Пример имени файла: Data.bcde1348-7620-4f93-bb89-0eed3455890b.txt.
2.	Файл mypath/2015/1/1/9/Data.<Guid>.txt с данными.

		10002345,334,1,2015-01-01 09:13:00.3900000
		24379245,569,23,2015-01-01 09:25:00.3130000
		16777799,21,115,2015-01-01 09:47:34.3130000
3.	Файл mypath/2015/1/1/10/Data.<Guid>.txt без данных.


## Срезы данных, активный период конвейера и параллельное выполнение срезов

В статье [Создание конвейеров](data-factory-create-pipelines.md) представлена концепция активного периода для конвейера, указываемого установкой свойств **start** и **end** конвейера.
 
Вы можете задать дату начала для активного периода конвейера, находящуюся в прошлом, и фабрика данных автоматически вычислит все срезы данных из прошлого (выполнит обратное заполнение) и начнет их обработку.

После обратного заполнения срезов данных можно настроить их параллельное выполнение. Вы можете это сделать, задав свойство concurrency в разделе **policy** JSON действия, как показано в статье [Создание конвейеров](data-factory-create-pipelines.md).

## Повторный запуск сбойных срезов данных и автоматическое отслеживание зависимостей данных

Существуют широкие визуальные возможности для мониторинга выполнения срезов. Подробные сведения см. в статье [Мониторинг конвейеров и управление ими](data-factory-monitor-manage-pipelines.md).

Рассмотрим следующий пример, в котором показаны два действия. Действие Activity1 выдает набор данных временного ряда со срезами в качестве выходных данных, которые потребляются как входные данные действием Activity2 для формирования конечного выходного набора данных временного ряда.

![Срез, в котором произошла ошибка](./media/data-factory-scheduling-and-execution/failed-slice.png)

<br/>

На схеме выше показано, что среди 3 последних срезов произошла ошибка создания среза 9–10 AM для набора данных **Dataset2**. Фабрика данных автоматически отслеживает зависимости набора данных временного ряда и в результате задерживает запуск действия для нижестоящего среза 9-10 AM.


Средства мониторинга и управления фабриками данных позволяют детально просмотреть журналы диагностики на предмет неудачного среза, легко найти причину неполадки и устранить ее. После устранения неполадки можно также легко инициировать запуск действия для создания среза, в котором произошла ошибка. Дополнительные сведения о том, как инициировать повторные запуски, а также о переходах от одного состояния срезов данных к другому, см. в статье о [мониторинге и управлении](data-factory-monitor-manage-pipelines.md).

После инициирования повторного запуска, когда срез 9–10AM для dataset2 будет готов, фабрика данных инициирует запуск для зависимого среза 9–10 AM в конечном наборе данных, как показано на схеме ниже.

![Повторный запуск среза, в котором произошла ошибка](./media/data-factory-scheduling-and-execution/rerun-failed-slice.png)

Для более глубокого знакомства с указанием и отслеживанием зависимостей в сложных цепочках действий и наборов см. следующие разделы.

## Моделирование наборов данных с разной частотой

В примерах, приведенных выше, частоты входных и выходных наборов данных и окон расписания действий были одинаковыми. В некоторых сценариях требуется возможность создавать выходные данные с частотой, отличной от частоты одного или нескольких наборов входных данных. Фабрика данных поддерживает моделирование таких сценариев.

### Пример 1. Создание ежедневного выходного отчета по входным данным, которые доступны каждый час

Рассмотрим сценарий, где имеются входные данные измерений датчиков, доступные каждый час в BLOB-объекте Azure, а нам требуется формировать ежедневный совокупный отчет со статистикой средних, максимальных, минимальных и т. п. показателей за день с помощью фабрики данных [Действие Hive](data-factory-hive-activity.md).

Смоделировать это с помощью фабрики данных можно приведенным ниже способом.

**Входной набор данных BLOB-объекта Azure**

Почасовые входные файлы за заданный день удаляются из папки. Для входных данных устанавливается почасовая доступность (frequency: Hour, interval: 1).

	{
	  "name": "AzureBlobInput",
	  "properties": {
	    "type": "AzureBlob",
	    "linkedServiceName": "StorageLinkedService",
	    "typeProperties": {
	      "folderPath": "mycontainer/myfolder/{Year}/{Month}/{Day}/",
	      "partitionedBy": [
	        { "name": "Year", "value": {"type": "DateTime","date": "SliceStart","format": "yyyy"}},
	        { "name": "Month","value": {"type": "DateTime","date": "SliceStart","format": "%M"}},
	        { "name": "Day","value": {"type": "DateTime","date": "SliceStart","format": "%d"}}
	      ],
	      "format": {
	        "type": "TextFormat"
	      }
	    },
		"external": true,
	    "availability": {
	      "frequency": "Hour",
	      "interval": 1
	    }
	  }
	}

**Выходной набор данных BLOB-объекта Azure**

Каждый день из папки удаляется один выходной файл за целый день. Для выходных данных устанавливается ежедневная доступность (frequency: Day, interval: 1).


	{
	  "name": "AzureBlobOutput",
	  "properties": {
	    "type": "AzureBlob",
	    "linkedServiceName": "StorageLinkedService",
	    "typeProperties": {
	      "folderPath": "mycontainer/myfolder/{Year}/{Month}/{Day}/",
	      "partitionedBy": [
	        { "name": "Year", "value": {"type": "DateTime","date": "SliceStart","format": "yyyy"}},
	        { "name": "Month","value": {"type": "DateTime","date": "SliceStart","format": "%M"}},
	        { "name": "Day","value": {"type": "DateTime","date": "SliceStart","format": "%d"}}
	      ],
	      "format": {
	        "type": "TextFormat"
	      }
	    },
	    "availability": {
	      "frequency": "Day",
	      "interval": 1
	    }
	  }
	}

**Действие: действие Hive в конвейере**

Скрипт hive получает соответствующие сведения о дате и времени в качестве параметров, используя переменную **WindowStart**, как показано ниже. Эту переменную скрипт hive использует для загрузки данных из нужной папки за день и для создания выходных данных путем агрегирования.

		{  
		    "name":"SamplePipeline",
		    "properties":{  
		    "start":"2015-01-01T08:00:00",
		    "end":"2015-01-01T11:00:00",
		    "description":"hive activity",
		    "activities": [
		        {
		            "name": "SampleHiveActivity",
		            "inputs": [
		                {
		                    "name": "AzureBlobInput"
		                }
		            ],
		            "outputs": [
		                {
		                    "name": "AzureBlobOutput"
		                }
		            ],
		            "linkedServiceName": "HDInsightLinkedService",
		            "type": "HDInsightHive",
		            "typeProperties": {
		                "scriptPath": "adftutorial\\hivequery.hql",
		                "scriptLinkedService": "StorageLinkedService",
		                "defines": {
		                    "Year": "$$Text.Format('{0:yyyy}',WindowsStart)",
		                    "Month": "$$Text.Format('{0:%M}',WindowStart)",
		                    "Day": "$$Text.Format('{0:%d}',WindowStart)"
		                }
		            },
		            "scheduler": {
		                "frequency": "Day",
		                "interval": 1
		            },			
		            "policy": {
		                "concurrency": 1,
		                "executionPriorityOrder": "OldestFirst",
		                "retry": 2,
		                "timeout": "01:00:00"
		            }
	             }
		     ]
		   }
		}

Вот как это выглядит с точки зрения зависимостей данных.

![Зависимость данных](./media/data-factory-scheduling-and-execution/data-dependency.png)

Выходной срез за каждый день зависит от 24 почасовых срезов из входного набора данных. Фабрика данных автоматически вычисляет эти зависимости, определяя срезы, которые попадают в тот же период времени, что и создаваемый выходной срез. Если какой-либо из 24 входных срезов недоступен (например, из-за продолжающейся обработки в вышестоящем действии, которое создает этот срез), фабрика данных будет дожидаться готовности входного среза перед запуском ежедневного действия.


### Пример 2. Указание зависимости с использованием выражений и функций фабрики данных

Рассмотрим другой сценарий. Предположим, имеется действие Hive, которое обрабатывает два входных набора данных, в одном из которых новые данные появляются ежедневно, а в другом — раз в неделю. Предположим, что требуется выполнить соединение двух входных наборов и выдать ежедневный выходной набор данных.
 
Раньше использовался простой подход: фабрика данных автоматически определяла нужные входные срезы для обработки, добавляя входные срезы данных, согласованные с периодом времени выходного среза данных. Сейчас этот подход уже не работает.

Необходим способ указать фабрике данных для каждого действия, что следует использовать срез данных за последнюю неделю из еженедельного входного набора данных. Это можно сделать с помощью функций фабрики данных Azure, как показано ниже.

**Входной набор 1: BLOB-объект Azure**

Первый входной набор данных представляет собой BLOB-объект Azure, обновляемый **ежедневно**.
	
	{
	  "name": "AzureBlobInputDaily",
	  "properties": {
	    "type": "AzureBlob",
	    "linkedServiceName": "StorageLinkedService",
	    "typeProperties": {
	      "folderPath": "mycontainer/myfolder/{Year}/{Month}/{Day}/",
	      "partitionedBy": [
	        { "name": "Year", "value": {"type": "DateTime","date": "SliceStart","format": "yyyy"}},
	        { "name": "Month","value": {"type": "DateTime","date": "SliceStart","format": "%M"}},
	        { "name": "Day","value": {"type": "DateTime","date": "SliceStart","format": "%d"}}
	      ],
	      "format": {
	        "type": "TextFormat"
	      }
	    },
		"external": true,
	    "availability": {
	      "frequency": "Day",
	      "interval": 1
	    }
	  }
	}

**Входной набор 2: BLOB-объект Azure**

Второй входной набор — BLOB-объект Azure, обновляемый **еженедельно**.

	{
	  "name": "AzureBlobInputWeekly",
	  "properties": {
	    "type": "AzureBlob",
	    "linkedServiceName": "StorageLinkedService",
	    "typeProperties": {
	      "folderPath": "mycontainer/myfolder/{Year}/{Month}/{Day}/",
	      "partitionedBy": [
	        { "name": "Year", "value": {"type": "DateTime","date": "SliceStart","format": "yyyy"}},
	        { "name": "Month","value": {"type": "DateTime","date": "SliceStart","format": "%M"}},
	        { "name": "Day","value": {"type": "DateTime","date": "SliceStart","format": "%d"}}
	      ],
	      "format": {
	        "type": "TextFormat"
	      }
	    },
		"external": true,
	    "availability": {
	      "frequency": "Day",
	      "interval": 7
	    }
	  }
	}

**Выходной набор данных: BLOB-объект Azure**

Каждый день из папки удаляется один выходной файл за целый день. Для выходных данных задается ежедневная доступность (frequency: Day, interval: 1).
	
	{
	  "name": "AzureBlobOutputDaily",
	  "properties": {
	    "type": "AzureBlob",
	    "linkedServiceName": "StorageLinkedService",
	    "typeProperties": {
	      "folderPath": "mycontainer/myfolder/{Year}/{Month}/{Day}/",
	      "partitionedBy": [
	        { "name": "Year", "value": {"type": "DateTime","date": "SliceStart","format": "yyyy"}},
	        { "name": "Month","value": {"type": "DateTime","date": "SliceStart","format": "%M"}},
	        { "name": "Day","value": {"type": "DateTime","date": "SliceStart","format": "%d"}}
	      ],
	      "format": {
	        "type": "TextFormat"
	      }
	    },
	    "availability": {
	      "frequency": "Day",
	      "interval": 1
	    }
	  }
	}

**Действие: действие Hive в конвейере**

Действие hive принимает 2 входных набора данных и создает выходной срез данных каждый день. Зависимость ежедневного выходного среза от входного среза прошлой недели вы можете задать для еженедельных входных данных приведенным ниже образом.
	
	{  
	    "name":"SamplePipeline",
	    "properties":{  
	    "start":"2015-01-01T08:00:00",
	    "end":"2015-01-01T11:00:00",
	    "description":"hive activity",
	    "activities": [
	      {
	        "name": "SampleHiveActivity",
	        "inputs": [
	          {
	            "name": "AzureBlobInputDaily"
	          },
	          {
	            "name": "AzureBlobInputWeekly",
	            "startTime": "Date.AddDays(SliceStart,  -7 - Date.DayOfWeek(SliceStart))",
	            "endTime": "Date.AddDays(SliceEnd,  -Date.DayOfWeek(SliceEnd))"  
	          }
	        ],
	        "outputs": [
	          {
	            "name": "AzureBlobOutputDaily"
	          }
	        ],
	        "linkedServiceName": "HDInsightLinkedService",
	        "type": "HDInsightHive",
	        "typeProperties": {
	          "scriptPath": "adftutorial\\hivequery.hql",
	          "scriptLinkedService": "StorageLinkedService",
	          "defines": {
	            "Year": "$$Text.Format('{0:yyyy}',WindowsStart)",
	            "Month": "$$Text.Format('{0:%M}',WindowStart)",
	            "Day": "$$Text.Format('{0:%d}',WindowStart)"
	          }
	        },
	        "scheduler": {
	          "frequency": "Day",
	          "interval": 1
	        },			
	        "policy": {
	          "concurrency": 1,
	          "executionPriorityOrder": "OldestFirst",
	          "retry": 2,  
	          "timeout": "01:00:00"
	        }
		   } 
	     ]
	   }
	}



## Системные переменные фабрики данных

Имя переменной | Описание | Область объекта | Область JSON и примеры использования
------------- | ----------- | ------------ | ------------------------
WindowStart | Начало интервала времени для текущего окна запуска действия. | действие | <ol><li>Укажите запросы выбора данных. См. статьи о соединителях, на которые ссылается статья [Действия перемещения данных](data-factory-data-movement-activities.md).</li><li>Передайте параметры в скрипт Hive (пример показан выше).</li>
WindowEnd | Конец интервала времени для текущего окна запуска действия. | действие | то же, что и выше
SliceStart | Начало интервала времени для формируемого среза данных. | действие<br/>набор данных | <ol><li>Укажите динамические пути к папкам и имена файлов во время работы с [BLOB-объектом Azure](data-factory-azure-blob-connector.md) и [наборами данных файловой системы](data-factory-onprem-file-system-connector.md).</li><li>Укажите входные зависимости с помощью функций фабрики данных в коллекции входных данных действия.</li></ol>
SliceEnd | Конец интервала времени для текущего формируемого среза данных. | действие<br/>набор данных | то же, что и выше 

> [AZURE.NOTE]В настоящее время для фабрики данных требуется, чтобы расписание, указанное в действии, в точности соответствовало расписанию, указанному в разделе доступности выходного набора данных. Это значит, что WindowStart, WindowEnd, SliceStart и SliceEnd всегда сопоставляются с одним и тем же периодом времени и одиночным выходным срезом.
 
## Справочник по функциям фабрики данных

Функции фабрики данных можно использовать вместе с вышеупомянутыми системными переменными для следующих целей.

1.	Указание запросов выбора данных (см. статьи о соединителях, на которые ссылается статья [Действия перемещения данных](data-factory-data-movement-activities.md)).

	Синтаксис, необходимый для вызова функции фабрики данных, — это **$$<function>** (используется для запросов выбора данных и других свойств действий и наборов данных).  
2. Указание входных зависимостей с помощью функций фабрики данных в коллекции входных данных действия (см. пример выше).

	Префикс $$ не требуется при указании выражений входных зависимостей.

В следующем примере свойству **sqlReaderQuery** в JSON-файле присваивается значение, возвращаемое функцией **Text.Format**. Кроме того, в этом примере используется системная переменная с именем **WindowStart**, которая отображает время начала для окна запуска действия.
	
	{
	    "Type": "SqlSource",
	    "sqlReaderQuery": "$$Text.Format('SELECT * FROM MyTable WHERE StartTime = \\'{0:yyyyMMdd-HH}\\'', WindowStart)"
	}

### Функции

В приведенных ниже таблицах перечислены все функции фабрики данных Azure.

Категория | Функция | Параметры | Описание
-------- | -------- | ---------- | ----------- 
Время | AddHours(X,Y) | X: DateTime <p>Y: int</p> | Добавляет Y часов к заданному времени X. <p>Пример: 05.09.2013 12:00:00 + 2 часа = 05.09.2013 14:00:00</p>.
Время | AddMinutes(X,Y) | X: DateTime <p>Y: int</p> | Добавляет Y минут к X.<p>Пример: 15.09.2013 12:00:00 + 15 минут = 15.09.2013 12:15:00</p>.
Время | StartOfHour(X) | X: Datetime | Получает то время, когда начинался час, отображаемый компонентом часа в параметре X. <p>Пример: параметр StartOfHour для 15.09.2013 17:10:23 имеет значение 15.09.2013 17:00:00</p>.
Дата | AddDays(X,Y) | X: DateTime<p>Y: int</p> | Добавляет Y дней к X.<p>Пример: 15.09.2013 12:00:00 + 2 дня = 17.09.2013 12:00:00</p>.
Дата | AddMonths(X,Y) | X: DateTime<p>Y: int</p> | Добавляет Y месяцев к X.<p>Пример: 15.09.2013 12:00:00 + 1 месяц = 15.10.2013 12:00:00</p>. 
Дата | AddQuarters(X,Y) | X: DateTime <p>Y: int</p> | Добавляет Y (одна единица которого равна 3 месяцам) к X.<p>Пример: 15.09.2013 12:00:00 + 1 квартал = 15.12.2013 12:00:00</p>.
Дата | AddWeeks(X,Y) | X: DateTime<p>Y: int</p> | Добавляет Y (одна единица которого равна 7 дням) к X<p>Пример: 15.09.2013 12:00:00 + 1 неделя = 22.09.2013 12:00:00</p>.
Дата | AddYears(X,Y) | X: DateTime<p>Y: int</p> | Добавляет Y лет к X.<p>Пример: 15.09.2013 12:00:00 + 1 год = 15.09.2014 12:00:00</p>.
Дата | Day(X) | X: DateTime | Получает компонент дня для X.<p>Пример: Day для 15.09.2013 12:00:00 имеет значение 15. </p>
Дата | DayOfWeek(X) | X: DateTime | Получает компонент дня недели, имеющийся в параметре X.<p>Пример: DayOfWeek для 15.09.2013 12:00:00 имеет значение Sunday (Воскресенье).</p>
Дата | DayOfYear(X) | X: DateTime | Получает день года, представленный компонентом года в параметре X.<p>Примеры<br/>01.12.2015: 335 день 2015 года<br/>31.12.2015: 365 день 2015 года<br/>31.12.2016: 366 день 2016 года (високосного)</p>
Дата | DaysInMonth(X) | X: DateTime | Получает количество дней в месяце, представленном компонентом месяца в параметре X.<p>Пример: DaysInMonth для 15.09.2013 = 30, так как в сентябре 30 дней.</p>
Дата | EndOfDay(X) | X: DateTime | Получает значение даты и времени, которое соответствует окончанию суток (компонента дня) в параметре X.<p>Пример: EndOfDay для 15.09.2013 17:10:23 имеет значение 15.09.2013 23:59:59.</p>
Дата | EndOfMonth(X) | X: DateTime | Получает конец месяца, представленного компонентом месяца параметра X. <p>Пример: EndOfMonth для 15.09.2013 17:10:23 имеет значение 30.09.2013 23:59:59 (дата и время, соответствующие окончанию месяца сентября)</p>.
Дата | StartOfDay(X) | X: DateTime | Получает начало суток, представленных компонентом дня параметра X.<p>Пример: StartOfDay для 15.09.2013 17:10:23 имеет значение 15.09.2013 00:00:00.</p>
DateTime | From(X) | X: String | Разбор строки X в значение даты и времени.
DateTime | Ticks(X) | X: DateTime | Получает свойство тактов времени параметра X. Один такт равен 100 наносекундам. Значение этого свойства соответствует количеству тактов, прошедших с полуночи (24:00:00) 1 января 0001 года. 
текст | Format(X) | X: String (переменная) | Форматирует текст.

#### Пример функции Text.Format

	"defines": { 
	    "Year" : "$$Text.Format('{0:yyyy}',WindowStart)",
	    "Month" : "$$Text.Format('{0:MM}',WindowStart)",
	    "Day" : "$$Text.Format('{0:dd}',WindowStart)",
	    "Hour" : "$$Text.Format('{0:hh}',WindowStart)"
	}

> [AZURE.NOTE]При использовании функции внутри другой функции нет необходимости использовать для внутренней функции префикс **$$**. Например, $$Text.Format('PartitionKey eq \\'my\_pkey\_filter\_value\\' and RowKey ge \\'{0:yyyy-MM-dd HH:mm:ss}\\'', Time.AddHours(SliceStart, -6)). В этом примере обратите внимание, что префикс **$$** не используется для функции **Time.AddHours**.
  

## Подробный обзор зависимостей данных

Для создания среза набора данных путем запуска действия фабрика данных использует следующую **модель зависимостей**, которая позволяет определить связи между наборами данных, потребляемых действием, и наборами данных, которые оно создает.

Диапазон времени одного или нескольких входных наборов данных, необходимых для создания выходного среза набора данных, называется **периодом зависимости**.

При запуске действия срез набора данных создается только после того, как будут доступны срезы данных во входных наборах данных в пределах периода зависимости. Это значит, что все входные срезы, составляющие период зависимости, должны быть в статусе **Готово** для того, чтобы при запуске действия был создан выходной срез набора данных.

Для создания среза набора данных [start, end] требуется функция, сопоставляющая срез набора данных с его периодом зависимости. Эта функция, по сути, является формулой, которая преобразует начало и окончание среза набора данных, чтобы они соответствовали началу и окончанию периода зависимости. Более формально:
	
	DatasetSlice = [start, end]
	DependecyPeriod = [f(start, end), g(start, end)]

где f и g — функции сопоставления, которые вычисляют начало и окончание периода зависимости для каждого входного набора данных действия.

Как видно на примерах выше, в большинстве случаев период зависимости совпадает с периодом создания среза данных. В этих случаях фабрика данных автоматически вычисляет входные срезы данных, которые попадают в период зависимости.

В примере с агрегированием выше, где выходные данные формируются ежедневно, а входные доступны каждый час, период среза данных равен 24 часам. Фабрика данных находит актуальные почасовые входные срезы данных для этого периода времени и устанавливает зависимость выходного среза от входного.

Кроме того, вы можете указать собственное сопоставление для периода зависимости, как показано в примере выше, где один из входных наборов данных создавался еженедельно, а выходной набор — ежедневно.
   
## Зависимость и проверка данных

Для набора данных при необходимости можно определить политику проверки, указывающую, как данные, созданные выполнением среза, могут быть проверены перед готовностью к потреблению. Подробные сведения см. в статье [Создание наборов данных](data-factory-create-datasets.md).

В таких случаях после завершения выполнения среза состояние выходного среза меняется на **Ожидание** с подсостоянием **Проверка**. После проверки срезов их статус меняется на **Готово**.
   
Если срез данных был сформирован, но не прошел проверку, запуски действий для нижестоящих срезов, зависимых от не прошедшего проверку среза, обработаны не будут.

Различные состояния срезов данных в фабрике данных описаны в статье [Мониторинг конвейеров и управление ими](data-factory-monitor-manage-pipelines.md).

## Внешние данные

Набор данных можно пометить как внешний (как показано в JSON ниже), подразумевая, что он не был создан с помощью фабрики данных Azure. В таком случае политика набора данных может содержать дополнительный набор параметров для описания политики проверки и повторных попыток обработки набора данных. Описание всех свойств см. в разделе [Создание конвейеров](data-factory-create-pipelines.md).

Аналогично наборам данных, формируемым фабрикой данных, срезы внешних данных должны быть готовы до того, как можно будет обработать зависимые срезы.

	{
		"name": "AzureSqlInput",
		"properties": 
		{
			"type": "AzureSqlTable",
			"linkedServiceName": "AzureSqlLinkedService",
			"typeProperties": 
			{
				"tableName": "MyTable"	
			},
			"availability": 
			{
				"frequency": "Hour",
				"interval": 1     
			},
			"external": true,
			"policy": 
			{
				"externalData": 
				{
					"retryInterval": "00:01:00",
					"retryTimeout": "00:10:00",
					"maximumRetry": 3
				}
			}  
		} 
	} 






  









 
 












      

  

<!---HONumber=Oct15_HO4-->