<properties 
	pageTitle="Планирование и исполнение с использованием фабрики данных" 
	description="Сведения об аспектах планирования и исполнения в модели приложений фабрики данных Azure." 
	services="data-factory" 
	documentationCenter="" 
	authors="spelluru" 
	manager="jhubbard" 
	editor="monicar"/>

<tags 
	ms.service="data-factory" 
	ms.workload="data-services" 
	ms.tgt_pltfrm="na" 
	ms.devlang="na" 
	ms.topic="article" 
	ms.date="06/06/2016" 
	ms.author="spelluru"/>

# Планирование и исполнение с использованием фабрики данных
  
Здесь объясняются аспекты планирования и исполнения в модели приложений фабрики данных. Эта статья основана на статьях [Создание конвейеров](data-factory-create-pipelines.md) и [Создание наборов данных](data-factory-create-datasets.md). Чтобы понимать ее, нужно знать основные концепции, связанные с моделью приложений фабрики данных: действия, конвейеры, связанные службы и наборы данных.

## Планирование действий

С помощью раздела **scheduler** в JSON действия можно указать регулярное расписание данного действия. Например, можно запланировать выполнение действия каждый час следующим образом:

	"scheduler": {
		"frequency": "Hour",
	    "interval": 1
	},  
    
![Пример планировщика](./media/data-factory-scheduling-and-execution/scheduler-example.png)

Как показано выше, при задании расписания для действия создается последовательность стыкующихся окон. Стыкующиеся окна — это ряд интервалов фиксированного размера, которые не перекрываются и не соприкасаются. Эти логические стыкующиеся окна для действия называются **окнами действия**.
 
Для выполнения текущего окна действия доступ к связанному с ним интервалу времени можно получить с помощью системных переменных [WindowStart](data-factory-functions-variables.md#data-factory-system-variables) и [WindowEnd](data-factory-functions-variables.md#data-factory-system-variables) в JSON действия. Эти переменные вы можете использовать для различных целей в JSON действия и скриптах, связанных с действием, включая выбор данных из входных и выходных наборов данных, соответствующих временным рядам.

Свойство **scheduler** поддерживает те же подсвойства, что и свойство **availability** в наборе данных. Дополнительные сведения о различных свойствах, доступных для планировщика, в том числе о планировании с определенным смещением времени, установке режима выравнивания обработки в начале или в конце окна действия, см. в разделе [Доступность набора данных](data-factory-create-datasets.md#Availability).

На данном этапе указывать свойства планировщика для действия необязательно. Если свойства указываются, они должны соответствовать периодичности, заданной в определении выходного набора данных. В настоящее время расписание активируется с помощью выходного набора данных, поэтому его необходимо создать, даже если действие не создает никаких выходных данных. Если действие не принимает никаких входных данных, входной набор данных можно не создавать.

## Наборы данных и срезы данных временных рядов

Данные временного ряда — это непрерывная последовательность точек данных, состоящая, как правило, из последовательных измерений, выполненных с некоторым интервалом времени. Распространенными примерами данных временных рядов являются данные датчиков, данные телеметрии приложений и т. д.

С помощью фабрики данных Azure данные временных рядов можно обрабатывать в пакетном режиме с выполнением действий. Обычно входные данные поступают, а выходные данные требуют обработки с повторяющейся периодичностью. Эта периодичность моделируется путем указания раздела **availability** в наборе данных приведенным ниже образом.

    "availability": {
      "frequency": "Hour",
      "interval": 1
    },

Каждая единица данных, потребляемых и производимых запуском действия, называется **срезом** данных. На схеме ниже показан пример действия с входным и выходным наборами данных временных рядов, для каждого из которых доступность установлена с почасовой частотой.

![Планировщик доступности](./media/data-factory-scheduling-and-execution/availability-scheduler.png)

Почасовые срезы данных для входного и выходного наборов данных показаны на схеме выше. На схеме показаны три входных среза, готовых к обработке, и выполняемое действие 10–11 AM, которое выдает выходной срез 10–11 AM.

К интервалу времени, связанному с текущим обрабатываемым срезом, можно получить доступ в JSON набора данных посредством переменных [SliceStart](data-factory-functions-variables.md#data-factory-system-variables) и [SliceEnd](data-factory-functions-variables.md#data-factory-system-variables).

В настоящее время для фабрики данных требуется, чтобы расписание, указанное в действии, в точности соответствовало расписанию, указанному в разделе доступности выходного набора данных. Это значит, что WindowStart, WindowEnd, SliceStart и SliceEnd всегда сопоставляются с одним и тем же периодом времени и одиночным выходным срезом.

Дополнительные сведения о различных свойствах из раздела availability см. в статье [Создание наборов данных](data-factory-create-datasets.md).

## Пример. Действие копирования, перемещающее данные из SQL Azure в BLOB-объект Azure

Давайте объединим некоторые функции в действии, вернувшись к примеру из статьи [Создание конвейеров](data-factory-create-pipelines.md), где данные копируются каждый час из таблицы SQL Azure в BLOB-объект Azure.

**Входные данные: набор данных SQL Azure**

	{
	    "name": "AzureSqlInput",
	    "properties": {
	        "published": false,
	        "type": "AzureSqlTable",
	        "linkedServiceName": "AzureSqlLinkedService",
	        "typeProperties": {
	            "tableName": "MyTable"
	        },
	        "availability": {
	            "frequency": "Hour",
	            "interval": 1
	        },
	        "external": true,
	        "policy": {}
	    }
	}


Обратите внимание, что в разделе **availability** для параметра **frequency** установлено значение **Hour**, а для параметра **interval** — значение **1**.

**Выходные данные: набор данных BLOB-объекта Azure**
	
	{
	    "name": "AzureBlobOutput",
	    "properties": {
	        "published": false,
	        "type": "AzureBlob",
	        "linkedServiceName": "StorageLinkedService",
	        "typeProperties": {
	            "folderPath": "mypath/{Year}/{Month}/{Day}/{Hour}",
	            "format": {
	                "type": "TextFormat"
	            },
	            "partitionedBy": [
	                {
	                    "name": "Year",
	                    "value": {
	                        "type": "DateTime",
	                        "date": "SliceStart",
	                        "format": "yyyy"
	                    }
	                },
	                {
	                    "name": "Month",
	                    "value": {
	                        "type": "DateTime",
	                        "date": "SliceStart",
	                        "format": "%M"
	                    }
	                },
	                {
	                    "name": "Day",
	                    "value": {
	                        "type": "DateTime",
	                        "date": "SliceStart",
	                        "format": "%d"
	                    }
	                },
	                {
	                    "name": "Hour",
	                    "value": {
	                        "type": "DateTime",
	                        "date": "SliceStart",
	                        "format": "%H"
	                    }
	                }
	            ]
	        },
	        "availability": {
	            "frequency": "Hour",
	            "interval": 1
	        }
	    }
	}


Обратите внимание, что в разделе **availability** для параметра **frequency** установлено значение **Hour**, а для параметра **interval** — значение **1**.



**Действие: действие копирования**

	{
	    "name": "SamplePipeline",
	    "properties": {
	        "description": "copy activity",
	        "activities": [
	            {
	                "type": "Copy",
	                "name": "AzureSQLtoBlob",
	                "description": "copy activity",	
	                "typeProperties": {
	                    "source": {
	                        "type": "SqlSource",
	                        "sqlReaderQuery": "$$Text.Format('select * from MyTable where timestampcolumn >= \\'{0:yyyy-MM-dd HH:mm}\\' AND timestampcolumn < \\'{1:yyyy-MM-dd HH:mm}\\'', WindowStart, WindowEnd)"
	                    },
	                    "sink": {
	                        "type": "BlobSink",
	                        "writeBatchSize": 100000,
	                        "writeBatchTimeout": "00:05:00"
	                    }
	                },
	                "inputs": [
	                    {
	                        "name": "AzureSQLInput"
	                    }
	                ],
	                "outputs": [
	                    {
	                        "name": "AzureBlobOutput"
	                    }
	                ],
	       			"scheduler": {
	          			"frequency": "Hour",
	          			"interval": 1
	        		}
	            }
	        ],
	        "start": "2015-01-01T08:00:00Z",
	        "end": "2015-01-01T11:00:00Z"
	    }
	}


В предыдущем примере показаны разделы расписания действий и доступности набора данных, для которых установлена почасовая частота. В примере показано, как с помощью переменных **WindowStart** и **WindowEnd** выбрать значимые данные для заданного запуска действия и отправить их в BLOB-объект с соответствующим динамическим путем к папке (**folderPath**), параметризованным так, чтобы для каждого часа была отдельная папка.

При выполнении 3 срезов между 8 и 11 часами утра таблица Azure и BLOB-объект из примера будут выглядеть таким образом.

Предположим, что в SQL Azure содержатся приведенные ниже данные.

![Пример ввода](./media/data-factory-scheduling-and-execution/sample-input-data.png)

При развертывании указанного выше конвейера BLOB-объект Azure будет заполнен следующим образом:

1.	Файл mypath/2015/1/1/8/Data.<GUID>.txt с данными.

		10002345,334,2,2015-01-01 08:24:00.3130000
		10002345,347,15,2015-01-01 08:24:00.6570000
		10991568,2,7,2015-01-01 08:56:34.5300000

	**Примечание.** Вместо <GUID> будет указан фактический идентификатор GUID. Пример имени файла: Data.bcde1348-7620-4f93-bb89-0eed3455890b.txt.
2.	Файл mypath/2015/1/1/9/Data.<GUID>.txt с данными.

		10002345,334,1,2015-01-01 09:13:00.3900000
		24379245,569,23,2015-01-01 09:25:00.3130000
		16777799,21,115,2015-01-01 09:47:34.3130000
3.	Файл mypath/2015/1/1/10/Data.<GUID>.txt с данными.


## Срезы данных, активный период конвейера и параллельное выполнение срезов

В статье [Создание конвейеров](data-factory-create-pipelines.md) представлена концепция активного периода для конвейера, указываемого установкой свойств **start** и **end** конвейера.
 
Вы можете задать дату начала для активного периода конвейера, находящуюся в прошлом, и фабрика данных автоматически вычислит все срезы данных из прошлого (выполнит обратное заполнение) и начнет их обработку.

После обратного заполнения срезов данных можно настроить их параллельное выполнение. Вы можете это сделать, задав свойство **concurrency** в разделе **policy** JSON действия, как показано в статье [Создание конвейеров](data-factory-create-pipelines.md).

## Повторный запуск сбойных срезов данных и автоматическое отслеживание зависимостей данных

Существуют широкие визуальные возможности для мониторинга выполнения срезов. Дополнительные сведения см. в статьях о **мониторинге конвейеров и управлении ими с помощью** [портала Azure](data-factory-monitor-manage-pipelines.md) (или) [мониторинге и управлении с помощью приложений](data-factory-monitor-manage-app.md).

Рассмотрим следующий пример, в котором показаны два действия. Действие Activity1 выдает набор данных временного ряда со срезами в качестве выходных данных, которые потребляются как входные данные действием Activity2 для формирования конечного выходного набора данных временного ряда.

![Срез, в котором произошла ошибка](./media/data-factory-scheduling-and-execution/failed-slice.png)

.<br/>

На схеме выше показано, что среди 3 последних срезов произошла ошибка создания среза 9–10 AM для набора данных **Dataset2**. Фабрика данных автоматически отслеживает зависимости набора данных временного ряда и в результате задерживает запуск действия для нижестоящего среза 9-10 AM.


Средства мониторинга и управления фабриками данных позволяют детально просмотреть журналы диагностики на предмет неудачного среза, легко найти причину неполадки и устранить ее. После устранения неполадки можно также легко инициировать запуск действия для создания среза, в котором произошла ошибка. Дополнительные сведения о том, как инициировать повторные запуски, а также о переходах от одного состояния срезов данных к другому, см. в статьях о **мониторинге конвейеров и управлении ими с помощью** [портала Azure](data-factory-monitor-manage-pipelines.md) (или) [мониторинге и управлении с помощью приложений](data-factory-monitor-manage-app.md).

После инициирования повторного запуска, когда срез 9–10 AM для dataset2 будет готов, фабрика данных инициирует запуск для зависимого среза 9–10 AM в конечном наборе данных, как показано на схеме ниже.

![Повторный запуск среза, в котором произошла ошибка](./media/data-factory-scheduling-and-execution/rerun-failed-slice.png)

Для более глубокого знакомства с указанием и отслеживанием зависимостей в сложных цепочках действий и наборов см. следующие разделы.

## Цепочки действий
Можно объединить в цепочку два действия, используя выходной набор данных одного действия как входной набор данных другого действия. Действия могут находиться в одном конвейере или разных конвейерах. Второе действие выполняется только после успешного завершения первого.

Например, рассмотрим следующий случай.
 
1.	В конвейере P1 есть действие A1, для которого требуется внешний входной набор данных D1. Оно создает **выходной** набор данных **D2**.
2.	В конвейере P2 есть действие A2, для которого требуется **ввод** из набора данных **D2**. Оно создает выходной набор данных D3.
 
В этом случае действие A1 будет выполняться, когда доступны внешние данные и достигнута запланированная частота доступности. Действие A2 будет выполняться, когда доступны запланированные срезы из D2 и достигнута запланированная частота доступности. В случае ошибки в одном из срезов в наборе данных D2 действие A2 не запустится для этого среза, пока он не станет доступным.

Представление схемы будет выглядеть следующим образом.

![Построение цепочки действий в двух конвейерах](./media/data-factory-scheduling-and-execution/chaining-two-pipelines.png)

Представление схемы с обоими действиями в одном конвейере будет выглядеть следующим образом.

![Построение цепочки действий в одном конвейере](./media/data-factory-scheduling-and-execution/chaining-one-pipeline.png)

### Упорядоченное копирование
Несколько операций копирования можно выполнить друг за другом последовательно или упорядоченно. Предположим, что у вас в конвейере есть два действия копирования: CopyActivity1 и CopyActivity2 со следующими наборами входных и выходных данных.

CopyActivity1: входные данные — Dataset1, выходные данные — Dataset2

CopyActivity2: входные данные — Dataset2, выходные данные — Dataset4

Действие копирования CopyActivity2 будет выполнено только в том случае, если действие копирования CopyActivity1 прошло успешно и набор данных Dataset2 доступен.

В приведенном выше примере действие копирования CopyActivity2 может иметь другие входные данные, например набор данных Dataset3, но необходимо также указать набор Dataset2 в качестве входных данных, чтобы действие копирования CopyActivity2 не запускалось, пока не завершится действие копирования CopyActivity1. Например:

CopyActivity1: входные данные — Dataset1, выходные данные — Dataset2

CopyActivity2: входные данные — Dataset3, Dataset2, выходные данные —Dataset4

Если указано несколько наборов входных данных, то для копирования используется только первый набор, а другие наборы используются в качестве зависимостей. Действие CopyActivity2 запустилось бы только при соблюдении следующих условий:

- Действие CopyActivity1 успешно завершено, и набор данных Dataset2 доступен. Этот набор данных не будет использоваться при копировании данных в Dataset4. Он используется только как зависимость для планирования CopyActivity2.
- Набор данных Dataset3 доступен. Этот данные, которые копируются в место назначения.



## Моделирование наборов данных с разной частотой

В примерах, приведенных выше, частоты входных и выходных наборов данных и окон расписания действий были одинаковыми. В некоторых сценариях требуется возможность создавать выходные данные с частотой, отличной от частоты одного или нескольких наборов входных данных. Фабрика данных поддерживает моделирование таких сценариев.

### Пример 1. Создание ежедневного выходного отчета по входным данным, которые доступны каждый час

Рассмотрим сценарий, где имеются входные данные измерений датчиков, доступные каждый час в BLOB-объекте Azure, а нам требуется формировать ежедневный совокупный отчет со статистикой средних, максимальных, минимальных и т. п. показателей за день с помощью фабрики данных [Действие Hive](data-factory-hive-activity.md).

Смоделировать это с помощью фабрики данных можно приведенным ниже способом.

**Входной набор данных BLOB-объекта Azure**

Почасовые входные файлы за заданный день удаляются из папки. Для входных данных устанавливается почасовая доступность (frequency: Hour, interval: 1).

	{
	  "name": "AzureBlobInput",
	  "properties": {
	    "type": "AzureBlob",
	    "linkedServiceName": "StorageLinkedService",
	    "typeProperties": {
	      "folderPath": "mycontainer/myfolder/{Year}/{Month}/{Day}/",
	      "partitionedBy": [
	        { "name": "Year", "value": {"type": "DateTime","date": "SliceStart","format": "yyyy"}},
	        { "name": "Month","value": {"type": "DateTime","date": "SliceStart","format": "%M"}},
	        { "name": "Day","value": {"type": "DateTime","date": "SliceStart","format": "%d"}}
	      ],
	      "format": {
	        "type": "TextFormat"
	      }
	    },
		"external": true,
	    "availability": {
	      "frequency": "Hour",
	      "interval": 1
	    }
	  }
	}

**Выходной набор данных BLOB-объекта Azure**

Каждый день из папки удаляется один выходной файл за целый день. Для выходных данных устанавливается ежедневная доступность (frequency: Day, interval: 1).


	{
	  "name": "AzureBlobOutput",
	  "properties": {
	    "type": "AzureBlob",
	    "linkedServiceName": "StorageLinkedService",
	    "typeProperties": {
	      "folderPath": "mycontainer/myfolder/{Year}/{Month}/{Day}/",
	      "partitionedBy": [
	        { "name": "Year", "value": {"type": "DateTime","date": "SliceStart","format": "yyyy"}},
	        { "name": "Month","value": {"type": "DateTime","date": "SliceStart","format": "%M"}},
	        { "name": "Day","value": {"type": "DateTime","date": "SliceStart","format": "%d"}}
	      ],
	      "format": {
	        "type": "TextFormat"
	      }
	    },
	    "availability": {
	      "frequency": "Day",
	      "interval": 1
	    }
	  }
	}

**Действие: действие Hive в конвейере**

Скрипт hive получает соответствующие сведения о дате и времени в качестве параметров, используя переменную **WindowStart**, как показано ниже. Эту переменную скрипт hive использует для загрузки данных из нужной папки за день и для создания выходных данных путем агрегирования.

		{  
		    "name":"SamplePipeline",
		    "properties":{  
		    "start":"2015-01-01T08:00:00",
		    "end":"2015-01-01T11:00:00",
		    "description":"hive activity",
		    "activities": [
		        {
		            "name": "SampleHiveActivity",
		            "inputs": [
		                {
		                    "name": "AzureBlobInput"
		                }
		            ],
		            "outputs": [
		                {
		                    "name": "AzureBlobOutput"
		                }
		            ],
		            "linkedServiceName": "HDInsightLinkedService",
		            "type": "HDInsightHive",
		            "typeProperties": {
		                "scriptPath": "adftutorial\\hivequery.hql",
		                "scriptLinkedService": "StorageLinkedService",
		                "defines": {
		                    "Year": "$$Text.Format('{0:yyyy}',WindowStart)",
		                    "Month": "$$Text.Format('{0:%M}',WindowStart)",
		                    "Day": "$$Text.Format('{0:%d}',WindowStart)"
		                }
		            },
		            "scheduler": {
		                "frequency": "Day",
		                "interval": 1
		            },			
		            "policy": {
		                "concurrency": 1,
		                "executionPriorityOrder": "OldestFirst",
		                "retry": 2,
		                "timeout": "01:00:00"
		            }
	             }
		     ]
		   }
		}

Вот как это выглядит с точки зрения зависимостей данных.

![Зависимость данных](./media/data-factory-scheduling-and-execution/data-dependency.png)

Выходной срез за каждый день зависит от 24 почасовых срезов из входного набора данных. Фабрика данных автоматически вычисляет эти зависимости, определяя срезы, которые попадают в тот же период времени, что и создаваемый выходной срез. Если какой-либо из 24 входных срезов недоступен (например, из-за продолжающейся обработки в вышестоящем действии, которое создает этот срез), фабрика данных будет дожидаться готовности входного среза перед запуском ежедневного действия.


### Пример 2. Указание зависимости с использованием выражений и функций фабрики данных

Рассмотрим другой сценарий. Предположим, имеется действие Hive, которое обрабатывает два входных набора данных, в одном из которых новые данные появляются ежедневно, а в другом — раз в неделю. Предположим, что требуется выполнить соединение двух входных наборов и выдать ежедневный выходной набор данных.
 
Раньше использовался простой подход: фабрика данных автоматически определяла нужные входные срезы для обработки, добавляя входные срезы данных, согласованные с периодом времени выходного среза данных. Сейчас этот подход уже не работает.

Необходим способ указать фабрике данных для каждого действия, что следует использовать срез данных за последнюю неделю из еженедельного входного набора данных. Это можно сделать с помощью функций фабрики данных Azure, как показано ниже.

**Входной набор 1: BLOB-объект Azure**

Первый входной набор данных представляет собой BLOB-объект Azure, обновляемый **ежедневно**.
	
	{
	  "name": "AzureBlobInputDaily",
	  "properties": {
	    "type": "AzureBlob",
	    "linkedServiceName": "StorageLinkedService",
	    "typeProperties": {
	      "folderPath": "mycontainer/myfolder/{Year}/{Month}/{Day}/",
	      "partitionedBy": [
	        { "name": "Year", "value": {"type": "DateTime","date": "SliceStart","format": "yyyy"}},
	        { "name": "Month","value": {"type": "DateTime","date": "SliceStart","format": "%M"}},
	        { "name": "Day","value": {"type": "DateTime","date": "SliceStart","format": "%d"}}
	      ],
	      "format": {
	        "type": "TextFormat"
	      }
	    },
		"external": true,
	    "availability": {
	      "frequency": "Day",
	      "interval": 1
	    }
	  }
	}

**Входной набор 2: BLOB-объект Azure**

Второй входной набор — BLOB-объект Azure, обновляемый **еженедельно**.

	{
	  "name": "AzureBlobInputWeekly",
	  "properties": {
	    "type": "AzureBlob",
	    "linkedServiceName": "StorageLinkedService",
	    "typeProperties": {
	      "folderPath": "mycontainer/myfolder/{Year}/{Month}/{Day}/",
	      "partitionedBy": [
	        { "name": "Year", "value": {"type": "DateTime","date": "SliceStart","format": "yyyy"}},
	        { "name": "Month","value": {"type": "DateTime","date": "SliceStart","format": "%M"}},
	        { "name": "Day","value": {"type": "DateTime","date": "SliceStart","format": "%d"}}
	      ],
	      "format": {
	        "type": "TextFormat"
	      }
	    },
		"external": true,
	    "availability": {
	      "frequency": "Day",
	      "interval": 7
	    }
	  }
	}

**Выходной набор данных: BLOB-объект Azure**

Каждый день из папки удаляется один выходной файл за целый день. Для выходных данных задается ежедневная доступность (frequency: Day, interval: 1).
	
	{
	  "name": "AzureBlobOutputDaily",
	  "properties": {
	    "type": "AzureBlob",
	    "linkedServiceName": "StorageLinkedService",
	    "typeProperties": {
	      "folderPath": "mycontainer/myfolder/{Year}/{Month}/{Day}/",
	      "partitionedBy": [
	        { "name": "Year", "value": {"type": "DateTime","date": "SliceStart","format": "yyyy"}},
	        { "name": "Month","value": {"type": "DateTime","date": "SliceStart","format": "%M"}},
	        { "name": "Day","value": {"type": "DateTime","date": "SliceStart","format": "%d"}}
	      ],
	      "format": {
	        "type": "TextFormat"
	      }
	    },
	    "availability": {
	      "frequency": "Day",
	      "interval": 1
	    }
	  }
	}

**Действие: действие Hive в конвейере**

Действие hive принимает 2 входных набора данных и создает выходной срез данных каждый день. Зависимость ежедневного выходного среза от входного среза прошлой недели вы можете задать для еженедельных входных данных приведенным ниже образом.
	
	{  
	    "name":"SamplePipeline",
	    "properties":{  
	    "start":"2015-01-01T08:00:00",
	    "end":"2015-01-01T11:00:00",
	    "description":"hive activity",
	    "activities": [
	      {
	        "name": "SampleHiveActivity",
	        "inputs": [
	          {
	            "name": "AzureBlobInputDaily"
	          },
	          {
	            "name": "AzureBlobInputWeekly",
	            "startTime": "Date.AddDays(SliceStart, - Date.DayOfWeek(SliceStart))",
	            "endTime": "Date.AddDays(SliceEnd,  -Date.DayOfWeek(SliceEnd))"  
	          }
	        ],
	        "outputs": [
	          {
	            "name": "AzureBlobOutputDaily"
	          }
	        ],
	        "linkedServiceName": "HDInsightLinkedService",
	        "type": "HDInsightHive",
	        "typeProperties": {
	          "scriptPath": "adftutorial\\hivequery.hql",
	          "scriptLinkedService": "StorageLinkedService",
	          "defines": {
	            "Year": "$$Text.Format('{0:yyyy}',WindowStart)",
	            "Month": "$$Text.Format('{0:%M}',WindowStart)",
	            "Day": "$$Text.Format('{0:%d}',WindowStart)"
	          }
	        },
	        "scheduler": {
	          "frequency": "Day",
	          "interval": 1
	        },			
	        "policy": {
	          "concurrency": 1,
	          "executionPriorityOrder": "OldestFirst",
	          "retry": 2,  
	          "timeout": "01:00:00"
	        }
		   } 
	     ]
	   }
	}


## Функции и системные переменные фабрики данных   

В статье [Фабрика данных Azure — функции и системные переменные](data-factory-functions-variables.md) приведен список функций и системных переменных, поддерживаемых фабрикой данных Azure.

## Подробный обзор зависимостей данных

Для создания среза набора данных путем запуска действия фабрика данных использует следующую **модель зависимостей**, которая позволяет определить связи между наборами данных, потребляемых действием, и наборами данных, которые оно создает.

Диапазон времени одного или нескольких входных наборов данных, необходимых для создания выходного среза набора данных, называется **периодом зависимости**.

При запуске действия срез набора данных создается только после того, как будут доступны срезы данных во входных наборах данных в пределах периода зависимости. Это значит, что все входные срезы, составляющие период зависимости, должны быть в статусе **Готово** для того, чтобы при запуске действия был создан выходной срез набора данных.

Для создания среза набора данных [start, end] требуется функция, сопоставляющая срез набора данных с его периодом зависимости. Эта функция, по сути, является формулой, которая преобразует начало и окончание среза набора данных, чтобы они соответствовали началу и окончанию периода зависимости. Более формально:
	
	DatasetSlice = [start, end]
	DependecyPeriod = [f(start, end), g(start, end)]

где f и g — функции сопоставления, которые вычисляют начало и окончание периода зависимости для каждого входного набора данных действия.

Как видно на примерах выше, в большинстве случаев период зависимости совпадает с периодом создания среза данных. В этих случаях фабрика данных автоматически вычисляет входные срезы данных, которые попадают в период зависимости.

В примере с агрегированием выше, где выходные данные формируются ежедневно, а входные доступны каждый час, период среза данных равен 24 часам. Фабрика данных находит актуальные почасовые входные срезы данных для этого периода времени и устанавливает зависимость выходного среза от входного.

Кроме того, вы можете указать собственное сопоставление для периода зависимости, как показано в примере выше, где один из входных наборов данных создавался еженедельно, а выходной набор — ежедневно.
   
## Зависимость и проверка данных

Для набора данных при необходимости можно определить политику проверки, указывающую, как данные, созданные выполнением среза, могут быть проверены перед готовностью к потреблению. Подробные сведения см. в статье [Создание наборов данных](data-factory-create-datasets.md).

В таких случаях после завершения выполнения среза состояние выходного среза меняется на **Ожидание** с подсостоянием **Проверка**. После проверки срезов их статус меняется на **Готово**.
   
Если срез данных был сформирован, но не прошел проверку, запуски действий для нижестоящих срезов, зависимых от не прошедшего проверку среза, обработаны не будут.

Различные состояния срезов данных в фабрике данных описаны в статье [Мониторинг конвейеров и управление ими](data-factory-monitor-manage-pipelines.md).

## Внешние данные

Набор данных можно пометить как внешний (как показано в JSON ниже), подразумевая, что он не был создан с помощью фабрики данных Azure. В таком случае политика набора данных может содержать дополнительный набор параметров для описания политики проверки и повторных попыток обработки набора данных. Описание всех свойств см. в разделе [Создание конвейеров](data-factory-create-pipelines.md).

Аналогично наборам данных, формируемым фабрикой данных, срезы внешних данных должны быть готовы до того, как можно будет обработать зависимые срезы.

	{
		"name": "AzureSqlInput",
		"properties": 
		{
			"type": "AzureSqlTable",
			"linkedServiceName": "AzureSqlLinkedService",
			"typeProperties": 
			{
				"tableName": "MyTable"	
			},
			"availability": 
			{
				"frequency": "Hour",
				"interval": 1     
			},
			"external": true,
			"policy": 
			{
				"externalData": 
				{
					"retryInterval": "00:01:00",
					"retryTimeout": "00:10:00",
					"maximumRetry": 3
				}
			}  
		} 
	} 


## Однократный конвейер
Вы можете создать конвейер и настроить его на периодическое выполнение (ежечасно, ежедневно и т. д.) в пределах между временем начала и окончания, заданным в определении конвейера. Дополнительные сведения см. в разделе [Планирование действий](#scheduling-and-execution). Вы также можете создать конвейер, выполняемый однократно. Для этого свойству **pipelineMode** в определении конвейера необходимо присвоить значение **onetime** (однократный), как показано в приведенном ниже примере файла JSON. По умолчанию для этого свойства используется значение **scheduled** (по расписанию).

	{
	    "name": "CopyPipeline",
	    "properties": {
	        "activities": [
	            {
	                "type": "Copy",
	                "typeProperties": {
	                    "source": {
	                        "type": "BlobSource",
	                        "recursive": false
	                    },
	                    "sink": {
	                        "type": "BlobSink",
	                        "writeBatchSize": 0,
	                        "writeBatchTimeout": "00:00:00"
	                    }
	                },
	                "inputs": [
	                    {
	                        "name": "InputDataset"
	                    }
	                ],
	                "outputs": [
	                    {
	                        "name": "OutputDataset"
	                    }
	                ]
	                "name": "CopyActivity-0"
	            }
	        ]
	        "pipelineMode": "OneTime"
	    }
	}

Обратите внимание на следующее:
 
- Время **начала** и **окончания** для этого конвейера указывать не нужно.
- При этом нужно указывать доступность входных и выходных наборов данных (периодичность и интервал), несмотря на то что фабрика данных эти значения не использует.
- В представлении диаграммы однократные конвейеры не отображаются. Это сделано специально.
- Однократные конвейеры не обновляются. Однократный конвейер можно клонировать, переименовать, обновить его свойства и развернуть, чтобы создать другой конвейер.

  




  









 
 












      

  

<!---HONumber=AcomDC_0817_2016-->